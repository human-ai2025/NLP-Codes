{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-label-text-classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOo6u3ijInA0epP3q5Bm8+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e2defe5456e494da2028148d4166063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb01f46602e44013ac2ae892dc79f508",
              "IPY_MODEL_99e4a813ec3c4f4dad9a74b5b2f186a0",
              "IPY_MODEL_838f3bc32c764a55a46ae57fceeb01fe"
            ],
            "layout": "IPY_MODEL_b14f31575ea8484ab2dfd750c1df5e02"
          }
        },
        "fb01f46602e44013ac2ae892dc79f508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eff29395bb1746e992837556a9d4896d",
            "placeholder": "​",
            "style": "IPY_MODEL_d356eba98293465e895023b9f8d5f710",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "99e4a813ec3c4f4dad9a74b5b2f186a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70ae43856efa43beb58bd8d1c929497d",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7add36acfdcd487db259799a146e99e9",
            "value": 231508
          }
        },
        "838f3bc32c764a55a46ae57fceeb01fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a39bf952ef41dabdc9dd459e697930",
            "placeholder": "​",
            "style": "IPY_MODEL_f8d04b6f6d804ebeb03bdadc0bc1f4fa",
            "value": " 226k/226k [00:00&lt;00:00, 274kB/s]"
          }
        },
        "b14f31575ea8484ab2dfd750c1df5e02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eff29395bb1746e992837556a9d4896d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d356eba98293465e895023b9f8d5f710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70ae43856efa43beb58bd8d1c929497d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7add36acfdcd487db259799a146e99e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1a39bf952ef41dabdc9dd459e697930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d04b6f6d804ebeb03bdadc0bc1f4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ad04a9b3eeb4c97baf1a5ba9fcb8dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c58f0ff09e644f20a383940501615d23",
              "IPY_MODEL_21f4af300b2a4e9f9b59099a63bf917e",
              "IPY_MODEL_8fb6a71af4ae4b028bcbab1c3eae5f12"
            ],
            "layout": "IPY_MODEL_1341805d91464e42a7516600f5eac8f6"
          }
        },
        "c58f0ff09e644f20a383940501615d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c312fdab19b4ef08b9e2ae12be8dc93",
            "placeholder": "​",
            "style": "IPY_MODEL_a4a2a235f574472ba50e9fc200216222",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "21f4af300b2a4e9f9b59099a63bf917e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4635ccaef14c4e45a11665c43ecdd2f9",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1dfa479e1d84478b4b5c5fad150be3e",
            "value": 28
          }
        },
        "8fb6a71af4ae4b028bcbab1c3eae5f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237aed63c8d8463eb7b3c506f0e447c0",
            "placeholder": "​",
            "style": "IPY_MODEL_2bd07662bc5743cf9e7db57649ace178",
            "value": " 28.0/28.0 [00:00&lt;00:00, 827B/s]"
          }
        },
        "1341805d91464e42a7516600f5eac8f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c312fdab19b4ef08b9e2ae12be8dc93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4a2a235f574472ba50e9fc200216222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4635ccaef14c4e45a11665c43ecdd2f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1dfa479e1d84478b4b5c5fad150be3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "237aed63c8d8463eb7b3c506f0e447c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd07662bc5743cf9e7db57649ace178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e12cae7f9f6845849c65e3ea423c28ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f81ee31c1f0247718419b007dfc0819b",
              "IPY_MODEL_736e838498ce45bfb48c043f9c30fd36",
              "IPY_MODEL_c5debad22af94fdea6f00d0963ad3144"
            ],
            "layout": "IPY_MODEL_ee6c8ebc600e4334b0b22b6a36d8ff16"
          }
        },
        "f81ee31c1f0247718419b007dfc0819b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa36a8b0fae43e18466333805473af5",
            "placeholder": "​",
            "style": "IPY_MODEL_7a51b1e20bac42cc8c9604cd8d6a6f19",
            "value": "Downloading config.json: 100%"
          }
        },
        "736e838498ce45bfb48c043f9c30fd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01d350c5048d475399873b84567fa5fe",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92506b5f721448c6a64c091916faa19f",
            "value": 570
          }
        },
        "c5debad22af94fdea6f00d0963ad3144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f462075e12b4909ac04b8ab2528e296",
            "placeholder": "​",
            "style": "IPY_MODEL_deb52ce608c445c9863021232f32a158",
            "value": " 570/570 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "ee6c8ebc600e4334b0b22b6a36d8ff16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa36a8b0fae43e18466333805473af5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a51b1e20bac42cc8c9604cd8d6a6f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01d350c5048d475399873b84567fa5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92506b5f721448c6a64c091916faa19f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f462075e12b4909ac04b8ab2528e296": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb52ce608c445c9863021232f32a158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3715c806cbbc4190b336ae70d194d7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd5cdeaa8c804b66a2f3a772cdd992a7",
              "IPY_MODEL_7bfd55f0523b4972825e6b59ae9b8c9a",
              "IPY_MODEL_39a5b8bc73954e6d90e92c37f9aa99b2"
            ],
            "layout": "IPY_MODEL_ce1f863c49f8432291298ac1765917ba"
          }
        },
        "fd5cdeaa8c804b66a2f3a772cdd992a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ccac31742004312a2a6f7a939f310d4",
            "placeholder": "​",
            "style": "IPY_MODEL_1c61bc2939fe40f7b8a197443134a487",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "7bfd55f0523b4972825e6b59ae9b8c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee67e18201524e8bb6f37a0502e2dc1a",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c13e27e603d4be2ad3eb518da658420",
            "value": 440473133
          }
        },
        "39a5b8bc73954e6d90e92c37f9aa99b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f11e12381b3d429e95ae0aa842e33d8e",
            "placeholder": "​",
            "style": "IPY_MODEL_f9bed9be3bf44844bf5eb9b492bdbc49",
            "value": " 420M/420M [00:07&lt;00:00, 50.1MB/s]"
          }
        },
        "ce1f863c49f8432291298ac1765917ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ccac31742004312a2a6f7a939f310d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c61bc2939fe40f7b8a197443134a487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee67e18201524e8bb6f37a0502e2dc1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c13e27e603d4be2ad3eb518da658420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f11e12381b3d429e95ae0aa842e33d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9bed9be3bf44844bf5eb9b492bdbc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/human-ai2025/nlp_projects/blob/master/Multi_label_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M7u9q4D7AfW6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BFHPTKtDZxx",
        "outputId": "2c7e2b2e-ee69-411c-bf5e-b17627d0ad68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/tokens/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "xEXXIu3QDZuJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu1fMjTcDZqW",
        "outputId": "31a7df6f-b0bf-4d0c-ba0a-46bfdd462a39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading jigsaw-toxic-comment-classification-challenge.zip to /content\n",
            " 85% 45.0M/52.6M [00:01<00:00, 35.7MB/s]\n",
            "100% 52.6M/52.6M [00:01<00:00, 36.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip jigsaw-toxic-comment-classification-challenge.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwZ1twIlDwRq",
        "outputId": "bb6daeb5-52ab-4f96-c93a-63fc95c7fcba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  jigsaw-toxic-comment-classification-challenge.zip\n",
            "  inflating: sample_submission.csv.zip  \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: test_labels.csv.zip     \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip\n",
        "!unzip test_labels.csv.zip\n",
        "!unzip sample_submission.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQjpZr1aD-mE",
        "outputId": "aa862afa-948d-4bcf-b806-e1fee3048a3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  test_labels.csv.zip\n",
            "  inflating: test_labels.csv         \n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import Stuff \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import sys\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "1jtcqw9KA3Cz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainPath = \"/content/train.csv\"\n",
        "testPath = \"/content/test.csv\"\n",
        "trainDf = pd.read_csv(trainPath)\n",
        "testDf = pd.read_csv(testPath)"
      ],
      "metadata": {
        "id": "T1u_9yHsBbiq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JJzztIl8E1an",
        "outputId": "a94980ef-74a3-40ee-c811-ec449b315bf5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      id                                       comment_text  \\\n",
              "150445  6c6f1e6637ce5331  What about the Raj???\\nWhat about the Raj hist...   \n",
              "64406   ac5f7c469c228f98  \"\\n\\n More discussion of changes \\n\\nHere's a ...   \n",
              "97618   0a39e0153a2c4f3d  Yeah, ok. Insulting me is a good way to figure...   \n",
              "86402   e717efd705b960bd  Comments \\n\\nHi, I know you're actively involv...   \n",
              "144782  12508df370bfb38b  \"\\n\\nJokestress, enough! Multiple times you've...   \n",
              "\n",
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
              "150445      0             0        0       0       0              0  \n",
              "64406       0             0        0       0       0              0  \n",
              "97618       0             0        0       0       0              0  \n",
              "86402       0             0        0       0       0              0  \n",
              "144782      0             0        0       0       0              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f1919fb8-e7b4-424b-bf9b-4f481923b80c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150445</th>\n",
              "      <td>6c6f1e6637ce5331</td>\n",
              "      <td>What about the Raj???\\nWhat about the Raj hist...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64406</th>\n",
              "      <td>ac5f7c469c228f98</td>\n",
              "      <td>\"\\n\\n More discussion of changes \\n\\nHere's a ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97618</th>\n",
              "      <td>0a39e0153a2c4f3d</td>\n",
              "      <td>Yeah, ok. Insulting me is a good way to figure...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86402</th>\n",
              "      <td>e717efd705b960bd</td>\n",
              "      <td>Comments \\n\\nHi, I know you're actively involv...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144782</th>\n",
              "      <td>12508df370bfb38b</td>\n",
              "      <td>\"\\n\\nJokestress, enough! Multiple times you've...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1919fb8-e7b4-424b-bf9b-4f481923b80c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1919fb8-e7b4-424b-bf9b-4f481923b80c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1919fb8-e7b4-424b-bf9b-4f481923b80c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf.drop(labels=[\"id\"], axis=1, inplace=True)\n",
        "trainDf.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "djUiPvjzFAkA",
        "outputId": "525db861-6b01-4ea2-9f56-daca34262e06"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             comment_text  toxic  \\\n",
              "70876   My final statements in the discussion above sh...      0   \n",
              "22072   \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
              "113358  \"\\n\\nBecause as a certified loser who has spen...      1   \n",
              "104392  \"\\n\\nYou missed history between 1941. and 1945...      0   \n",
              "122982  \"\\n\\nWhat is it you're saying?\\nThat if you ge...      1   \n",
              "\n",
              "        severe_toxic  obscene  threat  insult  identity_hate  \n",
              "70876              0        0       0       0              0  \n",
              "22072              0        0       0       0              0  \n",
              "113358             0        1       0       1              0  \n",
              "104392             0        0       0       0              0  \n",
              "122982             0        0       0       0              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f7825cb-2a29-427b-98dd-55fe6bd194ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>70876</th>\n",
              "      <td>My final statements in the discussion above sh...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22072</th>\n",
              "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113358</th>\n",
              "      <td>\"\\n\\nBecause as a certified loser who has spen...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104392</th>\n",
              "      <td>\"\\n\\nYou missed history between 1941. and 1945...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122982</th>\n",
              "      <td>\"\\n\\nWhat is it you're saying?\\nThat if you ge...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f7825cb-2a29-427b-98dd-55fe6bd194ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f7825cb-2a29-427b-98dd-55fe6bd194ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f7825cb-2a29-427b-98dd-55fe6bd194ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainDf.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnUjwrxJFvjm",
        "outputId": "413c882c-5132-44e4-8a20-9117d5cc7213"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
              "       'identity_hate'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targetList = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
        "       'identity_hate']"
      ],
      "metadata": {
        "id": "cpEPv1v_GF7s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "VALID_BATCH_SIZE = 64\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-05"
      ],
      "metadata": {
        "id": "MAhFi-W4GPZs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the tokenizer \n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "2e2defe5456e494da2028148d4166063",
            "fb01f46602e44013ac2ae892dc79f508",
            "99e4a813ec3c4f4dad9a74b5b2f186a0",
            "838f3bc32c764a55a46ae57fceeb01fe",
            "b14f31575ea8484ab2dfd750c1df5e02",
            "eff29395bb1746e992837556a9d4896d",
            "d356eba98293465e895023b9f8d5f710",
            "70ae43856efa43beb58bd8d1c929497d",
            "7add36acfdcd487db259799a146e99e9",
            "a1a39bf952ef41dabdc9dd459e697930",
            "f8d04b6f6d804ebeb03bdadc0bc1f4fa",
            "2ad04a9b3eeb4c97baf1a5ba9fcb8dc9",
            "c58f0ff09e644f20a383940501615d23",
            "21f4af300b2a4e9f9b59099a63bf917e",
            "8fb6a71af4ae4b028bcbab1c3eae5f12",
            "1341805d91464e42a7516600f5eac8f6",
            "7c312fdab19b4ef08b9e2ae12be8dc93",
            "a4a2a235f574472ba50e9fc200216222",
            "4635ccaef14c4e45a11665c43ecdd2f9",
            "d1dfa479e1d84478b4b5c5fad150be3e",
            "237aed63c8d8463eb7b3c506f0e447c0",
            "2bd07662bc5743cf9e7db57649ace178",
            "e12cae7f9f6845849c65e3ea423c28ce",
            "f81ee31c1f0247718419b007dfc0819b",
            "736e838498ce45bfb48c043f9c30fd36",
            "c5debad22af94fdea6f00d0963ad3144",
            "ee6c8ebc600e4334b0b22b6a36d8ff16",
            "8aa36a8b0fae43e18466333805473af5",
            "7a51b1e20bac42cc8c9604cd8d6a6f19",
            "01d350c5048d475399873b84567fa5fe",
            "92506b5f721448c6a64c091916faa19f",
            "1f462075e12b4909ac04b8ab2528e296",
            "deb52ce608c445c9863021232f32a158"
          ]
        },
        "id": "x-6T9mBLGPWJ",
        "outputId": "ef9dd076-4524-4e27-96bf-fa154b37c97c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e2defe5456e494da2028148d4166063"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ad04a9b3eeb4c97baf1a5ba9fcb8dc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e12cae7f9f6845849c65e3ea423c28ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \" my name is human ai. dbdbdf\"\n",
        "# add [CLS] [PAD] [SEP]\n",
        "# word to index \n",
        "# \n",
        "encodins = tokenizer.encode_plus(\n",
        "    example_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=20, \n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "encodins"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKOD4_B2G3Z1",
        "outputId": "17099d17-963d-4243-9ea5-8294c0cb5463"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2026,  2171,  2003,  2529,  9932,  1012, 16962, 18939, 20952,\n",
              "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['comment_text']\n",
        "        self.targets = self.df[targetList].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get the example \n",
        "        title = str(self.title[index])\n",
        "        # seperate it by white space \n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ],
      "metadata": {
        "id": "V4HGZtZtGPT5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.8\n",
        "trainDf = trainDf.sample(frac=train_size, random_state=200).reset_index(drop=True)\n",
        "valDf = trainDf.drop(trainDf.index).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "wJ44PEK3GPRR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(trainDf, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(valDf, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "iVFIMXLDGPOz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False, # we dont want to suffle our validation dataset \n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "QOp3WJHiGPMm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtQnA_qoJgpX",
        "outputId": "88450001-5e69-43e1-b76f-82c7383d153c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "metadata": {
        "id": "SfRt1nUyJgl7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        # bert output will be 768 \n",
        "        # 5 is the number of classes\n",
        "        self.linear = torch.nn.Linear(768, 6)\n",
        "    \n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output = self.bert_model(\n",
        "            input_ids, \n",
        "            attention_mask=attn_mask, \n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3715c806cbbc4190b336ae70d194d7e2",
            "fd5cdeaa8c804b66a2f3a772cdd992a7",
            "7bfd55f0523b4972825e6b59ae9b8c9a",
            "39a5b8bc73954e6d90e92c37f9aa99b2",
            "ce1f863c49f8432291298ac1765917ba",
            "8ccac31742004312a2a6f7a939f310d4",
            "1c61bc2939fe40f7b8a197443134a487",
            "ee67e18201524e8bb6f37a0502e2dc1a",
            "4c13e27e603d4be2ad3eb518da658420",
            "f11e12381b3d429e95ae0aa842e33d8e",
            "f9bed9be3bf44844bf5eb9b492bdbc49"
          ]
        },
        "id": "p4DdTewbJgjZ",
        "outputId": "9adf26e1-dea3-43b3-95f3-be2ca3723ac3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3715c806cbbc4190b336ae70d194d7e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (bert_model): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (linear): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "Zn48P1PYJggu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_targets=[]\n",
        "val_outputs=[]"
      ],
      "metadata": {
        "id": "wA99O72BJgeE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "   \n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "   \n",
        " \n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      #print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "        \n",
        "        # save checkpoint\n",
        "      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "        \n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "2ls08JpUJzVH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = \"/content/drive/MyDrive/ColabNotebooks/mltc/curr_ckpt\"\n",
        "best_model_path = \"/content/drive/MyDrive/ColabNotebooks/mltc/best_model.pt\""
      ],
      "metadata": {
        "id": "VAaIGuFYJzRs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG5ElB6HJzQY",
        "outputId": "dd6c2545-3c21-440a-8d23-093cbe862785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############# Epoch 1: Training Start   #############\n",
            "before loss data in training 0.705392062664032 0\n",
            "after loss data in training 0.705392062664032 0.705392062664032\n",
            "before loss data in training 0.678778886795044 0.705392062664032\n",
            "after loss data in training 0.678778886795044 0.692085474729538\n",
            "before loss data in training 0.6452531814575195 0.692085474729538\n",
            "after loss data in training 0.6452531814575195 0.6764747103055319\n",
            "before loss data in training 0.6291648149490356 0.6764747103055319\n",
            "after loss data in training 0.6291648149490356 0.6646472364664078\n",
            "before loss data in training 0.618580162525177 0.6646472364664078\n",
            "after loss data in training 0.618580162525177 0.6554338216781617\n",
            "before loss data in training 0.5956538915634155 0.6554338216781617\n",
            "after loss data in training 0.5956538915634155 0.6454704999923706\n",
            "before loss data in training 0.5954684019088745 0.6454704999923706\n",
            "after loss data in training 0.5954684019088745 0.6383273431232998\n",
            "before loss data in training 0.5899028182029724 0.6383273431232998\n",
            "after loss data in training 0.5899028182029724 0.6322742775082588\n",
            "before loss data in training 0.566857099533081 0.6322742775082588\n",
            "after loss data in training 0.566857099533081 0.6250057021776835\n",
            "before loss data in training 0.5509060621261597 0.6250057021776835\n",
            "after loss data in training 0.5509060621261597 0.6175957381725311\n",
            "before loss data in training 0.5538092851638794 0.6175957381725311\n",
            "after loss data in training 0.5538092851638794 0.6117969697171991\n",
            "before loss data in training 0.5243701934814453 0.6117969697171991\n",
            "after loss data in training 0.5243701934814453 0.6045114050308863\n",
            "before loss data in training 0.5101951360702515 0.6045114050308863\n",
            "after loss data in training 0.5101951360702515 0.5972563074185298\n",
            "before loss data in training 0.5247586369514465 0.5972563074185298\n",
            "after loss data in training 0.5247586369514465 0.5920779023851667\n",
            "before loss data in training 0.49574869871139526 0.5920779023851667\n",
            "after loss data in training 0.49574869871139526 0.585655955473582\n",
            "before loss data in training 0.49592727422714233 0.585655955473582\n",
            "after loss data in training 0.49592727422714233 0.5800479128956795\n",
            "before loss data in training 0.5072911977767944 0.5800479128956795\n",
            "after loss data in training 0.5072911977767944 0.5757681061239803\n",
            "before loss data in training 0.47154784202575684 0.5757681061239803\n",
            "after loss data in training 0.47154784202575684 0.5699780914518568\n",
            "before loss data in training 0.4512931704521179 0.5699780914518568\n",
            "after loss data in training 0.4512931704521179 0.563731516662397\n",
            "before loss data in training 0.43485337495803833 0.563731516662397\n",
            "after loss data in training 0.43485337495803833 0.557287609577179\n",
            "before loss data in training 0.4307982921600342 0.557287609577179\n",
            "after loss data in training 0.4307982921600342 0.5512643087477912\n",
            "before loss data in training 0.4405451714992523 0.5512643087477912\n",
            "after loss data in training 0.4405451714992523 0.5462316206910395\n",
            "before loss data in training 0.42140573263168335 0.5462316206910395\n",
            "after loss data in training 0.42140573263168335 0.5408044081667196\n",
            "before loss data in training 0.4109140634536743 0.5408044081667196\n",
            "after loss data in training 0.4109140634536743 0.5353923104703427\n",
            "before loss data in training 0.41490504145622253 0.5353923104703427\n",
            "after loss data in training 0.41490504145622253 0.530572819709778\n",
            "before loss data in training 0.38052961230278015 0.530572819709778\n",
            "after loss data in training 0.38052961230278015 0.5248019271172012\n",
            "before loss data in training 0.37545761466026306 0.5248019271172012\n",
            "after loss data in training 0.37545761466026306 0.5192706562854628\n",
            "before loss data in training 0.3661218583583832 0.5192706562854628\n",
            "after loss data in training 0.3661218583583832 0.5138010563594957\n",
            "before loss data in training 0.35335129499435425 0.5138010563594957\n",
            "after loss data in training 0.35335129499435425 0.5082683059675943\n",
            "before loss data in training 0.34858930110931396 0.5082683059675943\n",
            "after loss data in training 0.34858930110931396 0.5029456724723183\n",
            "before loss data in training 0.3434300422668457 0.5029456724723183\n",
            "after loss data in training 0.3434300422668457 0.49780000698181914\n",
            "before loss data in training 0.33502137660980225 0.49780000698181914\n",
            "after loss data in training 0.33502137660980225 0.4927131747826936\n",
            "before loss data in training 0.33130598068237305 0.4927131747826936\n",
            "after loss data in training 0.33130598068237305 0.4878220476887445\n",
            "before loss data in training 0.3306942880153656 0.4878220476887445\n",
            "after loss data in training 0.3306942880153656 0.4832006429924686\n",
            "before loss data in training 0.31218284368515015 0.4832006429924686\n",
            "after loss data in training 0.31218284368515015 0.47831442015511666\n",
            "before loss data in training 0.3126257061958313 0.47831442015511666\n",
            "after loss data in training 0.3126257061958313 0.47371195587846987\n",
            "before loss data in training 0.32156461477279663 0.47371195587846987\n",
            "after loss data in training 0.32156461477279663 0.46959986557831657\n",
            "before loss data in training 0.3100288510322571 0.46959986557831657\n",
            "after loss data in training 0.3100288510322571 0.4654006283534203\n",
            "before loss data in training 0.3543550670146942 0.4654006283534203\n",
            "after loss data in training 0.3543550670146942 0.46255330626781194\n",
            "before loss data in training 0.29455721378326416 0.46255330626781194\n",
            "after loss data in training 0.29455721378326416 0.45835340395569824\n",
            "before loss data in training 0.29729777574539185 0.45835340395569824\n",
            "after loss data in training 0.29729777574539185 0.4544252179017883\n",
            "before loss data in training 0.3130491375923157 0.4544252179017883\n",
            "after loss data in training 0.3130491375923157 0.45105912075156274\n",
            "before loss data in training 0.31821179389953613 0.45105912075156274\n",
            "after loss data in training 0.31821179389953613 0.44796964803407374\n",
            "before loss data in training 0.28176653385162354 0.44796964803407374\n",
            "after loss data in training 0.28176653385162354 0.44419230452992714\n",
            "before loss data in training 0.29422134160995483 0.44419230452992714\n",
            "after loss data in training 0.29422134160995483 0.44085961646503885\n",
            "before loss data in training 0.2877383828163147 0.44085961646503885\n",
            "after loss data in training 0.2877383828163147 0.4375308939944144\n",
            "before loss data in training 0.2602165937423706 0.4375308939944144\n",
            "after loss data in training 0.2602165937423706 0.4337582493082007\n",
            "before loss data in training 0.2730083465576172 0.4337582493082007\n",
            "after loss data in training 0.2730083465576172 0.4304092930008969\n",
            "before loss data in training 0.2932083010673523 0.4304092930008969\n",
            "after loss data in training 0.2932083010673523 0.4276092727573552\n",
            "before loss data in training 0.2894667387008667 0.4276092727573552\n",
            "after loss data in training 0.2894667387008667 0.42484642207622547\n",
            "before loss data in training 0.23085856437683105 0.42484642207622547\n",
            "after loss data in training 0.23085856437683105 0.4210427385919236\n",
            "before loss data in training 0.24428527057170868 0.4210427385919236\n",
            "after loss data in training 0.24428527057170868 0.41764355651461177\n",
            "before loss data in training 0.23137149214744568 0.41764355651461177\n",
            "after loss data in training 0.23137149214744568 0.4141289892624011\n",
            "before loss data in training 0.23280131816864014 0.4141289892624011\n",
            "after loss data in training 0.23280131816864014 0.41077106942733144\n",
            "before loss data in training 0.2141267955303192 0.41077106942733144\n",
            "after loss data in training 0.2141267955303192 0.4071957189928403\n",
            "before loss data in training 0.23144567012786865 0.4071957189928403\n",
            "after loss data in training 0.23144567012786865 0.40405732526310867\n",
            "before loss data in training 0.2514362931251526 0.40405732526310867\n",
            "after loss data in training 0.2514362931251526 0.4013797632957761\n",
            "before loss data in training 0.2402329295873642 0.4013797632957761\n",
            "after loss data in training 0.2402329295873642 0.39860136961114834\n",
            "before loss data in training 0.2048674076795578 0.39860136961114834\n",
            "after loss data in training 0.2048674076795578 0.39531774313773155\n",
            "before loss data in training 0.2206999510526657 0.39531774313773155\n",
            "after loss data in training 0.2206999510526657 0.39240744660298044\n",
            "before loss data in training 0.20080044865608215 0.39240744660298044\n",
            "after loss data in training 0.20080044865608215 0.3892663482759821\n",
            "before loss data in training 0.24627956748008728 0.3892663482759821\n",
            "after loss data in training 0.24627956748008728 0.38696010987604834\n",
            "before loss data in training 0.2052323818206787 0.38696010987604834\n",
            "after loss data in training 0.2052323818206787 0.38407554276405836\n",
            "before loss data in training 0.19202081859111786 0.38407554276405836\n",
            "after loss data in training 0.19202081859111786 0.38107468769885616\n",
            "before loss data in training 0.2378482073545456 0.38107468769885616\n",
            "after loss data in training 0.2378482073545456 0.3788712033858668\n",
            "before loss data in training 0.22145912051200867 0.3788712033858668\n",
            "after loss data in training 0.22145912051200867 0.376486171827172\n",
            "before loss data in training 0.22351384162902832 0.376486171827172\n",
            "after loss data in training 0.22351384162902832 0.374203002719737\n",
            "before loss data in training 0.18511861562728882 0.374203002719737\n",
            "after loss data in training 0.18511861562728882 0.3714223499683775\n",
            "before loss data in training 0.20050877332687378 0.3714223499683775\n",
            "after loss data in training 0.20050877332687378 0.36894534161125425\n",
            "before loss data in training 0.19456493854522705 0.36894534161125425\n",
            "after loss data in training 0.19456493854522705 0.3664541929960253\n",
            "before loss data in training 0.17741498351097107 0.3664541929960253\n",
            "after loss data in training 0.17741498351097107 0.363791668918771\n",
            "before loss data in training 0.2524319887161255 0.363791668918771\n",
            "after loss data in training 0.2524319887161255 0.3622450066937343\n",
            "before loss data in training 0.21929776668548584 0.3622450066937343\n",
            "after loss data in training 0.21929776668548584 0.3602868253237583\n",
            "before loss data in training 0.17452667653560638 0.3602868253237583\n",
            "after loss data in training 0.17452667653560638 0.3577765530428373\n",
            "before loss data in training 0.15818631649017334 0.3577765530428373\n",
            "after loss data in training 0.15818631649017334 0.3551153498888018\n",
            "before loss data in training 0.19547852873802185 0.3551153498888018\n",
            "after loss data in training 0.19547852873802185 0.35301486539997573\n",
            "before loss data in training 0.22086262702941895 0.35301486539997573\n",
            "after loss data in training 0.22086262702941895 0.3512986025639945\n",
            "before loss data in training 0.16289739310741425 0.3512986025639945\n",
            "after loss data in training 0.16289739310741425 0.3488832024427563\n",
            "before loss data in training 0.18873807787895203 0.3488832024427563\n",
            "after loss data in training 0.18873807787895203 0.3468560489672651\n",
            "before loss data in training 0.17639568448066711 0.3468560489672651\n",
            "after loss data in training 0.17639568448066711 0.3447252944111826\n",
            "before loss data in training 0.19237351417541504 0.3447252944111826\n",
            "after loss data in training 0.19237351417541504 0.3428444082354324\n",
            "before loss data in training 0.21690699458122253 0.3428444082354324\n",
            "after loss data in training 0.21690699458122253 0.3413085861176981\n",
            "before loss data in training 0.2152477204799652 0.3413085861176981\n",
            "after loss data in training 0.2152477204799652 0.33978978050760494\n",
            "before loss data in training 0.1960238814353943 0.33978978050760494\n",
            "after loss data in training 0.1960238814353943 0.33807828170912624\n",
            "before loss data in training 0.19319021701812744 0.33807828170912624\n",
            "after loss data in training 0.19319021701812744 0.3363737162421733\n",
            "before loss data in training 0.18189272284507751 0.3363737162421733\n",
            "after loss data in training 0.18189272284507751 0.33457742562127685\n",
            "before loss data in training 0.17754682898521423 0.33457742562127685\n",
            "after loss data in training 0.17754682898521423 0.33277247623465545\n",
            "before loss data in training 0.17107217013835907 0.33277247623465545\n",
            "after loss data in training 0.17107217013835907 0.3309349727562884\n",
            "before loss data in training 0.15720292925834656 0.3309349727562884\n",
            "after loss data in training 0.15720292925834656 0.32898292732372725\n",
            "before loss data in training 0.16934339702129364 0.32898292732372725\n",
            "after loss data in training 0.16934339702129364 0.3272091547648113\n",
            "before loss data in training 0.15229377150535583 0.3272091547648113\n",
            "after loss data in training 0.15229377150535583 0.3252870076960261\n",
            "before loss data in training 0.18311923742294312 0.3252870076960261\n",
            "after loss data in training 0.18311923742294312 0.3237417058452317\n",
            "before loss data in training 0.14642956852912903 0.3237417058452317\n",
            "after loss data in training 0.14642956852912903 0.32183512372355316\n",
            "before loss data in training 0.16610978543758392 0.32183512372355316\n",
            "after loss data in training 0.16610978543758392 0.32017847118859605\n",
            "before loss data in training 0.21441516280174255 0.32017847118859605\n",
            "after loss data in training 0.21441516280174255 0.31906517320557654\n",
            "before loss data in training 0.15833690762519836 0.31906517320557654\n",
            "after loss data in training 0.15833690762519836 0.31739092043911427\n",
            "before loss data in training 0.12587407231330872 0.31739092043911427\n",
            "after loss data in training 0.12587407231330872 0.31541651994297193\n",
            "before loss data in training 0.1988217532634735 0.31541651994297193\n",
            "after loss data in training 0.1988217532634735 0.3142267774258342\n",
            "before loss data in training 0.13468578457832336 0.3142267774258342\n",
            "after loss data in training 0.13468578457832336 0.3124132320435361\n",
            "before loss data in training 0.15719197690486908 0.3124132320435361\n",
            "after loss data in training 0.15719197690486908 0.31086101949214945\n",
            "before loss data in training 0.13185465335845947 0.31086101949214945\n",
            "after loss data in training 0.13185465335845947 0.30908867923340005\n",
            "before loss data in training 0.18809100985527039 0.30908867923340005\n",
            "after loss data in training 0.18809100985527039 0.30790242757283015\n",
            "before loss data in training 0.13592347502708435 0.30790242757283015\n",
            "after loss data in training 0.13592347502708435 0.30623272900442483\n",
            "before loss data in training 0.13934144377708435 0.30623272900442483\n",
            "after loss data in training 0.13934144377708435 0.3046280051080081\n",
            "before loss data in training 0.11066167801618576 0.3046280051080081\n",
            "after loss data in training 0.11066167801618576 0.3027807067547526\n",
            "before loss data in training 0.15957316756248474 0.3027807067547526\n",
            "after loss data in training 0.15957316756248474 0.30142969223407085\n",
            "before loss data in training 0.19564616680145264 0.30142969223407085\n",
            "after loss data in training 0.19564616680145264 0.3004410611552613\n",
            "before loss data in training 0.1631702035665512 0.3004410611552613\n",
            "after loss data in training 0.1631702035665512 0.2991700346961066\n",
            "before loss data in training 0.11770342290401459 0.2991700346961066\n",
            "after loss data in training 0.11770342290401459 0.2975052033952617\n",
            "before loss data in training 0.11275893449783325 0.2975052033952617\n",
            "after loss data in training 0.11275893449783325 0.29582569185983054\n",
            "before loss data in training 0.13043496012687683 0.29582569185983054\n",
            "after loss data in training 0.13043496012687683 0.29433568526764176\n",
            "before loss data in training 0.14431671798229218 0.29433568526764176\n",
            "after loss data in training 0.14431671798229218 0.29299623020259397\n",
            "before loss data in training 0.1386251002550125 0.29299623020259397\n",
            "after loss data in training 0.1386251002550125 0.2916301140083676\n",
            "before loss data in training 0.14411801099777222 0.2916301140083676\n",
            "after loss data in training 0.14411801099777222 0.29033614819248516\n",
            "before loss data in training 0.14508646726608276 0.29033614819248516\n",
            "after loss data in training 0.14508646726608276 0.2890731074887773\n",
            "before loss data in training 0.12010320276021957 0.2890731074887773\n",
            "after loss data in training 0.12010320276021957 0.28761647037904836\n",
            "before loss data in training 0.17014816403388977 0.28761647037904836\n",
            "after loss data in training 0.17014816403388977 0.2866124677607137\n",
            "before loss data in training 0.1457596719264984 0.2866124677607137\n",
            "after loss data in training 0.1457596719264984 0.2854187999994068\n",
            "before loss data in training 0.14363768696784973 0.2854187999994068\n",
            "after loss data in training 0.14363768696784973 0.28422736207477184\n",
            "before loss data in training 0.1793278455734253 0.28422736207477184\n",
            "after loss data in training 0.1793278455734253 0.2833531994372606\n",
            "before loss data in training 0.18497532606124878 0.2833531994372606\n",
            "after loss data in training 0.18497532606124878 0.28254015916142583\n",
            "before loss data in training 0.16804192960262299 0.28254015916142583\n",
            "after loss data in training 0.16804192960262299 0.281601649083075\n",
            "before loss data in training 0.11703287065029144 0.281601649083075\n",
            "after loss data in training 0.11703287065029144 0.280263691534841\n",
            "before loss data in training 0.14756350219249725 0.280263691534841\n",
            "after loss data in training 0.14756350219249725 0.27919352871756403\n",
            "before loss data in training 0.1453479826450348 0.27919352871756403\n",
            "after loss data in training 0.1453479826450348 0.2781227643489838\n",
            "before loss data in training 0.11514320969581604 0.2781227643489838\n",
            "after loss data in training 0.11514320969581604 0.2768292758199904\n",
            "before loss data in training 0.18993084132671356 0.2768292758199904\n",
            "after loss data in training 0.18993084132671356 0.27614503617831104\n",
            "before loss data in training 0.14294515550136566 0.27614503617831104\n",
            "after loss data in training 0.14294515550136566 0.2751044121105224\n",
            "before loss data in training 0.12274544686079025 0.2751044121105224\n",
            "after loss data in training 0.12274544686079025 0.27392333486052445\n",
            "before loss data in training 0.10867287963628769 0.27392333486052445\n",
            "after loss data in training 0.10867287963628769 0.2726521775126457\n",
            "before loss data in training 0.1389334350824356 0.2726521775126457\n",
            "after loss data in training 0.1389334350824356 0.27163142375363647\n",
            "before loss data in training 0.11597663164138794 0.27163142375363647\n",
            "after loss data in training 0.11597663164138794 0.2704522207830891\n",
            "before loss data in training 0.15041717886924744 0.2704522207830891\n",
            "after loss data in training 0.15041717886924744 0.269549701670955\n",
            "before loss data in training 0.09750854969024658 0.269549701670955\n",
            "after loss data in training 0.09750854969024658 0.2682658124770691\n",
            "before loss data in training 0.13157424330711365 0.2682658124770691\n",
            "after loss data in training 0.13157424330711365 0.26725328233506945\n",
            "before loss data in training 0.10091543197631836 0.26725328233506945\n",
            "after loss data in training 0.10091543197631836 0.266030209905961\n",
            "before loss data in training 0.12452101707458496 0.266030209905961\n",
            "after loss data in training 0.12452101707458496 0.2649972960896736\n",
            "before loss data in training 0.11672817170619965 0.2649972960896736\n",
            "after loss data in training 0.11672817170619965 0.2639228821448658\n",
            "before loss data in training 0.11051924526691437 0.2639228821448658\n",
            "after loss data in training 0.11051924526691437 0.2628192588579741\n",
            "before loss data in training 0.16399693489074707 0.2628192588579741\n",
            "after loss data in training 0.16399693489074707 0.26211338511535104\n",
            "before loss data in training 0.0974876880645752 0.26211338511535104\n",
            "after loss data in training 0.0974876880645752 0.26094582698023916\n",
            "before loss data in training 0.12363831698894501 0.26094582698023916\n",
            "after loss data in training 0.12363831698894501 0.2599788726845258\n",
            "before loss data in training 0.12365250289440155 0.2599788726845258\n",
            "after loss data in training 0.12365250289440155 0.2590255414272522\n",
            "before loss data in training 0.11987616121768951 0.2590255414272522\n",
            "after loss data in training 0.11987616121768951 0.25805922628690803\n",
            "before loss data in training 0.13699689507484436 0.25805922628690803\n",
            "after loss data in training 0.13699689507484436 0.2572243136578593\n",
            "before loss data in training 0.1174408420920372 0.2572243136578593\n",
            "after loss data in training 0.1174408420920372 0.25626689261973723\n",
            "before loss data in training 0.1106596440076828 0.25626689261973723\n",
            "after loss data in training 0.1106596440076828 0.25527636711897495\n",
            "before loss data in training 0.14203163981437683 0.25527636711897495\n",
            "after loss data in training 0.14203163981437683 0.25451120004259253\n",
            "before loss data in training 0.12499713897705078 0.25451120004259253\n",
            "after loss data in training 0.12499713897705078 0.2536419781562466\n",
            "before loss data in training 0.08082661032676697 0.2536419781562466\n",
            "after loss data in training 0.08082661032676697 0.25248987570405007\n",
            "before loss data in training 0.14027324318885803 0.25248987570405007\n",
            "after loss data in training 0.14027324318885803 0.25174671919732694\n",
            "before loss data in training 0.10221828520298004 0.25174671919732694\n",
            "after loss data in training 0.10221828520298004 0.2507629794999957\n",
            "before loss data in training 0.1053413450717926 0.2507629794999957\n",
            "after loss data in training 0.1053413450717926 0.24981251130111856\n",
            "before loss data in training 0.13617411255836487 0.24981251130111856\n",
            "after loss data in training 0.13617411255836487 0.2490745996209708\n",
            "before loss data in training 0.12969031929969788 0.2490745996209708\n",
            "after loss data in training 0.12969031929969788 0.24830437845760775\n",
            "before loss data in training 0.08581322431564331 0.24830437845760775\n",
            "after loss data in training 0.08581322431564331 0.24726276849515927\n",
            "before loss data in training 0.11572699248790741 0.24726276849515927\n",
            "after loss data in training 0.11572699248790741 0.2464249610046672\n",
            "before loss data in training 0.13865041732788086 0.2464249610046672\n",
            "after loss data in training 0.13865041732788086 0.24574284363962426\n",
            "before loss data in training 0.1263723373413086 0.24574284363962426\n",
            "after loss data in training 0.1263723373413086 0.244992085738377\n",
            "before loss data in training 0.09894909709692001 0.244992085738377\n",
            "after loss data in training 0.09894909709692001 0.2440793170593679\n",
            "before loss data in training 0.11407634615898132 0.2440793170593679\n",
            "after loss data in training 0.11407634615898132 0.2432718451904214\n",
            "before loss data in training 0.07562781870365143 0.2432718451904214\n",
            "after loss data in training 0.07562781870365143 0.24223700552074998\n",
            "before loss data in training 0.07451337575912476 0.24223700552074998\n",
            "after loss data in training 0.07451337575912476 0.24120802619705903\n",
            "before loss data in training 0.12671151757240295 0.24120802619705903\n",
            "after loss data in training 0.12671151757240295 0.24050987675422575\n",
            "before loss data in training 0.12044745683670044 0.24050987675422575\n",
            "after loss data in training 0.12044745683670044 0.23978222572442257\n",
            "before loss data in training 0.07686442136764526 0.23978222572442257\n",
            "after loss data in training 0.07686442136764526 0.23880079316805644\n",
            "before loss data in training 0.1071133017539978 0.23880079316805644\n",
            "after loss data in training 0.1071133017539978 0.23801224531527765\n",
            "before loss data in training 0.10587134212255478 0.23801224531527765\n",
            "after loss data in training 0.10587134212255478 0.23722569232008286\n",
            "before loss data in training 0.09745147079229355 0.23722569232008286\n",
            "after loss data in training 0.09745147079229355 0.23639862592051014\n",
            "before loss data in training 0.118289053440094 0.23639862592051014\n",
            "after loss data in training 0.118289053440094 0.23570386372944888\n",
            "before loss data in training 0.08004836738109589 0.23570386372944888\n",
            "after loss data in training 0.08004836738109589 0.23479359766893218\n",
            "before loss data in training 0.08749356120824814 0.23479359766893218\n",
            "after loss data in training 0.08749356120824814 0.23393720210811425\n",
            "before loss data in training 0.07138508558273315 0.23393720210811425\n",
            "after loss data in training 0.07138508558273315 0.23299759449814095\n",
            "before loss data in training 0.10642194747924805 0.23299759449814095\n",
            "after loss data in training 0.10642194747924805 0.23227014825090594\n",
            "before loss data in training 0.13586965203285217 0.23227014825090594\n",
            "after loss data in training 0.13586965203285217 0.23171928827251706\n",
            "before loss data in training 0.10692914575338364 0.23171928827251706\n",
            "after loss data in training 0.10692914575338364 0.23101025337184017\n",
            "before loss data in training 0.08660534024238586 0.23101025337184017\n",
            "after loss data in training 0.08660534024238586 0.2301944064050071\n",
            "before loss data in training 0.08673477172851562 0.2301944064050071\n",
            "after loss data in training 0.08673477172851562 0.2293884534012066\n",
            "before loss data in training 0.08426829427480698 0.2293884534012066\n",
            "after loss data in training 0.08426829427480698 0.22857772625524905\n",
            "before loss data in training 0.07320759445428848 0.22857772625524905\n",
            "after loss data in training 0.07320759445428848 0.22771455885635483\n",
            "before loss data in training 0.1026729866862297 0.22771455885635483\n",
            "after loss data in training 0.1026729866862297 0.22702372144105026\n",
            "before loss data in training 0.07910575717687607 0.22702372144105026\n",
            "after loss data in training 0.07910575717687607 0.22621098537366469\n",
            "before loss data in training 0.0990770161151886 0.22621098537366469\n",
            "after loss data in training 0.0990770161151886 0.22551626423017573\n",
            "before loss data in training 0.12799474596977234 0.22551626423017573\n",
            "after loss data in training 0.12799474596977234 0.22498625597876049\n",
            "before loss data in training 0.13828930258750916 0.22498625597876049\n",
            "after loss data in training 0.13828930258750916 0.22451762379826723\n",
            "before loss data in training 0.06906729191541672 0.22451762379826723\n",
            "after loss data in training 0.06906729191541672 0.2236818693257788\n",
            "before loss data in training 0.07829785346984863 0.2236818693257788\n",
            "after loss data in training 0.07829785346984863 0.22290441469553318\n",
            "before loss data in training 0.13882309198379517 0.22290441469553318\n",
            "after loss data in training 0.13882309198379517 0.22245717361727926\n",
            "before loss data in training 0.06573672592639923 0.22245717361727926\n",
            "after loss data in training 0.06573672592639923 0.2216279648993381\n",
            "before loss data in training 0.07705911248922348 0.2216279648993381\n",
            "after loss data in training 0.07705911248922348 0.22086707620244275\n",
            "before loss data in training 0.06277471780776978 0.22086707620244275\n",
            "after loss data in training 0.06277471780776978 0.22003936751974812\n",
            "before loss data in training 0.13582536578178406 0.22003936751974812\n",
            "after loss data in training 0.13582536578178406 0.2196007529273629\n",
            "before loss data in training 0.12558361887931824 0.2196007529273629\n",
            "after loss data in training 0.12558361887931824 0.21911361751778752\n",
            "before loss data in training 0.12710970640182495 0.21911361751778752\n",
            "after loss data in training 0.12710970640182495 0.21863937055327223\n",
            "before loss data in training 0.07024791836738586 0.21863937055327223\n",
            "after loss data in training 0.07024791836738586 0.21787838874719076\n",
            "before loss data in training 0.06725633889436722 0.21787838874719076\n",
            "after loss data in training 0.06725633889436722 0.21710990890100287\n",
            "before loss data in training 0.11296334862709045 0.21710990890100287\n",
            "after loss data in training 0.11296334862709045 0.2165812461584957\n",
            "before loss data in training 0.1075868010520935 0.2165812461584957\n",
            "after loss data in training 0.1075868010520935 0.2160307691630088\n",
            "before loss data in training 0.06840786337852478 0.2160307691630088\n",
            "after loss data in training 0.06840786337852478 0.2152889455158506\n",
            "before loss data in training 0.06163863465189934 0.2152889455158506\n",
            "after loss data in training 0.06163863465189934 0.21452069396153084\n",
            "before loss data in training 0.1005479097366333 0.21452069396153084\n",
            "after loss data in training 0.1005479097366333 0.21395366518429254\n",
            "before loss data in training 0.09628865122795105 0.21395366518429254\n",
            "after loss data in training 0.09628865122795105 0.21337116511520174\n",
            "before loss data in training 0.14038871228694916 0.21337116511520174\n",
            "after loss data in training 0.14038871228694916 0.21301164564314137\n",
            "before loss data in training 0.10109474509954453 0.21301164564314137\n",
            "after loss data in training 0.10109474509954453 0.21246303338557473\n",
            "before loss data in training 0.07103918492794037 0.21246303338557473\n",
            "after loss data in training 0.07103918492794037 0.21177316095407409\n",
            "before loss data in training 0.11346980184316635 0.21177316095407409\n",
            "after loss data in training 0.11346980184316635 0.21129596018169103\n",
            "before loss data in training 0.08717765659093857 0.21129596018169103\n",
            "after loss data in training 0.08717765659093857 0.21069635485033475\n",
            "before loss data in training 0.08096807450056076 0.21069635485033475\n",
            "after loss data in training 0.08096807450056076 0.210072661194807\n",
            "before loss data in training 0.04697713255882263 0.210072661194807\n",
            "after loss data in training 0.04697713255882263 0.2092922998137736\n",
            "before loss data in training 0.11168011277914047 0.2092922998137736\n",
            "after loss data in training 0.11168011277914047 0.20882747987551342\n",
            "before loss data in training 0.07218524813652039 0.20882747987551342\n",
            "after loss data in training 0.07218524813652039 0.20817988636016274\n",
            "before loss data in training 0.06970744580030441 0.20817988636016274\n",
            "after loss data in training 0.06970744580030441 0.20752671447072946\n",
            "before loss data in training 0.04923417419195175 0.20752671447072946\n",
            "after loss data in training 0.04923417419195175 0.2067835570046319\n",
            "before loss data in training 0.07098324596881866 0.2067835570046319\n",
            "after loss data in training 0.07098324596881866 0.2061489761119412\n",
            "before loss data in training 0.08785708248615265 0.2061489761119412\n",
            "after loss data in training 0.08785708248615265 0.20559878125786776\n",
            "before loss data in training 0.058388978242874146 0.20559878125786776\n",
            "after loss data in training 0.058388978242874146 0.2049172543920576\n",
            "before loss data in training 0.09876681864261627 0.2049172543920576\n",
            "after loss data in training 0.09876681864261627 0.20442808187708322\n",
            "before loss data in training 0.07419076561927795 0.20442808187708322\n",
            "after loss data in training 0.07419076561927795 0.2038306629951667\n",
            "before loss data in training 0.07503028213977814 0.2038306629951667\n",
            "after loss data in training 0.07503028213977814 0.2032425334022197\n",
            "before loss data in training 0.05636458843946457 0.2032425334022197\n",
            "after loss data in training 0.05636458843946457 0.20257490637966172\n",
            "before loss data in training 0.0642411857843399 0.20257490637966172\n",
            "after loss data in training 0.0642411857843399 0.2019489619425788\n",
            "before loss data in training 0.09277933090925217 0.2019489619425788\n",
            "after loss data in training 0.09277933090925217 0.20145720684783408\n",
            "before loss data in training 0.06005019694566727 0.20145720684783408\n",
            "after loss data in training 0.06005019694566727 0.20082309469580642\n",
            "before loss data in training 0.06651323288679123 0.20082309469580642\n",
            "after loss data in training 0.06651323288679123 0.20022349709844475\n",
            "before loss data in training 0.09619050472974777 0.20022349709844475\n",
            "after loss data in training 0.09619050472974777 0.19976112824347278\n",
            "before loss data in training 0.10327857732772827 0.19976112824347278\n",
            "after loss data in training 0.10327857732772827 0.19933421430136772\n",
            "before loss data in training 0.04997213929891586 0.19933421430136772\n",
            "after loss data in training 0.04997213929891586 0.19867623159210582\n",
            "before loss data in training 0.10519421100616455 0.19867623159210582\n",
            "after loss data in training 0.10519421100616455 0.19826622272988678\n",
            "before loss data in training 0.09015359729528427 0.19826622272988678\n",
            "after loss data in training 0.09015359729528427 0.19779411519523787\n",
            "before loss data in training 0.06055857986211777 0.19779411519523787\n",
            "after loss data in training 0.06055857986211777 0.1971974389546591\n",
            "before loss data in training 0.09178822487592697 0.1971974389546591\n",
            "after loss data in training 0.09178822487592697 0.19674112201059532\n",
            "before loss data in training 0.058944351971149445 0.19674112201059532\n",
            "after loss data in training 0.058944351971149445 0.1961471704155977\n",
            "before loss data in training 0.048622600734233856 0.1961471704155977\n",
            "after loss data in training 0.048622600734233856 0.19551401775602104\n",
            "before loss data in training 0.08771048486232758 0.19551401775602104\n",
            "after loss data in training 0.08771048486232758 0.19505331889750097\n",
            "before loss data in training 0.07524183392524719 0.19505331889750097\n",
            "after loss data in training 0.07524183392524719 0.19454348279123607\n",
            "before loss data in training 0.12007160484790802 0.19454348279123607\n",
            "after loss data in training 0.12007160484790802 0.19422792398639147\n",
            "before loss data in training 0.05369687080383301 0.19422792398639147\n",
            "after loss data in training 0.05369687080383301 0.19363496595608531\n",
            "before loss data in training 0.07366073131561279 0.19363496595608531\n",
            "after loss data in training 0.07366073131561279 0.1931308725332262\n",
            "before loss data in training 0.07426461577415466 0.1931308725332262\n",
            "after loss data in training 0.07426461577415466 0.19263352417858573\n",
            "before loss data in training 0.08173364400863647 0.19263352417858573\n",
            "after loss data in training 0.08173364400863647 0.19217144134454428\n",
            "before loss data in training 0.07592255622148514 0.19217144134454428\n",
            "after loss data in training 0.07592255622148514 0.19168908082536146\n",
            "before loss data in training 0.09288384020328522 0.19168908082536146\n",
            "after loss data in training 0.09288384020328522 0.19128079470708842\n",
            "before loss data in training 0.10984780639410019 0.19128079470708842\n",
            "after loss data in training 0.10984780639410019 0.19094567952884567\n",
            "before loss data in training 0.09984539449214935 0.19094567952884567\n",
            "after loss data in training 0.09984539449214935 0.19057231770492478\n",
            "before loss data in training 0.09952694177627563 0.19057231770492478\n",
            "after loss data in training 0.09952694177627563 0.19020070392562416\n",
            "before loss data in training 0.08036146312952042 0.19020070392562416\n",
            "after loss data in training 0.08036146312952042 0.18975420294677822\n",
            "before loss data in training 0.06943340599536896 0.18975420294677822\n",
            "after loss data in training 0.06943340599536896 0.18926707421418143\n",
            "before loss data in training 0.06863227486610413 0.18926707421418143\n",
            "after loss data in training 0.06863227486610413 0.18878064357164887\n",
            "before loss data in training 0.08400215953588486 0.18878064357164887\n",
            "after loss data in training 0.08400215953588486 0.18835984644700723\n",
            "before loss data in training 0.05218806862831116 0.18835984644700723\n",
            "after loss data in training 0.05218806862831116 0.18781515933573245\n",
            "before loss data in training 0.06135594844818115 0.18781515933573245\n",
            "after loss data in training 0.06135594844818115 0.18731133777841152\n",
            "before loss data in training 0.10463511198759079 0.18731133777841152\n",
            "after loss data in training 0.10463511198759079 0.18698325751733683\n",
            "before loss data in training 0.06497739255428314 0.18698325751733683\n",
            "after loss data in training 0.06497739255428314 0.1865010208969295\n",
            "before loss data in training 0.06218261644244194 0.1865010208969295\n",
            "after loss data in training 0.06218261644244194 0.1860115783597071\n",
            "before loss data in training 0.05385425314307213 0.1860115783597071\n",
            "after loss data in training 0.05385425314307213 0.1854933143392497\n",
            "before loss data in training 0.06450287997722626 0.1854933143392497\n",
            "after loss data in training 0.06450287997722626 0.18502069545502306\n",
            "before loss data in training 0.05660942196846008 0.18502069545502306\n",
            "after loss data in training 0.05660942196846008 0.18452104069437494\n",
            "before loss data in training 0.0681857019662857 0.18452104069437494\n",
            "after loss data in training 0.0681857019662857 0.1840701285287622\n",
            "before loss data in training 0.08837249130010605 0.1840701285287622\n",
            "after loss data in training 0.08837249130010605 0.18370063958193342\n",
            "before loss data in training 0.07801678031682968 0.18370063958193342\n",
            "after loss data in training 0.07801678031682968 0.18329416320014455\n",
            "before loss data in training 0.060277681797742844 0.18329416320014455\n",
            "after loss data in training 0.060277681797742844 0.1828228356851928\n",
            "before loss data in training 0.038138363510370255 0.1828228356851928\n",
            "after loss data in training 0.038138363510370255 0.18227060487536523\n",
            "before loss data in training 0.0753941610455513 0.18227060487536523\n",
            "after loss data in training 0.0753941610455513 0.18186423056422524\n",
            "before loss data in training 0.054948173463344574 0.18186423056422524\n",
            "after loss data in training 0.054948173463344574 0.1813834879236916\n",
            "before loss data in training 0.07906284183263779 0.1813834879236916\n",
            "after loss data in training 0.07906284183263779 0.18099737227806498\n",
            "before loss data in training 0.1021495908498764 0.18099737227806498\n",
            "after loss data in training 0.1021495908498764 0.18070095204713194\n",
            "before loss data in training 0.059364136308431625 0.18070095204713194\n",
            "after loss data in training 0.059364136308431625 0.18024650704436526\n",
            "before loss data in training 0.058730125427246094 0.18024650704436526\n",
            "after loss data in training 0.058730125427246094 0.17979308770997302\n",
            "before loss data in training 0.05415024608373642 0.17979308770997302\n",
            "after loss data in training 0.05415024608373642 0.1793260139492807\n",
            "before loss data in training 0.06964471936225891 0.1793260139492807\n",
            "after loss data in training 0.06964471936225891 0.1789197869322917\n",
            "before loss data in training 0.04579371213912964 0.1789197869322917\n",
            "after loss data in training 0.04579371213912964 0.17842854680390366\n",
            "before loss data in training 0.07282007485628128 0.17842854680390366\n",
            "after loss data in training 0.07282007485628128 0.17804028036291975\n",
            "before loss data in training 0.05995279178023338 0.17804028036291975\n",
            "after loss data in training 0.05995279178023338 0.17760772545968645\n",
            "before loss data in training 0.047090254724025726 0.17760772545968645\n",
            "after loss data in training 0.047090254724025726 0.1771313843256147\n",
            "before loss data in training 0.047384098172187805 0.1771313843256147\n",
            "after loss data in training 0.047384098172187805 0.1766595760123295\n",
            "before loss data in training 0.059534117579460144 0.1766595760123295\n",
            "after loss data in training 0.059534117579460144 0.17623520840931187\n",
            "before loss data in training 0.05286741256713867 0.17623520840931187\n",
            "after loss data in training 0.05286741256713867 0.1757898373051885\n",
            "before loss data in training 0.11140909790992737 0.1757898373051885\n",
            "after loss data in training 0.11140909790992737 0.17555825191168037\n",
            "before loss data in training 0.11001050472259521 0.17555825191168037\n",
            "after loss data in training 0.11001050472259521 0.17532331374971233\n",
            "before loss data in training 0.04898504912853241 0.17532331374971233\n",
            "after loss data in training 0.04898504912853241 0.17487210566177955\n",
            "before loss data in training 0.09333932399749756 0.17487210566177955\n",
            "after loss data in training 0.09333932399749756 0.17458195341386396\n",
            "before loss data in training 0.09644944965839386 0.17458195341386396\n",
            "after loss data in training 0.09644944965839386 0.1743048877977098\n",
            "before loss data in training 0.07382279634475708 0.1743048877977098\n",
            "after loss data in training 0.07382279634475708 0.17394982740388312\n",
            "before loss data in training 0.060631029307842255 0.17394982740388312\n",
            "after loss data in training 0.060631029307842255 0.17355081755143228\n",
            "before loss data in training 0.057658277451992035 0.17355081755143228\n",
            "after loss data in training 0.057658277451992035 0.1731441770598553\n",
            "before loss data in training 0.0848965272307396 0.1731441770598553\n",
            "after loss data in training 0.0848965272307396 0.17283561884366958\n",
            "before loss data in training 0.06340405344963074 0.17283561884366958\n",
            "after loss data in training 0.06340405344963074 0.17245432419072868\n",
            "before loss data in training 0.07340704649686813 0.17245432419072868\n",
            "after loss data in training 0.07340704649686813 0.17211041003206945\n",
            "before loss data in training 0.057262517511844635 0.17211041003206945\n",
            "after loss data in training 0.057262517511844635 0.17171301248009635\n",
            "before loss data in training 0.07508793473243713 0.17171301248009635\n",
            "after loss data in training 0.07508793473243713 0.17137982255682857\n",
            "before loss data in training 0.08126045763492584 0.17137982255682857\n",
            "after loss data in training 0.08126045763492584 0.17107013401757803\n",
            "before loss data in training 0.06294384598731995 0.17107013401757803\n",
            "after loss data in training 0.06294384598731995 0.1706998385106251\n",
            "before loss data in training 0.03727497532963753 0.1706998385106251\n",
            "after loss data in training 0.03727497532963753 0.170244463550963\n",
            "before loss data in training 0.08308275789022446 0.170244463550963\n",
            "after loss data in training 0.08308275789022446 0.16994799516436185\n",
            "before loss data in training 0.047244202345609665 0.16994799516436185\n",
            "after loss data in training 0.047244202345609665 0.1695320501039593\n",
            "before loss data in training 0.04215993732213974 0.1695320501039593\n",
            "after loss data in training 0.04215993732213974 0.16910173891212882\n",
            "before loss data in training 0.06316574662923813 0.16910173891212882\n",
            "after loss data in training 0.06316574662923813 0.16874505206942547\n",
            "before loss data in training 0.07359491288661957 0.16874505206942547\n",
            "after loss data in training 0.07359491288661957 0.16842575630035567\n",
            "before loss data in training 0.07062526792287827 0.16842575630035567\n",
            "after loss data in training 0.07062526792287827 0.16809866436598284\n",
            "before loss data in training 0.08953005075454712 0.16809866436598284\n",
            "after loss data in training 0.08953005075454712 0.16783676898727806\n",
            "before loss data in training 0.09458925575017929 0.16783676898727806\n",
            "after loss data in training 0.09458925575017929 0.16759342176722125\n",
            "before loss data in training 0.05696174129843712 0.16759342176722125\n",
            "after loss data in training 0.05696174129843712 0.16722709169944383\n",
            "before loss data in training 0.0786731168627739 0.16722709169944383\n",
            "after loss data in training 0.0786731168627739 0.16693483435674855\n",
            "before loss data in training 0.07359777390956879 0.16693483435674855\n",
            "after loss data in training 0.07359777390956879 0.16662780455264598\n",
            "before loss data in training 0.043576180934906006 0.16662780455264598\n",
            "after loss data in training 0.043576180934906006 0.16622435660635831\n",
            "before loss data in training 0.08399663865566254 0.16622435660635831\n",
            "after loss data in training 0.08399663865566254 0.16595563857383971\n",
            "before loss data in training 0.06238504871726036 0.16595563857383971\n",
            "after loss data in training 0.06238504871726036 0.16561827508896487\n",
            "before loss data in training 0.08045832812786102 0.16561827508896487\n",
            "after loss data in training 0.08045832812786102 0.16534178175467557\n",
            "before loss data in training 0.0381007194519043 0.16534178175467557\n",
            "after loss data in training 0.0381007194519043 0.16492999838152744\n",
            "before loss data in training 0.06971687078475952 0.16492999838152744\n",
            "after loss data in training 0.06971687078475952 0.16462285926024756\n",
            "before loss data in training 0.049815889447927475 0.16462285926024756\n",
            "after loss data in training 0.049815889447927475 0.16425370501647804\n",
            "before loss data in training 0.040289506316185 0.16425370501647804\n",
            "after loss data in training 0.040289506316185 0.16385638386679763\n",
            "before loss data in training 0.06591512262821198 0.16385638386679763\n",
            "after loss data in training 0.06591512262821198 0.16354347248903855\n",
            "before loss data in training 0.061837226152420044 0.16354347248903855\n",
            "after loss data in training 0.061837226152420044 0.16321956724592832\n",
            "before loss data in training 0.0780796930193901 0.16321956724592832\n",
            "after loss data in training 0.0780796930193901 0.16294928193092342\n",
            "before loss data in training 0.12182348221540451 0.16294928193092342\n",
            "after loss data in training 0.12182348221540451 0.16281913699511483\n",
            "before loss data in training 0.045073505491018295 0.16281913699511483\n",
            "after loss data in training 0.045073505491018295 0.16244769967175807\n",
            "before loss data in training 0.05130657181143761 0.16244769967175807\n",
            "after loss data in training 0.05130657181143761 0.16209819926968158\n",
            "before loss data in training 0.06744368374347687 0.16209819926968158\n",
            "after loss data in training 0.06744368374347687 0.16180147665047717\n",
            "before loss data in training 0.04758884757757187 0.16180147665047717\n",
            "after loss data in training 0.04758884757757187 0.16144456218462433\n",
            "before loss data in training 0.07178264856338501 0.16144456218462433\n",
            "after loss data in training 0.07178264856338501 0.1611652415814429\n",
            "before loss data in training 0.09193110466003418 0.1611652415814429\n",
            "after loss data in training 0.09193110466003418 0.1609502287338609\n",
            "before loss data in training 0.06738246977329254 0.1609502287338609\n",
            "after loss data in training 0.06738246977329254 0.16066054526958667\n",
            "before loss data in training 0.07250647991895676 0.16066054526958667\n",
            "after loss data in training 0.07250647991895676 0.16038846482097363\n",
            "before loss data in training 0.07802967727184296 0.16038846482097363\n",
            "after loss data in training 0.07802967727184296 0.1601350531669763\n",
            "before loss data in training 0.08164184540510178 0.1601350531669763\n",
            "after loss data in training 0.08164184540510178 0.1598942764560503\n",
            "before loss data in training 0.09783130139112473 0.1598942764560503\n",
            "after loss data in training 0.09783130139112473 0.15970448142527072\n",
            "before loss data in training 0.06194370239973068 0.15970448142527072\n",
            "after loss data in training 0.06194370239973068 0.15940643026970505\n",
            "before loss data in training 0.06875599920749664 0.15940643026970505\n",
            "after loss data in training 0.06875599920749664 0.15913089704459196\n",
            "before loss data in training 0.06549778580665588 0.15913089704459196\n",
            "after loss data in training 0.06549778580665588 0.15884716034387095\n",
            "before loss data in training 0.04710138961672783 0.15884716034387095\n",
            "after loss data in training 0.04710138961672783 0.15850955982807896\n",
            "before loss data in training 0.04460713267326355 0.15850955982807896\n",
            "after loss data in training 0.04460713267326355 0.15816648022821506\n",
            "before loss data in training 0.0782381147146225 0.15816648022821506\n",
            "after loss data in training 0.0782381147146225 0.15792645510655262\n",
            "before loss data in training 0.0667015090584755 0.15792645510655262\n",
            "after loss data in training 0.0667015090584755 0.15765332652557035\n",
            "before loss data in training 0.048918865621089935 0.15765332652557035\n",
            "after loss data in training 0.048918865621089935 0.15732874604525846\n",
            "before loss data in training 0.0657169446349144 0.15732874604525846\n",
            "after loss data in training 0.0657169446349144 0.15705609187439434\n",
            "before loss data in training 0.11110993474721909 0.15705609187439434\n",
            "after loss data in training 0.11110993474721909 0.15691975312920986\n",
            "before loss data in training 0.07923086732625961 0.15691975312920986\n",
            "after loss data in training 0.07923086732625961 0.15668990435464492\n",
            "before loss data in training 0.05723357945680618 0.15668990435464492\n",
            "after loss data in training 0.05723357945680618 0.15639652286527075\n",
            "before loss data in training 0.11374622583389282 0.15639652286527075\n",
            "after loss data in training 0.11374622583389282 0.15627108081517846\n",
            "before loss data in training 0.0713886022567749 0.15627108081517846\n",
            "after loss data in training 0.0713886022567749 0.156022158590667\n",
            "before loss data in training 0.06207532808184624 0.156022158590667\n",
            "after loss data in training 0.06207532808184624 0.1557474602558459\n",
            "before loss data in training 0.06679178029298782 0.1557474602558459\n",
            "after loss data in training 0.06679178029298782 0.1554881142501233\n",
            "before loss data in training 0.09569712728261948 0.1554881142501233\n",
            "after loss data in training 0.09569712728261948 0.15531430324149684\n",
            "before loss data in training 0.05123975872993469 0.15531430324149684\n",
            "after loss data in training 0.05123975872993469 0.1550126378950865\n",
            "before loss data in training 0.10266747325658798 0.1550126378950865\n",
            "after loss data in training 0.10266747325658798 0.1548613512920851\n",
            "before loss data in training 0.07784084975719452 0.1548613512920851\n",
            "after loss data in training 0.07784084975719452 0.1546393901925609\n",
            "before loss data in training 0.05996806547045708 0.1546393901925609\n",
            "after loss data in training 0.05996806547045708 0.15436734615600314\n",
            "before loss data in training 0.08406554907560349 0.15436734615600314\n",
            "after loss data in training 0.08406554907560349 0.15416590834201918\n",
            "before loss data in training 0.06494797766208649 0.15416590834201918\n",
            "after loss data in training 0.06494797766208649 0.15391099996864793\n",
            "before loss data in training 0.03890378773212433 0.15391099996864793\n",
            "after loss data in training 0.03890378773212433 0.1535833440933302\n",
            "before loss data in training 0.05503381788730621 0.1535833440933302\n",
            "after loss data in training 0.05503381788730621 0.1533033738484267\n",
            "before loss data in training 0.03969364985823631 0.1533033738484267\n",
            "after loss data in training 0.03969364985823631 0.1529815332705508\n",
            "before loss data in training 0.04556986689567566 0.1529815332705508\n",
            "after loss data in training 0.04556986689567566 0.1526781104841811\n",
            "before loss data in training 0.06486882269382477 0.1526781104841811\n",
            "after loss data in training 0.06486882269382477 0.15243076037772937\n",
            "before loss data in training 0.07025550305843353 0.15243076037772937\n",
            "after loss data in training 0.07025550305843353 0.15219993100323698\n",
            "before loss data in training 0.07892784476280212 0.15219993100323698\n",
            "after loss data in training 0.07892784476280212 0.15199468706418814\n",
            "before loss data in training 0.07993745803833008 0.15199468706418814\n",
            "after loss data in training 0.07993745803833008 0.15179340988813825\n",
            "before loss data in training 0.08601325005292892 0.15179340988813825\n",
            "after loss data in training 0.08601325005292892 0.15161017824514322\n",
            "before loss data in training 0.051688842475414276 0.15161017824514322\n",
            "after loss data in training 0.051688842475414276 0.1513326189791162\n",
            "before loss data in training 0.09228720515966415 0.1513326189791162\n",
            "after loss data in training 0.09228720515966415 0.15116905827601523\n",
            "before loss data in training 0.06504286825656891 0.15116905827601523\n",
            "after loss data in training 0.06504286825656891 0.15093114062402782\n",
            "before loss data in training 0.06566454470157623 0.15093114062402782\n",
            "after loss data in training 0.06566454470157623 0.1506962464203847\n",
            "before loss data in training 0.07154211401939392 0.1506962464203847\n",
            "after loss data in training 0.07154211401939392 0.15047879001268968\n",
            "before loss data in training 0.105586938560009 0.15047879001268968\n",
            "after loss data in training 0.105586938560009 0.1503557986388467\n",
            "before loss data in training 0.060176871716976166 0.1503557986388467\n",
            "after loss data in training 0.060176871716976166 0.15010940812813123\n",
            "before loss data in training 0.04646287485957146 0.15010940812813123\n",
            "after loss data in training 0.04646287485957146 0.14982699250614606\n",
            "before loss data in training 0.09342734515666962 0.14982699250614606\n",
            "after loss data in training 0.09342734515666962 0.14967373259487032\n",
            "before loss data in training 0.07417144626379013 0.14967373259487032\n",
            "after loss data in training 0.07417144626379013 0.14946911935278068\n",
            "before loss data in training 0.07949671149253845 0.14946911935278068\n",
            "after loss data in training 0.07949671149253845 0.1492800047369422\n",
            "before loss data in training 0.06859106570482254 0.1492800047369422\n",
            "after loss data in training 0.06859106570482254 0.14906251433523837\n",
            "before loss data in training 0.03428705781698227 0.14906251433523837\n",
            "after loss data in training 0.03428705781698227 0.1487539781618022\n",
            "before loss data in training 0.07058723270893097 0.1487539781618022\n",
            "after loss data in training 0.07058723270893097 0.1485444158415532\n",
            "before loss data in training 0.10967530310153961 0.1485444158415532\n",
            "after loss data in training 0.10967530310153961 0.1484404877326227\n",
            "before loss data in training 0.07829976081848145 0.1484404877326227\n",
            "after loss data in training 0.07829976081848145 0.14825344579418498\n",
            "before loss data in training 0.05839211866259575 0.14825344579418498\n",
            "after loss data in training 0.05839211866259575 0.14801445290287757\n",
            "before loss data in training 0.05837111920118332 0.14801445290287757\n",
            "after loss data in training 0.05837111920118332 0.14777667217687837\n",
            "before loss data in training 0.06290073692798615 0.14777667217687837\n",
            "after loss data in training 0.06290073692798615 0.1475521326656379\n",
            "before loss data in training 0.04993348568677902 0.1475521326656379\n",
            "after loss data in training 0.04993348568677902 0.14729456367624777\n",
            "before loss data in training 0.047328680753707886 0.14729456367624777\n",
            "after loss data in training 0.047328680753707886 0.14703149556329373\n",
            "before loss data in training 0.09420736879110336 0.14703149556329373\n",
            "after loss data in training 0.09420736879110336 0.14689284956126697\n",
            "before loss data in training 0.0645216703414917 0.14689284956126697\n",
            "after loss data in training 0.0645216703414917 0.1466772182020529\n",
            "before loss data in training 0.038314126431941986 0.1466772182020529\n",
            "after loss data in training 0.038314126431941986 0.14639428584756176\n",
            "before loss data in training 0.06268090009689331 0.14639428584756176\n",
            "after loss data in training 0.06268090009689331 0.14617628223883605\n",
            "before loss data in training 0.0629534125328064 0.14617628223883605\n",
            "after loss data in training 0.0629534125328064 0.1459601189408983\n",
            "before loss data in training 0.03744686767458916 0.1459601189408983\n",
            "after loss data in training 0.03744686767458916 0.14567899652829128\n",
            "before loss data in training 0.03798792511224747 0.14567899652829128\n",
            "after loss data in training 0.03798792511224747 0.14540072502592424\n",
            "before loss data in training 0.07581499218940735 0.14540072502592424\n",
            "after loss data in training 0.07581499218940735 0.1452213803536652\n",
            "before loss data in training 0.04573711007833481 0.1452213803536652\n",
            "after loss data in training 0.04573711007833481 0.14496563672827875\n",
            "before loss data in training 0.10894034802913666 0.14496563672827875\n",
            "after loss data in training 0.10894034802913666 0.14487326419315275\n",
            "before loss data in training 0.05667669326066971 0.14487326419315275\n",
            "after loss data in training 0.05667669326066971 0.14464769751557607\n",
            "before loss data in training 0.03501194715499878 0.14464769751557607\n",
            "after loss data in training 0.03501194715499878 0.14436801447894196\n",
            "before loss data in training 0.0607055127620697 0.14436801447894196\n",
            "after loss data in training 0.0607055127620697 0.14415513279518402\n",
            "before loss data in training 0.08230271935462952 0.14415513279518402\n",
            "after loss data in training 0.08230271935462952 0.1439981469742689\n",
            "before loss data in training 0.04954908415675163 0.1439981469742689\n",
            "after loss data in training 0.04954908415675163 0.14375903542283217\n",
            "before loss data in training 0.03955197334289551 0.14375903542283217\n",
            "after loss data in training 0.03955197334289551 0.14349588627616566\n",
            "before loss data in training 0.07237079739570618 0.14349588627616566\n",
            "after loss data in training 0.07237079739570618 0.14331672988100078\n",
            "before loss data in training 0.0624232143163681 0.14331672988100078\n",
            "after loss data in training 0.0624232143163681 0.14311347984189365\n",
            "before loss data in training 0.0534028559923172 0.14311347984189365\n",
            "after loss data in training 0.0534028559923172 0.14288864118562905\n",
            "before loss data in training 0.060876794159412384 0.14288864118562905\n",
            "after loss data in training 0.060876794159412384 0.1426836115680635\n",
            "before loss data in training 0.05332864820957184 0.1426836115680635\n",
            "after loss data in training 0.05332864820957184 0.14246078123549868\n",
            "before loss data in training 0.04920842498540878 0.14246078123549868\n",
            "after loss data in training 0.04920842498540878 0.1422288102000507\n",
            "before loss data in training 0.05830556899309158 0.1422288102000507\n",
            "after loss data in training 0.05830556899309158 0.14202056394395404\n",
            "before loss data in training 0.06831156462430954 0.14202056394395404\n",
            "after loss data in training 0.06831156462430954 0.14183811592583612\n",
            "before loss data in training 0.07288342714309692 0.14183811592583612\n",
            "after loss data in training 0.07288342714309692 0.14166785743501453\n",
            "before loss data in training 0.08932679891586304 0.14166785743501453\n",
            "after loss data in training 0.08932679891586304 0.14153893857166686\n",
            "before loss data in training 0.04062574729323387 0.14153893857166686\n",
            "after loss data in training 0.04062574729323387 0.1412909946127518\n",
            "before loss data in training 0.057264477014541626 0.1412909946127518\n",
            "after loss data in training 0.057264477014541626 0.14108504726569737\n",
            "before loss data in training 0.06668882071971893 0.14108504726569737\n",
            "after loss data in training 0.06668882071971893 0.14090314940128176\n",
            "before loss data in training 0.08289362490177155 0.14090314940128176\n",
            "after loss data in training 0.08289362490177155 0.140761662756161\n",
            "before loss data in training 0.057723864912986755 0.140761662756161\n",
            "after loss data in training 0.057723864912986755 0.14055962431858637\n",
            "before loss data in training 0.042550235986709595 0.14055962431858637\n",
            "after loss data in training 0.042550235986709595 0.1403217374537032\n",
            "before loss data in training 0.028762906789779663 0.1403217374537032\n",
            "after loss data in training 0.028762906789779663 0.1400516192196501\n",
            "before loss data in training 0.07679372280836105 0.1400516192196501\n",
            "after loss data in training 0.07679372280836105 0.13989882236841508\n",
            "before loss data in training 0.05353408679366112 0.13989882236841508\n",
            "after loss data in training 0.05353408679366112 0.13969071457184942\n",
            "before loss data in training 0.07056435942649841 0.13969071457184942\n",
            "after loss data in training 0.07056435942649841 0.13952454544890386\n",
            "before loss data in training 0.09766324609518051 0.13952454544890386\n",
            "after loss data in training 0.09766324609518051 0.13942415863990212\n",
            "before loss data in training 0.06652317941188812 0.13942415863990212\n",
            "after loss data in training 0.06652317941188812 0.13924975438337578\n",
            "before loss data in training 0.07074947655200958 0.13924975438337578\n",
            "after loss data in training 0.07074947655200958 0.13908626923342024\n",
            "before loss data in training 0.03297165781259537 0.13908626923342024\n",
            "after loss data in training 0.03297165781259537 0.138833615396704\n",
            "before loss data in training 0.02190404385328293 0.138833615396704\n",
            "after loss data in training 0.02190404385328293 0.13855587294648208\n",
            "before loss data in training 0.04523938149213791 0.13855587294648208\n",
            "after loss data in training 0.04523938149213791 0.13833474381981303\n",
            "before loss data in training 0.04668433219194412 0.13833474381981303\n",
            "after loss data in training 0.04668433219194412 0.13811807618003083\n",
            "before loss data in training 0.04387675225734711 0.13811807618003083\n",
            "after loss data in training 0.04387675225734711 0.13789580890662828\n",
            "before loss data in training 0.09185564517974854 0.13789580890662828\n",
            "after loss data in training 0.09185564517974854 0.13778747910962386\n",
            "before loss data in training 0.039871446788311005 0.13778747910962386\n",
            "after loss data in training 0.039871446788311005 0.13755762926849402\n",
            "before loss data in training 0.07033389806747437 0.13755762926849402\n",
            "after loss data in training 0.07033389806747437 0.1374001966427305\n",
            "before loss data in training 0.03215179219841957 0.1374001966427305\n",
            "after loss data in training 0.03215179219841957 0.1371542891557111\n",
            "before loss data in training 0.04604536294937134 0.1371542891557111\n",
            "after loss data in training 0.04604536294937134 0.136941914036349\n",
            "before loss data in training 0.08099040389060974 0.136941914036349\n",
            "after loss data in training 0.08099040389060974 0.1368117942453124\n",
            "before loss data in training 0.07573435455560684 0.1368117942453124\n",
            "after loss data in training 0.07573435455560684 0.13667008324835253\n",
            "before loss data in training 0.05500216409564018 0.13667008324835253\n",
            "after loss data in training 0.05500216409564018 0.13648103713920273\n",
            "before loss data in training 0.04639914631843567 0.13648103713920273\n",
            "after loss data in training 0.04639914631843567 0.13627299582090996\n",
            "before loss data in training 0.06311290711164474 0.13627299582090996\n",
            "after loss data in training 0.06311290711164474 0.1361044241879393\n",
            "before loss data in training 0.05050463601946831 0.1361044241879393\n",
            "after loss data in training 0.05050463601946831 0.13590764306571293\n",
            "before loss data in training 0.06664213538169861 0.13590764306571293\n",
            "after loss data in training 0.06664213538169861 0.13574877722240097\n",
            "before loss data in training 0.05777829885482788 0.13574877722240097\n",
            "after loss data in training 0.05777829885482788 0.13557035507510676\n",
            "before loss data in training 0.06872285902500153 0.13557035507510676\n",
            "after loss data in training 0.06872285902500153 0.13541773522111109\n",
            "before loss data in training 0.08015488088130951 0.13541773522111109\n",
            "after loss data in training 0.08015488088130951 0.1352918517260318\n",
            "before loss data in training 0.04786490648984909 0.1352918517260318\n",
            "after loss data in training 0.04786490648984909 0.13509315412322231\n",
            "before loss data in training 0.08383163064718246 0.13509315412322231\n",
            "after loss data in training 0.08383163064718246 0.13497691484096372\n",
            "before loss data in training 0.07552777975797653 0.13497691484096372\n",
            "after loss data in training 0.07552777975797653 0.1348424145353461\n",
            "before loss data in training 0.03186134248971939 0.1348424145353461\n",
            "after loss data in training 0.03186134248971939 0.13460995161876455\n",
            "before loss data in training 0.06069988012313843 0.13460995161876455\n",
            "after loss data in training 0.06069988012313843 0.13444348749377438\n",
            "before loss data in training 0.03618647903203964 0.13444348749377438\n",
            "after loss data in training 0.03618647903203964 0.13422268522756825\n",
            "before loss data in training 0.06910295784473419 0.13422268522756825\n",
            "after loss data in training 0.06910295784473419 0.13407667687020763\n",
            "before loss data in training 0.046778928488492966 0.13407667687020763\n",
            "after loss data in training 0.046778928488492966 0.13388137989396218\n",
            "before loss data in training 0.06346005946397781 0.13388137989396218\n",
            "after loss data in training 0.06346005946397781 0.13372418944657383\n",
            "before loss data in training 0.0529661700129509 0.13372418944657383\n",
            "after loss data in training 0.0529661700129509 0.13354432748792433\n",
            "before loss data in training 0.106513112783432 0.13354432748792433\n",
            "after loss data in training 0.106513112783432 0.13348425812191433\n",
            "before loss data in training 0.0642596185207367 0.13348425812191433\n",
            "after loss data in training 0.0642596185207367 0.13333076668155697\n",
            "before loss data in training 0.04309585690498352 0.13333076668155697\n",
            "after loss data in training 0.04309585690498352 0.1331311319254141\n",
            "before loss data in training 0.053996264934539795 0.1331311319254141\n",
            "after loss data in training 0.053996264934539795 0.1329564412698051\n",
            "before loss data in training 0.09508376568555832 0.1329564412698051\n",
            "after loss data in training 0.09508376568555832 0.13287302127953143\n",
            "before loss data in training 0.038823869079351425 0.13287302127953143\n",
            "after loss data in training 0.038823869079351425 0.13266631984612445\n",
            "before loss data in training 0.04572128504514694 0.13266631984612445\n",
            "after loss data in training 0.04572128504514694 0.1324756509101574\n",
            "before loss data in training 0.05412060767412186 0.1324756509101574\n",
            "after loss data in training 0.05412060767412186 0.13230419567331705\n",
            "before loss data in training 0.09681789577007294 0.13230419567331705\n",
            "after loss data in training 0.09681789577007294 0.1322267146691615\n",
            "before loss data in training 0.038976263254880905 0.1322267146691615\n",
            "after loss data in training 0.038976263254880905 0.13202355464429377\n",
            "before loss data in training 0.0708969309926033 0.13202355464429377\n",
            "after loss data in training 0.0708969309926033 0.13189067067983357\n",
            "before loss data in training 0.028442129492759705 0.13189067067983357\n",
            "after loss data in training 0.028442129492759705 0.13166627037357093\n",
            "before loss data in training 0.08171595633029938 0.13166627037357093\n",
            "after loss data in training 0.08171595633029938 0.1315581528107067\n",
            "before loss data in training 0.06607259064912796 0.1315581528107067\n",
            "after loss data in training 0.06607259064912796 0.13141671531143764\n",
            "before loss data in training 0.048055749386548996 0.13141671531143764\n",
            "after loss data in training 0.048055749386548996 0.13123705805728916\n",
            "before loss data in training 0.05045377463102341 0.13123705805728916\n",
            "after loss data in training 0.05045377463102341 0.13106333056604988\n",
            "before loss data in training 0.06548450887203217 0.13106333056604988\n",
            "after loss data in training 0.06548450887203217 0.1309226034808696\n",
            "before loss data in training 0.05334740877151489 0.1309226034808696\n",
            "after loss data in training 0.05334740877151489 0.13075648957356906\n",
            "before loss data in training 0.05060067027807236 0.13075648957356906\n",
            "after loss data in training 0.05060067027807236 0.1305852164554163\n",
            "before loss data in training 0.05617610737681389 0.1305852164554163\n",
            "after loss data in training 0.05617610737681389 0.13042656163861757\n",
            "before loss data in training 0.05727746710181236 0.13042656163861757\n",
            "after loss data in training 0.05727746710181236 0.13027092526726267\n",
            "before loss data in training 0.0632084310054779 0.13027092526726267\n",
            "after loss data in training 0.0632084310054779 0.13012854205226948\n",
            "before loss data in training 0.05348769575357437 0.13012854205226948\n",
            "after loss data in training 0.05348769575357437 0.12996616737790784\n",
            "before loss data in training 0.0653393417596817 0.12996616737790784\n",
            "after loss data in training 0.0653393417596817 0.1298295356112731\n",
            "before loss data in training 0.060894668102264404 0.1298295356112731\n",
            "after loss data in training 0.060894668102264404 0.12968410340133849\n",
            "before loss data in training 0.06726881861686707 0.12968410340133849\n",
            "after loss data in training 0.06726881861686707 0.12955270280179224\n",
            "before loss data in training 0.04413646087050438 0.12955270280179224\n",
            "after loss data in training 0.04413646087050438 0.12937325691538196\n",
            "before loss data in training 0.09301751852035522 0.12937325691538196\n",
            "after loss data in training 0.09301751852035522 0.12929703943446996\n",
            "before loss data in training 0.027555081993341446 0.12929703943446996\n",
            "after loss data in training 0.027555081993341446 0.12908419015112033\n",
            "before loss data in training 0.053115807473659515 0.12908419015112033\n",
            "after loss data in training 0.053115807473659515 0.12892559227496697\n",
            "before loss data in training 0.07249689102172852 0.12892559227496697\n",
            "after loss data in training 0.07249689102172852 0.1288080324806894\n",
            "before loss data in training 0.0504404753446579 0.1288080324806894\n",
            "after loss data in training 0.0504404753446579 0.12864510616647726\n",
            "before loss data in training 0.04267100989818573 0.12864510616647726\n",
            "after loss data in training 0.04267100989818573 0.1284667366721447\n",
            "before loss data in training 0.07269258797168732 0.1284667366721447\n",
            "after loss data in training 0.07269258797168732 0.12835126224419344\n",
            "before loss data in training 0.06376879662275314 0.12835126224419344\n",
            "after loss data in training 0.06376879662275314 0.12821782739786813\n",
            "before loss data in training 0.05642601102590561 0.12821782739786813\n",
            "after loss data in training 0.05642601102590561 0.1280698030342146\n",
            "before loss data in training 0.07998526096343994 0.1280698030342146\n",
            "after loss data in training 0.07998526096343994 0.1279708636472377\n",
            "before loss data in training 0.06117809936404228 0.1279708636472377\n",
            "after loss data in training 0.06117809936404228 0.1278337121805371\n",
            "before loss data in training 0.030672740191221237 0.1278337121805371\n",
            "after loss data in training 0.030672740191221237 0.1276346118281\n",
            "before loss data in training 0.04955911263823509 0.1276346118281\n",
            "after loss data in training 0.04955911263823509 0.1274749482305747\n",
            "before loss data in training 0.05499720573425293 0.1274749482305747\n",
            "after loss data in training 0.05499720573425293 0.12732703447037813\n",
            "before loss data in training 0.10072048753499985 0.12732703447037813\n",
            "after loss data in training 0.10072048753499985 0.12727284598374802\n",
            "before loss data in training 0.06650993973016739 0.12727284598374802\n",
            "after loss data in training 0.06650993973016739 0.1271493441417692\n",
            "before loss data in training 0.046262022107839584 0.1271493441417692\n",
            "after loss data in training 0.046262022107839584 0.12698527249464156\n",
            "before loss data in training 0.04575031250715256 0.12698527249464156\n",
            "after loss data in training 0.04575031250715256 0.1268208292558005\n",
            "before loss data in training 0.07785776257514954 0.1268208292558005\n",
            "after loss data in training 0.07785776257514954 0.12672191396957694\n",
            "before loss data in training 0.0787941962480545 0.12672191396957694\n",
            "after loss data in training 0.0787941962480545 0.1266252855064287\n",
            "before loss data in training 0.07854906469583511 0.1266252855064287\n",
            "after loss data in training 0.07854906469583511 0.1265285526677756\n",
            "before loss data in training 0.0683254599571228 0.1265285526677756\n",
            "after loss data in training 0.0683254599571228 0.12641167898763372\n",
            "before loss data in training 0.05845459923148155 0.12641167898763372\n",
            "after loss data in training 0.05845459923148155 0.12627549245505626\n",
            "before loss data in training 0.02765689417719841 0.12627549245505626\n",
            "after loss data in training 0.02765689417719841 0.12607825525850055\n",
            "before loss data in training 0.04462306946516037 0.12607825525850055\n",
            "after loss data in training 0.04462306946516037 0.12591567005731624\n",
            "before loss data in training 0.04233615845441818 0.12591567005731624\n",
            "after loss data in training 0.04233615845441818 0.12574917700631447\n",
            "before loss data in training 0.041494812816381454 0.12574917700631447\n",
            "after loss data in training 0.041494812816381454 0.12558167330017145\n",
            "before loss data in training 0.0406404510140419 0.12558167330017145\n",
            "after loss data in training 0.0406404510140419 0.12541313912896881\n",
            "before loss data in training 0.04882153123617172 0.12541313912896881\n",
            "after loss data in training 0.04882153123617172 0.12526147257868606\n",
            "before loss data in training 0.05032488703727722 0.12526147257868606\n",
            "after loss data in training 0.05032488703727722 0.12511337655982951\n",
            "before loss data in training 0.04096750542521477 0.12511337655982951\n",
            "after loss data in training 0.04096750542521477 0.12494740837218728\n",
            "before loss data in training 0.05729396268725395 0.12494740837218728\n",
            "after loss data in training 0.05729396268725395 0.12481423229800434\n",
            "before loss data in training 0.06217087432742119 0.12481423229800434\n",
            "after loss data in training 0.06217087432742119 0.12469116086780672\n",
            "before loss data in training 0.04156571626663208 0.12469116086780672\n",
            "after loss data in training 0.04156571626663208 0.12452816979996129\n",
            "before loss data in training 0.05383555591106415 0.12452816979996129\n",
            "after loss data in training 0.05383555591106415 0.12438982808980689\n",
            "before loss data in training 0.04353718087077141 0.12438982808980689\n",
            "after loss data in training 0.04353718087077141 0.12423191276320722\n",
            "before loss data in training 0.06088031455874443 0.12423191276320722\n",
            "after loss data in training 0.06088031455874443 0.12410842036904647\n",
            "before loss data in training 0.0713772177696228 0.12410842036904647\n",
            "after loss data in training 0.0713772177696228 0.12400583048072074\n",
            "before loss data in training 0.08167089521884918 0.12400583048072074\n",
            "after loss data in training 0.08167089521884918 0.1239236267229307\n",
            "before loss data in training 0.036947138607501984 0.1239236267229307\n",
            "after loss data in training 0.036947138607501984 0.12375506763743568\n",
            "before loss data in training 0.0476534441113472 0.12375506763743568\n",
            "after loss data in training 0.0476534441113472 0.12360786913931945\n",
            "before loss data in training 0.04745827615261078 0.12360786913931945\n",
            "after loss data in training 0.04745827615261078 0.12346086220305168\n",
            "before loss data in training 0.059426963329315186 0.12346086220305168\n",
            "after loss data in training 0.059426963329315186 0.12333748282179206\n",
            "before loss data in training 0.03984946757555008 0.12333748282179206\n",
            "after loss data in training 0.03984946757555008 0.12317692894631851\n",
            "before loss data in training 0.0352933406829834 0.12317692894631851\n",
            "after loss data in training 0.0352933406829834 0.12300824643525646\n",
            "before loss data in training 0.06797149777412415 0.12300824643525646\n",
            "after loss data in training 0.06797149777412415 0.12290281205084816\n",
            "before loss data in training 0.07157158106565475 0.12290281205084816\n",
            "after loss data in training 0.07157158106565475 0.1228046643816604\n",
            "before loss data in training 0.04465916380286217 0.1228046643816604\n",
            "after loss data in training 0.04465916380286217 0.12265553174696804\n",
            "before loss data in training 0.04373187944293022 0.12265553174696804\n",
            "after loss data in training 0.04373187944293022 0.12250520098067463\n",
            "before loss data in training 0.06534652411937714 0.12250520098067463\n",
            "after loss data in training 0.06534652411937714 0.12239653429462653\n",
            "before loss data in training 0.043551720678806305 0.12239653429462653\n",
            "after loss data in training 0.043551720678806305 0.1222469236426041\n",
            "before loss data in training 0.08631947636604309 0.1222469236426041\n",
            "after loss data in training 0.08631947636604309 0.12217887923488334\n",
            "before loss data in training 0.04549074172973633 0.12217887923488334\n",
            "after loss data in training 0.04549074172973633 0.12203391111105509\n",
            "before loss data in training 0.042168259620666504 0.12203391111105509\n",
            "after loss data in training 0.042168259620666504 0.12188322120258266\n",
            "before loss data in training 0.04865541681647301 0.12188322120258266\n",
            "after loss data in training 0.04865541681647301 0.12174531573292897\n",
            "before loss data in training 0.019771330058574677 0.12174531573292897\n",
            "after loss data in training 0.019771330058574677 0.12155363530872906\n",
            "before loss data in training 0.0445663258433342 0.12155363530872906\n",
            "after loss data in training 0.0445663258433342 0.12140919382755572\n",
            "before loss data in training 0.038373254239559174 0.12140919382755572\n",
            "after loss data in training 0.038373254239559174 0.12125369581334598\n",
            "before loss data in training 0.04044179618358612 0.12125369581334598\n",
            "after loss data in training 0.04044179618358612 0.12110264553366419\n",
            "before loss data in training 0.05804823338985443 0.12110264553366419\n",
            "after loss data in training 0.05804823338985443 0.12098500670503769\n",
            "before loss data in training 0.07148443907499313 0.12098500670503769\n",
            "after loss data in training 0.07148443907499313 0.12089282687704878\n",
            "before loss data in training 0.06802602112293243 0.12089282687704878\n",
            "after loss data in training 0.06802602112293243 0.1207945614388441\n",
            "before loss data in training 0.05999011546373367 0.1207945614388441\n",
            "after loss data in training 0.05999011546373367 0.12068175170605168\n",
            "before loss data in training 0.04669947549700737 0.12068175170605168\n",
            "after loss data in training 0.04669947549700737 0.12054474749084974\n",
            "before loss data in training 0.07378726452589035 0.12054474749084974\n",
            "after loss data in training 0.07378726452589035 0.12045831961106239\n",
            "before loss data in training 0.04582139104604721 0.12045831961106239\n",
            "after loss data in training 0.04582139104604721 0.12032061310079484\n",
            "before loss data in training 0.056186288595199585 0.12032061310079484\n",
            "after loss data in training 0.056186288595199585 0.12020250200594107\n",
            "before loss data in training 0.04856020212173462 0.12020250200594107\n",
            "after loss data in training 0.04856020212173462 0.12007080660174216\n",
            "before loss data in training 0.05599584802985191 0.12007080660174216\n",
            "after loss data in training 0.05599584802985191 0.11995323787041759\n",
            "before loss data in training 0.033595968037843704 0.11995323787041759\n",
            "after loss data in training 0.033595968037843704 0.11979507437255574\n",
            "before loss data in training 0.04939044266939163 0.11979507437255574\n",
            "after loss data in training 0.04939044266939163 0.11966636389412216\n",
            "before loss data in training 0.1024923175573349 0.11966636389412216\n",
            "after loss data in training 0.1024923175573349 0.1196350243935076\n",
            "before loss data in training 0.06368313729763031 0.1196350243935076\n",
            "after loss data in training 0.06368313729763031 0.11953310838786847\n",
            "before loss data in training 0.07273329794406891 0.11953310838786847\n",
            "after loss data in training 0.07273329794406891 0.1194480178234252\n",
            "before loss data in training 0.048199884593486786 0.1194480178234252\n",
            "after loss data in training 0.048199884593486786 0.11931871086656506\n",
            "before loss data in training 0.05926741287112236 0.11931871086656506\n",
            "after loss data in training 0.05926741287112236 0.11920992228323998\n",
            "before loss data in training 0.030778296291828156 0.11920992228323998\n",
            "after loss data in training 0.030778296291828156 0.11905000975884322\n",
            "before loss data in training 0.06327688694000244 0.11905000975884322\n",
            "after loss data in training 0.06327688694000244 0.11894933625195\n",
            "before loss data in training 0.05385246127843857 0.11894933625195\n",
            "after loss data in training 0.05385246127843857 0.11883204458533106\n",
            "before loss data in training 0.03556100279092789 0.11883204458533106\n",
            "after loss data in training 0.03556100279092789 0.11868227652454975\n",
            "before loss data in training 0.02954782545566559 0.11868227652454975\n",
            "after loss data in training 0.02954782545566559 0.11852225058008138\n",
            "before loss data in training 0.034831028431653976 0.11852225058008138\n",
            "after loss data in training 0.034831028431653976 0.11837226631099818\n",
            "before loss data in training 0.04954839497804642 0.11837226631099818\n",
            "after loss data in training 0.04954839497804642 0.11824914668428449\n",
            "before loss data in training 0.08850541710853577 0.11824914668428449\n",
            "after loss data in training 0.08850541710853577 0.11819603288147065\n",
            "before loss data in training 0.07028494775295258 0.11819603288147065\n",
            "after loss data in training 0.07028494775295258 0.11811062987767651\n",
            "before loss data in training 0.02696482464671135 0.11811062987767651\n",
            "after loss data in training 0.02696482464671135 0.1179484487295787\n",
            "before loss data in training 0.046633683145046234 0.1179484487295787\n",
            "after loss data in training 0.046633683145046234 0.11782177951894898\n",
            "before loss data in training 0.0462837815284729 0.11782177951894898\n",
            "after loss data in training 0.0462837815284729 0.11769493909698005\n",
            "before loss data in training 0.06790617108345032 0.11769493909698005\n",
            "after loss data in training 0.06790617108345032 0.11760681738368177\n",
            "before loss data in training 0.05776691436767578 0.11760681738368177\n",
            "after loss data in training 0.05776691436767578 0.11750109317340614\n",
            "before loss data in training 0.048718344420194626 0.11750109317340614\n",
            "after loss data in training 0.048718344420194626 0.11737978321087843\n",
            "before loss data in training 0.06123179942369461 0.11737978321087843\n",
            "after loss data in training 0.06123179942369461 0.11728093112674606\n",
            "before loss data in training 0.03198441490530968 0.11728093112674606\n",
            "after loss data in training 0.03198441490530968 0.11713102512284196\n",
            "before loss data in training 0.06492199003696442 0.11713102512284196\n",
            "after loss data in training 0.06492199003696442 0.11703943032444568\n",
            "before loss data in training 0.05113605782389641 0.11703943032444568\n",
            "after loss data in training 0.05113605782389641 0.11692401285947099\n",
            "before loss data in training 0.06994088739156723 0.11692401285947099\n",
            "after loss data in training 0.06994088739156723 0.1168418745282334\n",
            "before loss data in training 0.042156629264354706 0.1168418745282334\n",
            "after loss data in training 0.042156629264354706 0.11671153378606258\n",
            "before loss data in training 0.050355296581983566 0.11671153378606258\n",
            "after loss data in training 0.050355296581983566 0.11659593058535861\n",
            "before loss data in training 0.043305110186338425 0.11659593058535861\n",
            "after loss data in training 0.043305110186338425 0.11646846828901249\n",
            "before loss data in training 0.0436854287981987 0.11646846828901249\n",
            "after loss data in training 0.0436854287981987 0.11634210884545204\n",
            "before loss data in training 0.04257219284772873 0.11634210884545204\n",
            "after loss data in training 0.04257219284772873 0.11621425803783034\n",
            "before loss data in training 0.047087907791137695 0.11621425803783034\n",
            "after loss data in training 0.047087907791137695 0.11609466227615786\n",
            "before loss data in training 0.026581302285194397 0.11609466227615786\n",
            "after loss data in training 0.026581302285194397 0.1159400623452581\n",
            "before loss data in training 0.06911662220954895 0.1159400623452581\n",
            "after loss data in training 0.06911662220954895 0.1158593322760586\n",
            "before loss data in training 0.02170511707663536 0.1158593322760586\n",
            "after loss data in training 0.02170511707663536 0.11569727682821106\n",
            "before loss data in training 0.03233300894498825 0.11569727682821106\n",
            "after loss data in training 0.03233300894498825 0.11555403925452855\n",
            "before loss data in training 0.06077498942613602 0.11555403925452855\n",
            "after loss data in training 0.06077498942613602 0.11546007862017453\n",
            "before loss data in training 0.06344679743051529 0.11546007862017453\n",
            "after loss data in training 0.06344679743051529 0.115371014782521\n",
            "before loss data in training 0.056486450135707855 0.115371014782521\n",
            "after loss data in training 0.056486450135707855 0.11527035740705636\n",
            "before loss data in training 0.04271968826651573 0.11527035740705636\n",
            "after loss data in training 0.04271968826651573 0.11514655080442746\n",
            "before loss data in training 0.05415472015738487 0.11514655080442746\n",
            "after loss data in training 0.05415472015738487 0.11504264649327406\n",
            "before loss data in training 0.0479881726205349 0.11504264649327406\n",
            "after loss data in training 0.0479881726205349 0.11492860827240205\n",
            "before loss data in training 0.04958373308181763 0.11492860827240205\n",
            "after loss data in training 0.04958373308181763 0.11481766620926014\n",
            "before loss data in training 0.057853203266859055 0.11481766620926014\n",
            "after loss data in training 0.057853203266859055 0.11472111627206963\n",
            "before loss data in training 0.035898059606552124 0.11472111627206963\n",
            "after loss data in training 0.035898059606552124 0.11458774392576587\n",
            "before loss data in training 0.04781769961118698 0.11458774392576587\n",
            "after loss data in training 0.04781769961118698 0.114474956688748\n",
            "before loss data in training 0.07620232552289963 0.114474956688748\n",
            "after loss data in training 0.07620232552289963 0.11441041599538232\n",
            "before loss data in training 0.03868446499109268 0.11441041599538232\n",
            "after loss data in training 0.03868446499109268 0.11428293122938184\n",
            "before loss data in training 0.04185384139418602 0.11428293122938184\n",
            "after loss data in training 0.04185384139418602 0.11416120166663361\n",
            "before loss data in training 0.025139374658465385 0.11416120166663361\n",
            "after loss data in training 0.025139374658465385 0.11401183618507628\n",
            "before loss data in training 0.057709209620952606 0.11401183618507628\n",
            "after loss data in training 0.057709209620952606 0.11391752692784994\n",
            "before loss data in training 0.03343811631202698 0.11391752692784994\n",
            "after loss data in training 0.03343811631202698 0.11378294597364288\n",
            "before loss data in training 0.07006213068962097 0.11378294597364288\n",
            "after loss data in training 0.07006213068962097 0.11370995629871129\n",
            "before loss data in training 0.04807370528578758 0.11370995629871129\n",
            "after loss data in training 0.04807370528578758 0.11360056254702308\n",
            "before loss data in training 0.05951561778783798 0.11360056254702308\n",
            "after loss data in training 0.05951561778783798 0.11351057095840547\n",
            "before loss data in training 0.03172868862748146 0.11351057095840547\n",
            "after loss data in training 0.03172868862748146 0.11337472065553018\n",
            "before loss data in training 0.05437244102358818 0.11337472065553018\n",
            "after loss data in training 0.05437244102358818 0.11327687276227655\n",
            "before loss data in training 0.04548512026667595 0.11327687276227655\n",
            "after loss data in training 0.04548512026667595 0.11316463476145602\n",
            "before loss data in training 0.04654112458229065 0.11316463476145602\n",
            "after loss data in training 0.04654112458229065 0.11305451325702764\n",
            "before loss data in training 0.06849823147058487 0.11305451325702764\n",
            "after loss data in training 0.06849823147058487 0.11298098803955826\n",
            "before loss data in training 0.0355578288435936 0.11298098803955826\n",
            "after loss data in training 0.0355578288435936 0.11285343753017446\n",
            "before loss data in training 0.0596437081694603 0.11285343753017446\n",
            "after loss data in training 0.0596437081694603 0.11276592152793645\n",
            "before loss data in training 0.04920630156993866 0.11276592152793645\n",
            "after loss data in training 0.04920630156993866 0.11266155433588719\n",
            "before loss data in training 0.08905571699142456 0.11266155433588719\n",
            "after loss data in training 0.08905571699142456 0.11262285624187987\n",
            "before loss data in training 0.0454341322183609 0.11262285624187987\n",
            "after loss data in training 0.0454341322183609 0.11251289106344531\n",
            "before loss data in training 0.03889375552535057 0.11251289106344531\n",
            "after loss data in training 0.03889375552535057 0.1123925983583177\n",
            "before loss data in training 0.03809648007154465 0.1123925983583177\n",
            "after loss data in training 0.03809648007154465 0.11227139751282542\n",
            "before loss data in training 0.05247390270233154 0.11227139751282542\n",
            "after loss data in training 0.05247390270233154 0.1121740074561308\n",
            "before loss data in training 0.05487674102187157 0.1121740074561308\n",
            "after loss data in training 0.05487674102187157 0.11208084116924583\n",
            "before loss data in training 0.04311327263712883 0.11208084116924583\n",
            "after loss data in training 0.04311327263712883 0.11196888083071967\n",
            "before loss data in training 0.055151551961898804 0.11196888083071967\n",
            "after loss data in training 0.055151551961898804 0.11187679439819322\n",
            "before loss data in training 0.059192508459091187 0.11187679439819322\n",
            "after loss data in training 0.059192508459091187 0.11179154474457008\n",
            "before loss data in training 0.030236832797527313 0.11179154474457008\n",
            "after loss data in training 0.030236832797527313 0.11165979238278165\n",
            "before loss data in training 0.02914821170270443 0.11165979238278165\n",
            "after loss data in training 0.02914821170270443 0.11152670918813636\n",
            "before loss data in training 0.06449726223945618 0.11152670918813636\n",
            "after loss data in training 0.06449726223945618 0.11145097738950724\n",
            "before loss data in training 0.029308823868632317 0.11145097738950724\n",
            "after loss data in training 0.029308823868632317 0.11131891604944152\n",
            "before loss data in training 0.04282308742403984 0.11131891604944152\n",
            "after loss data in training 0.04282308742403984 0.11120897089915999\n",
            "before loss data in training 0.060386449098587036 0.11120897089915999\n",
            "after loss data in training 0.060386449098587036 0.11112752455012061\n",
            "before loss data in training 0.042641665786504745 0.11112752455012061\n",
            "after loss data in training 0.042641665786504745 0.11101794717609882\n",
            "before loss data in training 0.017052102833986282 0.11101794717609882\n",
            "after loss data in training 0.017052102833986282 0.11086784199344368\n",
            "before loss data in training 0.08147228509187698 0.11086784199344368\n",
            "after loss data in training 0.08147228509187698 0.11082095912757196\n",
            "before loss data in training 0.05957871302962303 0.11082095912757196\n",
            "after loss data in training 0.05957871302962303 0.11073936319429498\n",
            "before loss data in training 0.049351152032613754 0.11073936319429498\n",
            "after loss data in training 0.049351152032613754 0.11064176667416512\n",
            "before loss data in training 0.05412977188825607 0.11064176667416512\n",
            "after loss data in training 0.05412977188825607 0.11055206509513986\n",
            "before loss data in training 0.06261079758405685 0.11055206509513986\n",
            "after loss data in training 0.06261079758405685 0.11047608844298283\n",
            "before loss data in training 0.06419375538825989 0.11047608844298283\n",
            "after loss data in training 0.06419375538825989 0.11040285690333929\n",
            "before loss data in training 0.0479297935962677 0.11040285690333929\n",
            "after loss data in training 0.0479297935962677 0.11030416328042132\n",
            "before loss data in training 0.056802019476890564 0.11030416328042132\n",
            "after loss data in training 0.056802019476890564 0.11021977504098357\n",
            "before loss data in training 0.09464624524116516 0.11021977504098357\n",
            "after loss data in training 0.09464624524116516 0.11019524979720433\n",
            "before loss data in training 0.044610463082790375 0.11019524979720433\n",
            "after loss data in training 0.044610463082790375 0.11009212906337663\n",
            "before loss data in training 0.025113103911280632 0.11009212906337663\n",
            "after loss data in training 0.025113103911280632 0.10995872400034351\n",
            "before loss data in training 0.03765404224395752 0.10995872400034351\n",
            "after loss data in training 0.03765404224395752 0.10984539377815482\n",
            "before loss data in training 0.05091310292482376 0.10984539377815482\n",
            "after loss data in training 0.05091310292482376 0.1097531679708726\n",
            "before loss data in training 0.054738812148571014 0.1097531679708726\n",
            "after loss data in training 0.054738812148571014 0.10966720803990025\n",
            "before loss data in training 0.04921576380729675 0.10966720803990025\n",
            "after loss data in training 0.04921576380729675 0.10957290001457638\n",
            "before loss data in training 0.062394123524427414 0.10957290001457638\n",
            "after loss data in training 0.062394123524427414 0.10949941282378176\n",
            "before loss data in training 0.05288008973002434 0.10949941282378176\n",
            "after loss data in training 0.05288008973002434 0.10941135788895476\n",
            "before loss data in training 0.030921470373868942 0.10941135788895476\n",
            "after loss data in training 0.030921470373868942 0.10928947918163319\n",
            "before loss data in training 0.04946189373731613 0.10928947918163319\n",
            "after loss data in training 0.04946189373731613 0.10919672323520789\n",
            "before loss data in training 0.03247138857841492 0.10919672323520789\n",
            "after loss data in training 0.03247138857841492 0.10907795336731811\n",
            "before loss data in training 0.05253427475690842 0.10907795336731811\n",
            "after loss data in training 0.05253427475690842 0.10899055973731747\n",
            "before loss data in training 0.04981965944170952 0.10899055973731747\n",
            "after loss data in training 0.04981965944170952 0.10889924661957734\n",
            "before loss data in training 0.043180447071790695 0.10889924661957734\n",
            "after loss data in training 0.043180447071790695 0.10879798498699216\n",
            "before loss data in training 0.06049243360757828 0.10879798498699216\n",
            "after loss data in training 0.06049243360757828 0.10872366875410075\n",
            "before loss data in training 0.07049964368343353 0.10872366875410075\n",
            "after loss data in training 0.07049964368343353 0.10866495289377714\n",
            "before loss data in training 0.07523666322231293 0.10866495289377714\n",
            "after loss data in training 0.07523666322231293 0.10861368251084545\n",
            "before loss data in training 0.043270327150821686 0.10861368251084545\n",
            "after loss data in training 0.043270327150821686 0.10851361611672597\n",
            "before loss data in training 0.04931681975722313 0.10851361611672597\n",
            "after loss data in training 0.04931681975722313 0.108423101137583\n",
            "before loss data in training 0.04727497324347496 0.108423101137583\n",
            "after loss data in training 0.04727497324347496 0.10832974521713397\n",
            "before loss data in training 0.02311929129064083 0.10832974521713397\n",
            "after loss data in training 0.02311929129064083 0.10819985123248993\n",
            "before loss data in training 0.054209187626838684 0.10819985123248993\n",
            "after loss data in training 0.054209187626838684 0.1081176736623139\n",
            "before loss data in training 0.049366295337677 0.1081176736623139\n",
            "after loss data in training 0.049366295337677 0.10802838585330989\n",
            "before loss data in training 0.03797295689582825 0.10802838585330989\n",
            "after loss data in training 0.03797295689582825 0.1079220801948008\n",
            "before loss data in training 0.06811060011386871 0.1079220801948008\n",
            "after loss data in training 0.06811060011386871 0.10786175977043576\n",
            "before loss data in training 0.03278679400682449 0.10786175977043576\n",
            "after loss data in training 0.03278679400682449 0.10774818190997644\n",
            "before loss data in training 0.040357016026973724 0.10774818190997644\n",
            "after loss data in training 0.040357016026973724 0.1076463825657423\n",
            "before loss data in training 0.032358333468437195 0.1076463825657423\n",
            "after loss data in training 0.032358333468437195 0.10753282593060308\n",
            "before loss data in training 0.0434638112783432 0.10753282593060308\n",
            "after loss data in training 0.0434638112783432 0.10743633645070509\n",
            "before loss data in training 0.02837477996945381 0.10743633645070509\n",
            "after loss data in training 0.02837477996945381 0.10731744689208667\n",
            "before loss data in training 0.05367688834667206 0.10731744689208667\n",
            "after loss data in training 0.05367688834667206 0.10723690551288935\n",
            "before loss data in training 0.0668349489569664 0.10723690551288935\n",
            "after loss data in training 0.0668349489569664 0.10717633286437972\n",
            "before loss data in training 0.06258562207221985 0.10717633286437972\n",
            "after loss data in training 0.06258562207221985 0.1071095803033136\n",
            "before loss data in training 0.03159219026565552 0.1071095803033136\n",
            "after loss data in training 0.03159219026565552 0.10699669930176255\n",
            "before loss data in training 0.04945005476474762 0.10699669930176255\n",
            "after loss data in training 0.04945005476474762 0.10691080878752819\n",
            "before loss data in training 0.05241303890943527 0.10691080878752819\n",
            "after loss data in training 0.05241303890943527 0.1068295900544759\n",
            "before loss data in training 0.06455940753221512 0.1068295900544759\n",
            "after loss data in training 0.06455940753221512 0.10676668799715111\n",
            "before loss data in training 0.05972981080412865 0.10676668799715111\n",
            "after loss data in training 0.05972981080412865 0.10669679664916742\n",
            "before loss data in training 0.0608077272772789 0.10669679664916742\n",
            "after loss data in training 0.0608077272772789 0.10662871197650883\n",
            "before loss data in training 0.06270717084407806 0.10662871197650883\n",
            "after loss data in training 0.06270717084407806 0.10656364302668302\n",
            "before loss data in training 0.03839387744665146 0.10656364302668302\n",
            "after loss data in training 0.03839387744665146 0.10646280017819185\n",
            "before loss data in training 0.07000623643398285 0.10646280017819185\n",
            "after loss data in training 0.07000623643398285 0.10640895001017972\n",
            "before loss data in training 0.051752083003520966 0.10640895001017972\n",
            "after loss data in training 0.051752083003520966 0.1063283351620873\n",
            "before loss data in training 0.042315494269132614 0.1063283351620873\n",
            "after loss data in training 0.042315494269132614 0.10623405999140548\n",
            "before loss data in training 0.02955714426934719 0.10623405999140548\n",
            "after loss data in training 0.02955714426934719 0.10612129982122598\n",
            "before loss data in training 0.06244581937789917 0.10612129982122598\n",
            "after loss data in training 0.06244581937789917 0.10605716548871008\n",
            "before loss data in training 0.03680688142776489 0.10605716548871008\n",
            "after loss data in training 0.03680688142776489 0.10595562548275561\n",
            "before loss data in training 0.057692982256412506 0.10595562548275561\n",
            "after loss data in training 0.057692982256412506 0.10588496275475218\n",
            "before loss data in training 0.09461047500371933 0.10588496275475218\n",
            "after loss data in training 0.09461047500371933 0.10586847958552552\n",
            "before loss data in training 0.04550669342279434 0.10586847958552552\n",
            "after loss data in training 0.04550669342279434 0.10578036018966752\n",
            "before loss data in training 0.05761189013719559 0.10578036018966752\n",
            "after loss data in training 0.05761189013719559 0.1057101437610196\n",
            "before loss data in training 0.03535373508930206 0.1057101437610196\n",
            "after loss data in training 0.03535373508930206 0.10560773268580603\n",
            "before loss data in training 0.06655938178300858 0.10560773268580603\n",
            "after loss data in training 0.06655938178300858 0.10555097636181941\n",
            "before loss data in training 0.058225661516189575 0.10555097636181941\n",
            "after loss data in training 0.058225661516189575 0.10548228940268208\n",
            "before loss data in training 0.04624935984611511 0.10548228940268208\n",
            "after loss data in training 0.04624935984611511 0.10539644457723778\n",
            "before loss data in training 0.04578983038663864 0.10539644457723778\n",
            "after loss data in training 0.04578983038663864 0.10531018319635413\n",
            "before loss data in training 0.06166767701506615 0.10531018319635413\n",
            "after loss data in training 0.06166767701506615 0.10524711599088984\n",
            "before loss data in training 0.124346062541008 0.10524711599088984\n",
            "after loss data in training 0.124346062541008 0.10527467579832148\n",
            "before loss data in training 0.06253497302532196 0.10527467579832148\n",
            "after loss data in training 0.06253497302532196 0.10521309121219323\n",
            "before loss data in training 0.04823978990316391 0.10521309121219323\n",
            "after loss data in training 0.04823978990316391 0.1051311152390867\n",
            "before loss data in training 0.07635979354381561 0.1051311152390867\n",
            "after loss data in training 0.07635979354381561 0.1050897771332027\n",
            "before loss data in training 0.044550687074661255 0.1050897771332027\n",
            "after loss data in training 0.044550687074661255 0.10500292047601684\n",
            "before loss data in training 0.024892210960388184 0.10500292047601684\n",
            "after loss data in training 0.024892210960388184 0.10488814868587984\n",
            "before loss data in training 0.0707874521613121 0.10488814868587984\n",
            "after loss data in training 0.0707874521613121 0.10483936371231108\n",
            "before loss data in training 0.0629071593284607 0.10483936371231108\n",
            "after loss data in training 0.0629071593284607 0.10477946056319129\n",
            "before loss data in training 0.0434875413775444 0.10477946056319129\n",
            "after loss data in training 0.0434875413775444 0.104692025585751\n",
            "before loss data in training 0.06761312484741211 0.104692025585751\n",
            "after loss data in training 0.06761312484741211 0.10463920663883029\n",
            "before loss data in training 0.047766685485839844 0.10463920663883029\n",
            "after loss data in training 0.047766685485839844 0.10455830689323571\n",
            "before loss data in training 0.09231384843587875 0.10455830689323571\n",
            "after loss data in training 0.09231384843587875 0.10454091419656333\n",
            "before loss data in training 0.06271986663341522 0.10454091419656333\n",
            "after loss data in training 0.06271986663341522 0.10448159356172199\n",
            "before loss data in training 0.06180080771446228 0.10448159356172199\n",
            "after loss data in training 0.06180080771446228 0.10442113919083351\n",
            "before loss data in training 0.07140041142702103 0.10442113919083351\n",
            "after loss data in training 0.07140041142702103 0.10437443377674042\n",
            "before loss data in training 0.01962929591536522 0.10437443377674042\n",
            "after loss data in training 0.01962929591536522 0.10425473725433736\n",
            "before loss data in training 0.024107009172439575 0.10425473725433736\n",
            "after loss data in training 0.024107009172439575 0.10414169391430647\n",
            "before loss data in training 0.034941114485263824 0.10414169391430647\n",
            "after loss data in training 0.034941114485263824 0.10404422830947684\n",
            "before loss data in training 0.06242702528834343 0.10404422830947684\n",
            "after loss data in training 0.06242702528834343 0.10398569497189437\n",
            "before loss data in training 0.03209006413817406 0.10398569497189437\n",
            "after loss data in training 0.03209006413817406 0.10388471796229645\n",
            "before loss data in training 0.05376143753528595 0.10388471796229645\n",
            "after loss data in training 0.05376143753528595 0.10381441883126277\n",
            "before loss data in training 0.0628412589430809 0.10381441883126277\n",
            "after loss data in training 0.0628412589430809 0.10375703345326812\n",
            "before loss data in training 0.08738219738006592 0.10375703345326812\n",
            "after loss data in training 0.08738219738006592 0.10373413158463426\n",
            "before loss data in training 0.050067417323589325 0.10373413158463426\n",
            "after loss data in training 0.050067417323589325 0.10365917807309649\n",
            "before loss data in training 0.05645511671900749 0.10365917807309649\n",
            "after loss data in training 0.05645511671900749 0.1035933425621424\n",
            "before loss data in training 0.04390118271112442 0.1035933425621424\n",
            "after loss data in training 0.04390118271112442 0.10351020584925796\n",
            "before loss data in training 0.12023904174566269 0.10351020584925796\n",
            "after loss data in training 0.12023904174566269 0.10353347265857146\n",
            "before loss data in training 0.042453162372112274 0.10353347265857146\n",
            "after loss data in training 0.042453162372112274 0.10344863889428471\n",
            "before loss data in training 0.06255172938108444 0.10344863889428471\n",
            "after loss data in training 0.06255172938108444 0.10339191641229692\n",
            "before loss data in training 0.07583202421665192 0.10339191641229692\n",
            "after loss data in training 0.07583202421665192 0.10335374481645808\n",
            "before loss data in training 0.0618460550904274 0.10335374481645808\n",
            "after loss data in training 0.0618460550904274 0.10329633445722429\n",
            "before loss data in training 0.056012339890003204 0.10329633445722429\n",
            "after loss data in training 0.056012339890003204 0.10323102507246293\n",
            "before loss data in training 0.06296296417713165 0.10323102507246293\n",
            "after loss data in training 0.06296296417713165 0.10317548291950385\n",
            "before loss data in training 0.06409299373626709 0.10317548291950385\n",
            "after loss data in training 0.06409299373626709 0.10312165028977487\n",
            "before loss data in training 0.026709934696555138 0.10312165028977487\n",
            "after loss data in training 0.026709934696555138 0.10301654476626287\n",
            "before loss data in training 0.04914342612028122 0.10301654476626287\n",
            "after loss data in training 0.04914342612028122 0.10294254322966125\n",
            "before loss data in training 0.042441338300704956 0.10294254322966125\n",
            "after loss data in training 0.042441338300704956 0.10285955117900424\n",
            "before loss data in training 0.05712370201945305 0.10285955117900424\n",
            "after loss data in training 0.05712370201945305 0.10279689933084046\n",
            "before loss data in training 0.032886579632759094 0.10279689933084046\n",
            "after loss data in training 0.032886579632759094 0.1027012627785859\n",
            "before loss data in training 0.056259896606206894 0.1027012627785859\n",
            "after loss data in training 0.056259896606206894 0.10263781828927937\n",
            "before loss data in training 0.04648220166563988 0.10263781828927937\n",
            "after loss data in training 0.04648220166563988 0.10256120762539991\n",
            "before loss data in training 0.03456540405750275 0.10256120762539991\n",
            "after loss data in training 0.03456540405750275 0.10246857029083875\n",
            "before loss data in training 0.05520026385784149 0.10246857029083875\n",
            "after loss data in training 0.05520026385784149 0.10240425966984147\n",
            "before loss data in training 0.017342720180749893 0.10240425966984147\n",
            "after loss data in training 0.017342720180749893 0.10228868692597043\n",
            "before loss data in training 0.06286394596099854 0.10228868692597043\n",
            "after loss data in training 0.06286394596099854 0.10223519338327712\n",
            "before loss data in training 0.02467217668890953 0.10223519338327712\n",
            "after loss data in training 0.02467217668890953 0.10213009444466686\n",
            "before loss data in training 0.07524421066045761 0.10213009444466686\n",
            "after loss data in training 0.07524421066045761 0.10209371300517538\n",
            "before loss data in training 0.0662437230348587 0.10209371300517538\n",
            "after loss data in training 0.0662437230348587 0.10204526707278305\n",
            "before loss data in training 0.0464182123541832 0.10204526707278305\n",
            "after loss data in training 0.0464182123541832 0.1019701968235002\n",
            "before loss data in training 0.04822060465812683 0.1019701968235002\n",
            "after loss data in training 0.04822060465812683 0.10189775802004282\n",
            "before loss data in training 0.0723370909690857 0.10189775802004282\n",
            "after loss data in training 0.0723370909690857 0.10185797246546549\n",
            "before loss data in training 0.06074657291173935 0.10185797246546549\n",
            "after loss data in training 0.06074657291173935 0.1018027152080008\n",
            "before loss data in training 0.10589462518692017 0.1018027152080008\n",
            "after loss data in training 0.10589462518692017 0.1018082077046168\n",
            "before loss data in training 0.045469895005226135 0.1018082077046168\n",
            "after loss data in training 0.045469895005226135 0.1017326871782101\n",
            "before loss data in training 0.08042589575052261 0.1017326871782101\n",
            "after loss data in training 0.08042589575052261 0.10170416403038188\n",
            "before loss data in training 0.03358563408255577 0.10170416403038188\n",
            "after loss data in training 0.03358563408255577 0.1016130964769757\n",
            "before loss data in training 0.0750422477722168 0.1016130964769757\n",
            "after loss data in training 0.0750422477722168 0.10157762137857147\n",
            "before loss data in training 0.0931033119559288 0.10157762137857147\n",
            "after loss data in training 0.0931033119559288 0.10156632229934129\n",
            "before loss data in training 0.018567824736237526 0.10156632229934129\n",
            "after loss data in training 0.018567824736237526 0.10145580499233316\n",
            "before loss data in training 0.058796703815460205 0.10145580499233316\n",
            "after loss data in training 0.058796703815460205 0.10139907746417243\n",
            "before loss data in training 0.058882057666778564 0.10139907746417243\n",
            "after loss data in training 0.058882057666778564 0.1013426139584654\n",
            "before loss data in training 0.04645393416285515 0.1013426139584654\n",
            "after loss data in training 0.04645393416285515 0.10126981730091154\n",
            "before loss data in training 0.05458153411746025 0.10126981730091154\n",
            "after loss data in training 0.05458153411746025 0.10120797851523809\n",
            "before loss data in training 0.050763048231601715 0.10120797851523809\n",
            "after loss data in training 0.050763048231601715 0.10114125241697931\n",
            "before loss data in training 0.03908386453986168 0.10114125241697931\n",
            "after loss data in training 0.03908386453986168 0.10105927436165947\n",
            "before loss data in training 0.03855123370885849 0.10105927436165947\n",
            "after loss data in training 0.03855123370885849 0.10097680992808057\n",
            "before loss data in training 0.04272454231977463 0.10097680992808057\n",
            "after loss data in training 0.04272454231977463 0.10090006122240427\n",
            "before loss data in training 0.05980687588453293 0.10090006122240427\n",
            "after loss data in training 0.05980687588453293 0.10084599124169655\n",
            "before loss data in training 0.03418286144733429 0.10084599124169655\n",
            "after loss data in training 0.03418286144733429 0.10075839185957518\n",
            "before loss data in training 0.07178203016519547 0.10075839185957518\n",
            "after loss data in training 0.07178203016519547 0.10072036513819148\n",
            "before loss data in training 0.02981102094054222 0.10072036513819148\n",
            "after loss data in training 0.02981102094054222 0.10062743021788001\n",
            "before loss data in training 0.046496741473674774 0.10062743021788001\n",
            "after loss data in training 0.046496741473674774 0.10055657853104205\n",
            "before loss data in training 0.014805416576564312 0.10055657853104205\n",
            "after loss data in training 0.014805416576564312 0.10044448550887934\n",
            "before loss data in training 0.058183543384075165 0.10044448550887934\n",
            "after loss data in training 0.058183543384075165 0.10038931456615767\n",
            "before loss data in training 0.047347910702228546 0.10038931456615767\n",
            "after loss data in training 0.047347910702228546 0.10032016019345372\n",
            "before loss data in training 0.03302411735057831 0.10032016019345372\n",
            "after loss data in training 0.03302411735057831 0.10023253513766872\n",
            "before loss data in training 0.06275863200426102 0.10023253513766872\n",
            "after loss data in training 0.06275863200426102 0.1001838044443873\n",
            "before loss data in training 0.030923258513212204 0.1001838044443873\n",
            "after loss data in training 0.030923258513212204 0.10009385568343773\n",
            "before loss data in training 0.06772609800100327 0.10009385568343773\n",
            "after loss data in training 0.06772609800100327 0.10005187415596375\n",
            "before loss data in training 0.06736285984516144 0.10005187415596375\n",
            "after loss data in training 0.06736285984516144 0.10000953087317774\n",
            "before loss data in training 0.06388118863105774 0.10000953087317774\n",
            "after loss data in training 0.06388118863105774 0.09996279304362778\n",
            "before loss data in training 0.06151057407259941 0.09996279304362778\n",
            "after loss data in training 0.06151057407259941 0.09991311317415616\n",
            "before loss data in training 0.07696819305419922 0.09991311317415616\n",
            "after loss data in training 0.07696819305419922 0.09988350682561428\n",
            "before loss data in training 0.03606295585632324 0.09988350682561428\n",
            "after loss data in training 0.03606295585632324 0.09980126384756108\n",
            "before loss data in training 0.07672938704490662 0.09980126384756108\n",
            "after loss data in training 0.07672938704490662 0.09977157031242252\n",
            "before loss data in training 0.04334302991628647 0.09977157031242252\n",
            "after loss data in training 0.04334302991628647 0.09969904005484394\n",
            "before loss data in training 0.016014568507671356 0.09969904005484394\n",
            "after loss data in training 0.016014568507671356 0.0995916145457975\n",
            "before loss data in training 0.03187214583158493 0.0995916145457975\n",
            "after loss data in training 0.03187214583158493 0.09950479471411261\n",
            "before loss data in training 0.03373941034078598 0.09950479471411261\n",
            "after loss data in training 0.03373941034078598 0.09942058807599055\n",
            "before loss data in training 0.0531536340713501 0.09942058807599055\n",
            "after loss data in training 0.0531536340713501 0.09936142317317131\n",
            "before loss data in training 0.0652494952082634 0.09936142317317131\n",
            "after loss data in training 0.0652494952082634 0.09931785749250094\n",
            "before loss data in training 0.04540319740772247 0.09931785749250094\n",
            "after loss data in training 0.04540319740772247 0.09924908879341321\n",
            "before loss data in training 0.04481905326247215 0.09924908879341321\n",
            "after loss data in training 0.04481905326247215 0.09917975116853303\n",
            "before loss data in training 0.051368292421102524 0.09917975116853303\n",
            "after loss data in training 0.051368292421102524 0.0991189223406101\n",
            "before loss data in training 0.061877816915512085 0.0991189223406101\n",
            "after loss data in training 0.061877816915512085 0.09907160200334822\n",
            "before loss data in training 0.030324211344122887 0.09907160200334822\n",
            "after loss data in training 0.030324211344122887 0.09898435912180098\n",
            "before loss data in training 0.03231073543429375 0.09898435912180098\n",
            "after loss data in training 0.03231073543429375 0.09889985516275471\n",
            "before loss data in training 0.04465527832508087 0.09889985516275471\n",
            "after loss data in training 0.04465527832508087 0.09883119114144119\n",
            "before loss data in training 0.05083369463682175 0.09883119114144119\n",
            "after loss data in training 0.05083369463682175 0.09877051162626468\n",
            "before loss data in training 0.03307837247848511 0.09877051162626468\n",
            "after loss data in training 0.03307837247848511 0.0986875670061286\n",
            "before loss data in training 0.039099425077438354 0.0986875670061286\n",
            "after loss data in training 0.039099425077438354 0.09861242433030427\n",
            "before loss data in training 0.08376163244247437 0.09861242433030427\n",
            "after loss data in training 0.08376163244247437 0.09859372056218359\n",
            "before loss data in training 0.03965859115123749 0.09859372056218359\n",
            "after loss data in training 0.03965859115123749 0.09851958832393082\n",
            "before loss data in training 0.046907760202884674 0.09851958832393082\n",
            "after loss data in training 0.046907760202884674 0.09845474934387925\n",
            "before loss data in training 0.04300801455974579 0.09845474934387925\n",
            "after loss data in training 0.04300801455974579 0.09838518004051146\n",
            "before loss data in training 0.07865380495786667 0.09838518004051146\n",
            "after loss data in training 0.07865380495786667 0.09836045400657331\n",
            "before loss data in training 0.04828792065382004 0.09836045400657331\n",
            "after loss data in training 0.04828792065382004 0.0982977850036287\n",
            "before loss data in training 0.05038019269704819 0.0982977850036287\n",
            "after loss data in training 0.05038019269704819 0.09823788801324547\n",
            "before loss data in training 0.03307779133319855 0.09823788801324547\n",
            "after loss data in training 0.03307779133319855 0.09815653957793954\n",
            "before loss data in training 0.08939018845558167 0.09815653957793954\n",
            "after loss data in training 0.08939018845558167 0.09814560896556752\n",
            "before loss data in training 0.054258547723293304 0.09814560896556752\n",
            "after loss data in training 0.054258547723293304 0.09809095509104414\n",
            "before loss data in training 0.049811623990535736 0.09809095509104414\n",
            "after loss data in training 0.049811623990535736 0.0980309061717649\n",
            "before loss data in training 0.07164306193590164 0.0980309061717649\n",
            "after loss data in training 0.07164306193590164 0.09799812624103711\n",
            "before loss data in training 0.039394915103912354 0.09799812624103711\n",
            "after loss data in training 0.039394915103912354 0.09792541754235581\n",
            "before loss data in training 0.05447198450565338 0.09792541754235581\n",
            "after loss data in training 0.05447198450565338 0.09787157190042682\n",
            "before loss data in training 0.05698906630277634 0.09787157190042682\n",
            "after loss data in training 0.05698906630277634 0.09782097474003369\n",
            "before loss data in training 0.05412116274237633 0.09782097474003369\n",
            "after loss data in training 0.05412116274237633 0.09776695766710705\n",
            "before loss data in training 0.032276153564453125 0.09776695766710705\n",
            "after loss data in training 0.032276153564453125 0.09768610482253587\n",
            "before loss data in training 0.09634542465209961 0.09768610482253587\n",
            "after loss data in training 0.09634542465209961 0.09768445170272029\n",
            "before loss data in training 0.05180276185274124 0.09768445170272029\n",
            "after loss data in training 0.05180276185274124 0.09762794715857007\n",
            "before loss data in training 0.07326486706733704 0.09762794715857007\n",
            "after loss data in training 0.07326486706733704 0.09759798027038896\n",
            "before loss data in training 0.05668561905622482 0.09759798027038896\n",
            "after loss data in training 0.05668561905622482 0.09754771938437648\n",
            "before loss data in training 0.05063091963529587 0.09754771938437648\n",
            "after loss data in training 0.05063091963529587 0.09749015275891748\n",
            "before loss data in training 0.04369952902197838 0.09749015275891748\n",
            "after loss data in training 0.04369952902197838 0.09742423287688692\n",
            "before loss data in training 0.06201336905360222 0.09742423287688692\n",
            "after loss data in training 0.06201336905360222 0.09738089032630763\n",
            "before loss data in training 0.09866007417440414 0.09738089032630763\n",
            "after loss data in training 0.09866007417440414 0.09738245412074295\n",
            "before loss data in training 0.06071370840072632 0.09738245412074295\n",
            "after loss data in training 0.06071370840072632 0.09733768153744622\n",
            "before loss data in training 0.05123039335012436 0.09733768153744622\n",
            "after loss data in training 0.05123039335012436 0.09728145313721778\n",
            "before loss data in training 0.05614478513598442 0.09728145313721778\n",
            "after loss data in training 0.05614478513598442 0.09723134757326987\n",
            "before loss data in training 0.045714303851127625 0.09723134757326987\n",
            "after loss data in training 0.045714303851127625 0.09716867477068819\n",
            "before loss data in training 0.07668590545654297 0.09716867477068819\n",
            "after loss data in training 0.07668590545654297 0.09714378683713516\n",
            "before loss data in training 0.04590000957250595 0.09714378683713516\n",
            "after loss data in training 0.04590000957250595 0.09708159778705673\n",
            "before loss data in training 0.04943304508924484 0.09708159778705673\n",
            "after loss data in training 0.04943304508924484 0.09702384196560483\n",
            "before loss data in training 0.06993532180786133 0.09702384196560483\n",
            "after loss data in training 0.06993532180786133 0.09699104714701193\n",
            "before loss data in training 0.030659964308142662 0.09699104714701193\n",
            "after loss data in training 0.030659964308142662 0.09691084027538088\n",
            "before loss data in training 0.03999563306570053 0.09691084027538088\n",
            "after loss data in training 0.03999563306570053 0.09684210210242233\n",
            "before loss data in training 0.018609773367643356 0.09684210210242233\n",
            "after loss data in training 0.018609773367643356 0.09674773258645758\n",
            "before loss data in training 0.050244446843862534 0.09674773258645758\n",
            "after loss data in training 0.050244446843862534 0.09669170453134603\n",
            "before loss data in training 0.07617354393005371 0.09669170453134603\n",
            "after loss data in training 0.07617354393005371 0.09666701360402799\n",
            "before loss data in training 0.04609318822622299 0.09666701360402799\n",
            "after loss data in training 0.04609318822622299 0.09660622775621813\n",
            "before loss data in training 0.040378447622060776 0.09660622775621813\n",
            "after loss data in training 0.040378447622060776 0.09653872741992263\n",
            "before loss data in training 0.027753230184316635 0.09653872741992263\n",
            "after loss data in training 0.027753230184316635 0.09645625080453221\n",
            "before loss data in training 0.04896746575832367 0.09645625080453221\n",
            "after loss data in training 0.04896746575832367 0.09639937800806969\n",
            "before loss data in training 0.07251893728971481 0.09639937800806969\n",
            "after loss data in training 0.07251893728971481 0.0963708128875932\n",
            "before loss data in training 0.0684988871216774 0.0963708128875932\n",
            "after loss data in training 0.0684988871216774 0.09633751309575816\n",
            "before loss data in training 0.07765422016382217 0.09633751309575816\n",
            "after loss data in training 0.07765422016382217 0.09631521799679404\n",
            "before loss data in training 0.04642689973115921 0.09631521799679404\n",
            "after loss data in training 0.04642689973115921 0.09625575635404597\n",
            "before loss data in training 0.05061569809913635 0.09625575635404597\n",
            "after loss data in training 0.05061569809913635 0.09620142295136155\n",
            "before loss data in training 0.046287160366773605 0.09620142295136155\n",
            "after loss data in training 0.046287160366773605 0.09614207186624314\n",
            "before loss data in training 0.030204322189092636 0.09614207186624314\n",
            "after loss data in training 0.030204322189092636 0.0960637609996432\n",
            "before loss data in training 0.03016701340675354 0.0960637609996432\n",
            "after loss data in training 0.03016701340675354 0.0959855916667928\n",
            "before loss data in training 0.026137908920645714 0.0959855916667928\n",
            "after loss data in training 0.026137908920645714 0.09590283374884713\n",
            "before loss data in training 0.08684898912906647 0.09590283374884713\n",
            "after loss data in training 0.08684898912906647 0.09589211913982963\n",
            "before loss data in training 0.03410393372178078 0.09589211913982963\n",
            "after loss data in training 0.03410393372178078 0.09581908345966646\n",
            "before loss data in training 0.057282768189907074 0.09581908345966646\n",
            "after loss data in training 0.057282768189907074 0.095773586039041\n",
            "before loss data in training 0.02434496581554413 0.095773586039041\n",
            "after loss data in training 0.02434496581554413 0.0956893541755699\n",
            "before loss data in training 0.020710138604044914 0.0956893541755699\n",
            "after loss data in training 0.020710138604044914 0.0956010394340251\n",
            "before loss data in training 0.04349502921104431 0.0956010394340251\n",
            "after loss data in training 0.04349502921104431 0.09553973824552749\n",
            "before loss data in training 0.02921277843415737 0.09553973824552749\n",
            "after loss data in training 0.02921277843415737 0.09546179822224737\n",
            "before loss data in training 0.03811705857515335 0.09546179822224737\n",
            "after loss data in training 0.03811705857515335 0.09539449218979773\n",
            "before loss data in training 0.07191839814186096 0.09539449218979773\n",
            "after loss data in training 0.07191839814186096 0.0953669703913828\n",
            "before loss data in training 0.05101398751139641 0.0953669703913828\n",
            "after loss data in training 0.05101398751139641 0.09531503481423996\n",
            "before loss data in training 0.03479684516787529 0.09531503481423996\n",
            "after loss data in training 0.03479684516787529 0.09524425330588164\n",
            "before loss data in training 0.022403066977858543 0.09524425330588164\n",
            "after loss data in training 0.022403066977858543 0.0951591584620405\n",
            "before loss data in training 0.056535057723522186 0.0951591584620405\n",
            "after loss data in training 0.056535057723522186 0.09511408949968517\n",
            "before loss data in training 0.016928719356656075 0.09511408949968517\n",
            "after loss data in training 0.016928719356656075 0.09502296435965833\n",
            "before loss data in training 0.049395643174648285 0.09502296435965833\n",
            "after loss data in training 0.049395643174648285 0.09496984757131723\n",
            "before loss data in training 0.0451718270778656 0.09496984757131723\n",
            "after loss data in training 0.0451718270778656 0.09491194289632485\n",
            "before loss data in training 0.036073170602321625 0.09491194289632485\n",
            "after loss data in training 0.036073170602321625 0.09484360518169767\n",
            "before loss data in training 0.10108505189418793 0.09484360518169767\n",
            "after loss data in training 0.10108505189418793 0.09485084583913675\n",
            "before loss data in training 0.054120611399412155 0.09485084583913675\n",
            "after loss data in training 0.054120611399412155 0.09480364973897484\n",
            "before loss data in training 0.04463501274585724 0.09480364973897484\n",
            "after loss data in training 0.04463501274585724 0.09474558418689948\n",
            "before loss data in training 0.06933950632810593 0.09474558418689948\n",
            "after loss data in training 0.06933950632810593 0.09471621299862341\n",
            "before loss data in training 0.04584265872836113 0.09471621299862341\n",
            "after loss data in training 0.04584265872836113 0.09465977702371549\n",
            "before loss data in training 0.0626634955406189 0.09465977702371549\n",
            "after loss data in training 0.0626634955406189 0.09462287243146278\n",
            "before loss data in training 0.057380497455596924 0.09462287243146278\n",
            "after loss data in training 0.057380497455596924 0.09457996646950902\n",
            "before loss data in training 0.03703273832798004 0.09457996646950902\n",
            "after loss data in training 0.03703273832798004 0.09451374411261428\n",
            "before loss data in training 0.04066833108663559 0.09451374411261428\n",
            "after loss data in training 0.04066833108663559 0.09445185283327406\n",
            "before loss data in training 0.07147266715765 0.09445185283327406\n",
            "after loss data in training 0.07147266715765 0.09442547030092548\n",
            "before loss data in training 0.023162849247455597 0.09442547030092548\n",
            "after loss data in training 0.023162849247455597 0.09434374711164398\n",
            "before loss data in training 0.046765103936195374 0.09434374711164398\n",
            "after loss data in training 0.046765103936195374 0.09428924694764003\n",
            "before loss data in training 0.041812025010585785 0.09428924694764003\n",
            "after loss data in training 0.041812025010585785 0.09422920435961137\n",
            "before loss data in training 0.0720682442188263 0.09422920435961137\n",
            "after loss data in training 0.0720682442188263 0.0942038775480219\n",
            "before loss data in training 0.038863252848386765 0.0942038775480219\n",
            "after loss data in training 0.038863252848386765 0.09414070331891272\n",
            "before loss data in training 0.04153706878423691 0.09414070331891272\n",
            "after loss data in training 0.04153706878423691 0.09408072197964855\n",
            "before loss data in training 0.024380095303058624 0.09408072197964855\n",
            "after loss data in training 0.024380095303058624 0.09400133630006245\n",
            "before loss data in training 0.06049996614456177 0.09400133630006245\n",
            "after loss data in training 0.06049996614456177 0.09396322325096632\n",
            "before loss data in training 0.05131138488650322 0.09396322325096632\n",
            "after loss data in training 0.05131138488650322 0.09391475525282487\n",
            "before loss data in training 0.03455284237861633 0.09391475525282487\n",
            "after loss data in training 0.03455284237861633 0.09384737510200285\n",
            "before loss data in training 0.052581578493118286 0.09384737510200285\n",
            "after loss data in training 0.052581578493118286 0.09380058848453246\n",
            "before loss data in training 0.04796283692121506 0.09380058848453246\n",
            "after loss data in training 0.04796283692121506 0.09374867710110854\n",
            "before loss data in training 0.0374571830034256 0.09374867710110854\n",
            "after loss data in training 0.0374571830034256 0.09368499894036456\n",
            "before loss data in training 0.06379909068346024 0.09368499894036456\n",
            "after loss data in training 0.06379909068346024 0.09365122955250364\n",
            "before loss data in training 0.04134339466691017 0.09365122955250364\n",
            "after loss data in training 0.04134339466691017 0.09359219136414519\n",
            "before loss data in training 0.04310866445302963 0.09359219136414519\n",
            "after loss data in training 0.04310866445302963 0.09353527645218226\n",
            "before loss data in training 0.03570263460278511 0.09353527645218226\n",
            "after loss data in training 0.03570263460278511 0.09347014960325277\n",
            "before loss data in training 0.05497779697179794 0.09347014960325277\n",
            "after loss data in training 0.05497779697179794 0.09342685111885293\n",
            "before loss data in training 0.031472139060497284 0.09342685111885293\n",
            "after loss data in training 0.031472139060497284 0.0933572390828323\n",
            "before loss data in training 0.057750873267650604 0.0933572390828323\n",
            "after loss data in training 0.057750873267650604 0.09331727683163682\n",
            "before loss data in training 0.07467107474803925 0.09331727683163682\n",
            "after loss data in training 0.07467107474803925 0.09329637301764175\n",
            "before loss data in training 0.037913978099823 0.09329637301764175\n",
            "after loss data in training 0.037913978099823 0.09323435465827129\n",
            "before loss data in training 0.04275207966566086 0.09323435465827129\n",
            "after loss data in training 0.04275207966566086 0.09317788678915204\n",
            "before loss data in training 0.031826652586460114 0.09317788678915204\n",
            "after loss data in training 0.031826652586460114 0.0931093379241211\n",
            "before loss data in training 0.0374494343996048 0.0931093379241211\n",
            "after loss data in training 0.0374494343996048 0.09304721749608034\n",
            "before loss data in training 0.059626467525959015 0.09304721749608034\n",
            "after loss data in training 0.059626467525959015 0.09300995913490964\n",
            "before loss data in training 0.07632891833782196 0.09300995913490964\n",
            "after loss data in training 0.07632891833782196 0.09299138336564786\n",
            "before loss data in training 0.04310433194041252 0.09299138336564786\n",
            "after loss data in training 0.04310433194041252 0.09293589165104804\n",
            "before loss data in training 0.0628800317645073 0.09293589165104804\n",
            "after loss data in training 0.0628800317645073 0.09290249625117411\n",
            "before loss data in training 0.04070325940847397 0.09290249625117411\n",
            "after loss data in training 0.04070325940847397 0.09284456147110452\n",
            "before loss data in training 0.09569695591926575 0.09284456147110452\n",
            "after loss data in training 0.09569695591926575 0.09284772377093618\n",
            "before loss data in training 0.07547944784164429 0.09284772377093618\n",
            "after loss data in training 0.07547944784164429 0.0928284897998074\n",
            "before loss data in training 0.029369916766881943 0.0928284897998074\n",
            "after loss data in training 0.029369916766881943 0.09275829226326655\n",
            "before loss data in training 0.03607964515686035 0.09275829226326655\n",
            "after loss data in training 0.03607964515686035 0.09269566392392246\n",
            "before loss data in training 0.03579083830118179 0.09269566392392246\n",
            "after loss data in training 0.03579083830118179 0.09263285506561922\n",
            "before loss data in training 0.04870256781578064 0.09263285506561922\n",
            "after loss data in training 0.04870256781578064 0.09258442034979801\n",
            "before loss data in training 0.041912518441677094 0.09258442034979801\n",
            "after loss data in training 0.041912518441677094 0.09252861429042783\n",
            "before loss data in training 0.026033466681838036 0.09252861429042783\n",
            "after loss data in training 0.026033466681838036 0.09245546231286063\n",
            "before loss data in training 0.0619145929813385 0.09245546231286063\n",
            "after loss data in training 0.0619145929813385 0.09242190091799082\n",
            "before loss data in training 0.05232871323823929 0.09242190091799082\n",
            "after loss data in training 0.05232871323823929 0.09237789083272216\n",
            "before loss data in training 0.0672689899802208 0.09237789083272216\n",
            "after loss data in training 0.0672689899802208 0.09235035914319091\n",
            "before loss data in training 0.04990290850400925 0.09235035914319091\n",
            "after loss data in training 0.04990290850400925 0.0923038668642871\n",
            "before loss data in training 0.04865415021777153 0.0923038668642871\n",
            "after loss data in training 0.04865415021777153 0.09225611006270448\n",
            "before loss data in training 0.048791900277137756 0.09225611006270448\n",
            "after loss data in training 0.048791900277137756 0.09220860819408638\n",
            "before loss data in training 0.06410336494445801 0.09220860819408638\n",
            "after loss data in training 0.06410336494445801 0.09217792561411954\n",
            "before loss data in training 0.0533284954726696 0.09217792561411954\n",
            "after loss data in training 0.0533284954726696 0.09213555982334369\n",
            "before loss data in training 0.05183633789420128 0.09213555982334369\n",
            "after loss data in training 0.05183633789420128 0.09209166088878036\n",
            "before loss data in training 0.04535367339849472 0.09209166088878036\n",
            "after loss data in training 0.04535367339849472 0.09204080344863859\n",
            "before loss data in training 0.021274453029036522 0.09204080344863859\n",
            "after loss data in training 0.021274453029036522 0.09196388350253032\n",
            "before loss data in training 0.059816956520080566 0.09196388350253032\n",
            "after loss data in training 0.059816956520080566 0.0919289791301281\n",
            "before loss data in training 0.05410599708557129 0.0919289791301281\n",
            "after loss data in training 0.05410599708557129 0.09188795637302988\n",
            "before loss data in training 0.029364094138145447 0.09188795637302988\n",
            "after loss data in training 0.029364094138145447 0.09182021654395633\n",
            "before loss data in training 0.08085811883211136 0.09182021654395633\n",
            "after loss data in training 0.08085811883211136 0.09180835280184395\n",
            "before loss data in training 0.05658385157585144 0.09180835280184395\n",
            "after loss data in training 0.05658385157585144 0.09177027225997801\n",
            "before loss data in training 0.04353035241365433 0.09177027225997801\n",
            "after loss data in training 0.04353035241365433 0.09171817731413964\n",
            "before loss data in training 0.048456065356731415 0.09171817731413964\n",
            "after loss data in training 0.048456065356731415 0.09167150836920177\n",
            "before loss data in training 0.07261411845684052 0.09167150836920177\n",
            "after loss data in training 0.07261411845684052 0.09165097238869276\n",
            "before loss data in training 0.04269849509000778 0.09165097238869276\n",
            "after loss data in training 0.04269849509000778 0.09159827865640138\n",
            "before loss data in training 0.05643803998827934 0.09159827865640138\n",
            "after loss data in training 0.05643803998827934 0.09156047194815609\n",
            "before loss data in training 0.03805435821413994 0.09156047194815609\n",
            "after loss data in training 0.03805435821413994 0.09150300029000999\n",
            "before loss data in training 0.03322996944189072 0.09150300029000999\n",
            "after loss data in training 0.03322996944189072 0.09144047557879956\n",
            "before loss data in training 0.055909812450408936 0.09144047557879956\n",
            "after loss data in training 0.055909812450408936 0.09140239341038757\n",
            "before loss data in training 0.0689949244260788 0.09140239341038757\n",
            "after loss data in training 0.0689949244260788 0.09137840254423735\n",
            "before loss data in training 0.024312559515237808 0.09137840254423735\n",
            "after loss data in training 0.024312559515237808 0.09130667436987477\n",
            "before loss data in training 0.07277294993400574 0.09130667436987477\n",
            "after loss data in training 0.07277294993400574 0.09128687338222961\n",
            "before loss data in training 0.08172506839036942 0.09128687338222961\n",
            "after loss data in training 0.08172506839036942 0.09127666868106434\n",
            "before loss data in training 0.04398632049560547 0.09127666868106434\n",
            "after loss data in training 0.04398632049560547 0.09122625253161289\n",
            "before loss data in training 0.033362001180648804 0.09122625253161289\n",
            "after loss data in training 0.033362001180648804 0.0911646292607386\n",
            "before loss data in training 0.06296305358409882 0.0911646292607386\n",
            "after loss data in training 0.06296305358409882 0.09113462758448684\n",
            "before loss data in training 0.03923646733164787 0.09113462758448684\n",
            "after loss data in training 0.03923646733164787 0.09107947544819264\n",
            "before loss data in training 0.07016415894031525 0.09107947544819264\n",
            "after loss data in training 0.07016415894031525 0.0910572723521121\n",
            "before loss data in training 0.030566822737455368 0.0910572723521121\n",
            "after loss data in training 0.030566822737455368 0.0909931255338569\n",
            "before loss data in training 0.07411429286003113 0.0909931255338569\n",
            "after loss data in training 0.07411429286003113 0.09097524541449903\n",
            "before loss data in training 0.05476372689008713 0.09097524541449903\n",
            "after loss data in training 0.05476372689008713 0.09093692634727743\n",
            "before loss data in training 0.03206425532698631 0.09093692634727743\n",
            "after loss data in training 0.03206425532698631 0.09087469307981412\n",
            "before loss data in training 0.06039543077349663 0.09087469307981412\n",
            "after loss data in training 0.06039543077349663 0.09084250800874093\n",
            "before loss data in training 0.05833534523844719 0.09084250800874093\n",
            "after loss data in training 0.05833534523844719 0.09080821775265412\n",
            "before loss data in training 0.05435646325349808 0.09080821775265412\n",
            "after loss data in training 0.05435646325349808 0.09076980705244425\n",
            "before loss data in training 0.09212376922369003 0.09076980705244425\n",
            "after loss data in training 0.09212376922369003 0.0907712322757824\n",
            "before loss data in training 0.04941219091415405 0.0907712322757824\n",
            "after loss data in training 0.04941219091415405 0.09072774222177438\n",
            "before loss data in training 0.018664047122001648 0.09072774222177438\n",
            "after loss data in training 0.018664047122001648 0.09065204506305613\n",
            "before loss data in training 0.058659326285123825 0.09065204506305613\n",
            "after loss data in training 0.058659326285123825 0.09061847452918631\n",
            "before loss data in training 0.044533099979162216 0.09061847452918631\n",
            "after loss data in training 0.044533099979162216 0.09057016700869362\n",
            "before loss data in training 0.041412901133298874 0.09057016700869362\n",
            "after loss data in training 0.041412901133298874 0.09051869343186075\n",
            "before loss data in training 0.05312315747141838 0.09051869343186075\n",
            "after loss data in training 0.05312315747141838 0.09047957676244607\n",
            "before loss data in training 0.04366223141551018 0.09047957676244607\n",
            "after loss data in training 0.04366223141551018 0.09043065581642001\n",
            "before loss data in training 0.0735853910446167 0.09043065581642001\n",
            "after loss data in training 0.0735853910446167 0.09041307203273337\n",
            "before loss data in training 0.048919133841991425 0.09041307203273337\n",
            "after loss data in training 0.048919133841991425 0.09036980410969818\n",
            "before loss data in training 0.04073728621006012 0.09036980410969818\n",
            "after loss data in training 0.04073728621006012 0.0903181035702194\n",
            "before loss data in training 0.07848571240901947 0.0903181035702194\n",
            "after loss data in training 0.07848571240901947 0.09030579098836591\n",
            "before loss data in training 0.06335815042257309 0.09030579098836591\n",
            "after loss data in training 0.06335815042257309 0.09027777888798567\n",
            "before loss data in training 0.030642252415418625 0.09027777888798567\n",
            "after loss data in training 0.030642252415418625 0.09021585206921873\n",
            "before loss data in training 0.04164842516183853 0.09021585206921873\n",
            "after loss data in training 0.04164842516183853 0.09016547092097456\n",
            "before loss data in training 0.05797653645277023 0.09016547092097456\n",
            "after loss data in training 0.05797653645277023 0.09013211451219921\n",
            "before loss data in training 0.07839076966047287 0.09013211451219921\n",
            "after loss data in training 0.07839076966047287 0.09011995991090344\n",
            "before loss data in training 0.023774858564138412 0.09011995991090344\n",
            "after loss data in training 0.023774858564138412 0.09005135070578786\n",
            "before loss data in training 0.04493562877178192 0.09005135070578786\n",
            "after loss data in training 0.04493562877178192 0.09000474355502959\n",
            "before loss data in training 0.06763838231563568 0.09000474355502959\n",
            "after loss data in training 0.06763838231563568 0.08998166165488573\n",
            "before loss data in training 0.051789816468954086 0.08998166165488573\n",
            "after loss data in training 0.051789816468954086 0.08994228861861157\n",
            "before loss data in training 0.04711982607841492 0.08994228861861157\n",
            "after loss data in training 0.04711982607841492 0.0898981872153776\n",
            "before loss data in training 0.04654855281114578 0.0898981872153776\n",
            "after loss data in training 0.04654855281114578 0.08985358882607283\n",
            "before loss data in training 0.03545203432440758 0.08985358882607283\n",
            "after loss data in training 0.03545203432440758 0.08979767767036712\n",
            "before loss data in training 0.07579465955495834 0.08979767767036712\n",
            "after loss data in training 0.07579465955495834 0.08978330085505355\n",
            "before loss data in training 0.04882454872131348 0.08978330085505355\n",
            "after loss data in training 0.04882454872131348 0.08974129187850613\n",
            "before loss data in training 0.057829033583402634 0.08974129187850613\n",
            "after loss data in training 0.057829033583402634 0.08970859489254804\n",
            "before loss data in training 0.09691797196865082 0.08970859489254804\n",
            "after loss data in training 0.09691797196865082 0.08971597398883883\n",
            "before loss data in training 0.05176896974444389 0.08971597398883883\n",
            "after loss data in training 0.05176896974444389 0.08967717337100203\n",
            "before loss data in training 0.04150424152612686 0.08967717337100203\n",
            "after loss data in training 0.04150424152612686 0.08962796710762627\n",
            "before loss data in training 0.04779841750860214 0.08962796710762627\n",
            "after loss data in training 0.04779841750860214 0.08958528389374971\n",
            "before loss data in training 0.08497779816389084 0.08958528389374971\n",
            "after loss data in training 0.08497779816389084 0.08958058717027381\n",
            "before loss data in training 0.05870059132575989 0.08958058717027381\n",
            "after loss data in training 0.05870059132575989 0.08954914114599223\n",
            "before loss data in training 0.042701851576566696 0.08954914114599223\n",
            "after loss data in training 0.042701851576566696 0.0895014836794923\n",
            "before loss data in training 0.04937976598739624 0.0895014836794923\n",
            "after loss data in training 0.04937976598739624 0.08946070957614667\n",
            "before loss data in training 0.06241871044039726 0.08946070957614667\n",
            "after loss data in training 0.06241871044039726 0.08943325576991748\n",
            "before loss data in training 0.08229231089353561 0.08943325576991748\n",
            "after loss data in training 0.08229231089353561 0.08942601343231465\n",
            "before loss data in training 0.04521428048610687 0.08942601343231465\n",
            "after loss data in training 0.04521428048610687 0.08938121937664474\n",
            "before loss data in training 0.041209593415260315 0.08938121937664474\n",
            "after loss data in training 0.041209593415260315 0.08933246267020609\n",
            "before loss data in training 0.057876408100128174 0.08933246267020609\n",
            "after loss data in training 0.057876408100128174 0.08930065675051946\n",
            "before loss data in training 0.05120999738574028 0.08930065675051946\n",
            "after loss data in training 0.05120999738574028 0.08926218133701969\n",
            "before loss data in training 0.04232163354754448 0.08926218133701969\n",
            "after loss data in training 0.04232163354754448 0.0892148144875853\n",
            "before loss data in training 0.05943542718887329 0.0892148144875853\n",
            "after loss data in training 0.05943542718887329 0.0891847949439374\n",
            "before loss data in training 0.027713872492313385 0.0891847949439374\n",
            "after loss data in training 0.027713872492313385 0.08912289069172026\n",
            "before loss data in training 0.032163217663764954 0.08912289069172026\n",
            "after loss data in training 0.032163217663764954 0.08906558719772835\n",
            "before loss data in training 0.03951812535524368 0.08906558719772835\n",
            "after loss data in training 0.03951812535524368 0.08901579075366554\n",
            "before loss data in training 0.04797592759132385 0.08901579075366554\n",
            "after loss data in training 0.04797592759132385 0.08897458607177565\n",
            "before loss data in training 0.03322528302669525 0.08897458607177565\n",
            "after loss data in training 0.03322528302669525 0.08891866901756794\n",
            "before loss data in training 0.05786043405532837 0.08891866901756794\n",
            "after loss data in training 0.05786043405532837 0.08888754854165387\n",
            "before loss data in training 0.03229900449514389 0.08888754854165387\n",
            "after loss data in training 0.03229900449514389 0.08883090335241813\n",
            "before loss data in training 0.03945499658584595 0.08883090335241813\n",
            "after loss data in training 0.03945499658584595 0.08878152744565156\n",
            "before loss data in training 0.08507803082466125 0.08878152744565156\n",
            "after loss data in training 0.08507803082466125 0.08877782764882738\n",
            "before loss data in training 0.06741953641176224 0.08877782764882738\n",
            "after loss data in training 0.06741953641176224 0.08875651198891016\n",
            "before loss data in training 0.07641780376434326 0.08875651198891016\n",
            "after loss data in training 0.07641780376434326 0.08874421018609403\n",
            "before loss data in training 0.05680716037750244 0.08874421018609403\n",
            "after loss data in training 0.05680716037750244 0.0887124003755277\n",
            "before loss data in training 0.018977947533130646 0.0887124003755277\n",
            "after loss data in training 0.018977947533130646 0.08864301286026163\n",
            "before loss data in training 0.02823752537369728 0.08864301286026163\n",
            "after loss data in training 0.02823752537369728 0.08858296764407221\n",
            "before loss data in training 0.021561166271567345 0.08858296764407221\n",
            "after loss data in training 0.021561166271567345 0.08851641173406972\n",
            "before loss data in training 0.037323057651519775 0.08851641173406972\n",
            "after loss data in training 0.037323057651519775 0.08846562467644815\n",
            "before loss data in training 0.04353141039609909 0.08846562467644815\n",
            "after loss data in training 0.04353141039609909 0.08842109126288983\n",
            "before loss data in training 0.03651764616370201 0.08842109126288983\n",
            "after loss data in training 0.03651764616370201 0.08836970171328667\n",
            "before loss data in training 0.043959956616163254 0.08836970171328667\n",
            "after loss data in training 0.043959956616163254 0.08832577516027271\n",
            "before loss data in training 0.026333943009376526 0.08832577516027271\n",
            "after loss data in training 0.026333943009376526 0.08826451840913546\n",
            "before loss data in training 0.04025958850979805 0.08826451840913546\n",
            "after loss data in training 0.04025958850979805 0.08821712953460502\n",
            "before loss data in training 0.06698755919933319 0.08821712953460502\n",
            "after loss data in training 0.06698755919933319 0.0881961930747083\n",
            "before loss data in training 0.05147881433367729 0.0881961930747083\n",
            "after loss data in training 0.05147881433367729 0.08816001831732798\n",
            "before loss data in training 0.09262467920780182 0.08816001831732798\n",
            "after loss data in training 0.09262467920780182 0.08816441266859813\n",
            "before loss data in training 0.07528285682201385 0.08816441266859813\n",
            "after loss data in training 0.07528285682201385 0.08815174643866049\n",
            "before loss data in training 0.02760249190032482 0.08815174643866049\n",
            "after loss data in training 0.02760249190032482 0.0880922677996248\n",
            "before loss data in training 0.035094812512397766 0.0880922677996248\n",
            "after loss data in training 0.035094812512397766 0.08804025852063832\n",
            "before loss data in training 0.03404785320162773 0.08804025852063832\n",
            "after loss data in training 0.03404785320162773 0.0879873247899334\n",
            "before loss data in training 0.06563908606767654 0.0879873247899334\n",
            "after loss data in training 0.06563908606767654 0.08796543621136116\n",
            "before loss data in training 0.03659516200423241 0.08796543621136116\n",
            "after loss data in training 0.03659516200423241 0.0879151717551898\n",
            "before loss data in training 0.03824777156114578 0.0879151717551898\n",
            "after loss data in training 0.03824777156114578 0.08786662102186228\n",
            "before loss data in training 0.03502605855464935 0.08786662102186228\n",
            "after loss data in training 0.03502605855464935 0.0878150189100779\n",
            "before loss data in training 0.04070879518985748 0.0878150189100779\n",
            "after loss data in training 0.04070879518985748 0.08776906161864353\n",
            "before loss data in training 0.048509739339351654 0.08776906161864353\n",
            "after loss data in training 0.048509739339351654 0.08773079717197756\n",
            "before loss data in training 0.04551778361201286 0.08773079717197756\n",
            "after loss data in training 0.04551778361201286 0.08768969394553164\n",
            "before loss data in training 0.024700617417693138 0.08768969394553164\n",
            "after loss data in training 0.024700617417693138 0.08762842052478471\n",
            "before loss data in training 0.04631505906581879 0.08762842052478471\n",
            "after loss data in training 0.04631505906581879 0.087588271485466\n",
            "before loss data in training 0.03748190402984619 0.087588271485466\n",
            "after loss data in training 0.03748190402984619 0.08753962452677122\n",
            "before loss data in training 0.052609633654356 0.08753962452677122\n",
            "after loss data in training 0.052609633654356 0.08750574480720534\n",
            "before loss data in training 0.06619221717119217 0.08750574480720534\n",
            "after loss data in training 0.06619221717119217 0.08748509216414718\n",
            "before loss data in training 0.04550255462527275 0.08748509216414718\n",
            "after loss data in training 0.04550255462527275 0.08744445079189271\n",
            "before loss data in training 0.0413789227604866 0.08744445079189271\n",
            "after loss data in training 0.0413789227604866 0.08739989999108864\n",
            "before loss data in training 0.051658689975738525 0.08739989999108864\n",
            "after loss data in training 0.051658689975738525 0.0873653674210255\n",
            "before loss data in training 0.0505673885345459 0.0873653674210255\n",
            "after loss data in training 0.0505673885345459 0.08732984813638604\n",
            "before loss data in training 0.04929868131875992 0.08732984813638604\n",
            "after loss data in training 0.04929868131875992 0.08729317391573259\n",
            "before loss data in training 0.04884660243988037 0.08729317391573259\n",
            "after loss data in training 0.04884660243988037 0.08725613482953234\n",
            "before loss data in training 0.04851006716489792 0.08725613482953234\n",
            "after loss data in training 0.04851006716489792 0.08721884313784357\n",
            "before loss data in training 0.06261645257472992 0.08721884313784357\n",
            "after loss data in training 0.06261645257472992 0.08719518699307136\n",
            "before loss data in training 0.05359238386154175 0.08719518699307136\n",
            "after loss data in training 0.05359238386154175 0.08716290764328122\n",
            "before loss data in training 0.05748271197080612 0.08716290764328122\n",
            "after loss data in training 0.05748271197080612 0.087134423770275\n",
            "before loss data in training 0.04242327809333801 0.087134423770275\n",
            "after loss data in training 0.04242327809333801 0.08709155594124629\n",
            "before loss data in training 0.03746107965707779 0.08709155594124629\n",
            "after loss data in training 0.03746107965707779 0.08704401717085916\n",
            "before loss data in training 0.052976273000240326 0.08704401717085916\n",
            "after loss data in training 0.052976273000240326 0.08701141645873417\n",
            "before loss data in training 0.038946863263845444 0.08701141645873417\n",
            "after loss data in training 0.038946863263845444 0.08696546564306028\n",
            "before loss data in training 0.052413564175367355 0.08696546564306028\n",
            "after loss data in training 0.052413564175367355 0.08693246478205961\n",
            "before loss data in training 0.023067383095622063 0.08693246478205961\n",
            "after loss data in training 0.023067383095622063 0.08687152481861836\n",
            "before loss data in training 0.022940684109926224 0.08687152481861836\n",
            "after loss data in training 0.022940684109926224 0.08681058026122208\n",
            "before loss data in training 0.03593435138463974 0.08681058026122208\n",
            "after loss data in training 0.03593435138463974 0.08676212670991106\n",
            "before loss data in training 0.08674266189336777 0.08676212670991106\n",
            "after loss data in training 0.08674266189336777 0.0867621081896289\n",
            "before loss data in training 0.048256389796733856 0.0867621081896289\n",
            "after loss data in training 0.048256389796733856 0.0867255057957193\n",
            "before loss data in training 0.017508160322904587 0.0867255057957193\n",
            "after loss data in training 0.017508160322904587 0.08665977232423515\n",
            "before loss data in training 0.03034268692135811 0.08665977232423515\n",
            "after loss data in training 0.03034268692135811 0.0866063405544032\n",
            "before loss data in training 0.09777792543172836 0.0866063405544032\n",
            "after loss data in training 0.09777792543172836 0.0866169297343817\n",
            "before loss data in training 0.04581330344080925 0.0866169297343817\n",
            "after loss data in training 0.04581330344080925 0.08657828993675522\n",
            "before loss data in training 0.03641030937433243 0.08657828993675522\n",
            "after loss data in training 0.03641030937433243 0.08653082732505946\n",
            "before loss data in training 0.06531863659620285 0.08653082732505946\n",
            "after loss data in training 0.06531863659620285 0.08651077799544805\n",
            "before loss data in training 0.03866102173924446 0.08651077799544805\n",
            "after loss data in training 0.03866102173924446 0.08646559408963483\n",
            "before loss data in training 0.05999871715903282 0.08646559408963483\n",
            "after loss data in training 0.05999871715903282 0.0864406253378135\n",
            "before loss data in training 0.04754602536559105 0.0864406253378135\n",
            "after loss data in training 0.04754602536559105 0.08640396690240142\n",
            "before loss data in training 0.03341980278491974 0.08640396690240142\n",
            "after loss data in training 0.03341980278491974 0.08635407597573712\n",
            "before loss data in training 0.05430722236633301 0.08635407597573712\n",
            "after loss data in training 0.05430722236633301 0.08632392841824944\n",
            "before loss data in training 0.03252720087766647 0.08632392841824944\n",
            "after loss data in training 0.03252720087766647 0.08627336758409476\n",
            "before loss data in training 0.035030465573072433 0.08627336758409476\n",
            "after loss data in training 0.035030465573072433 0.08622525218314545\n",
            "before loss data in training 0.059767186641693115 0.08622525218314545\n",
            "after loss data in training 0.059767186641693115 0.08620043223423227\n",
            "before loss data in training 0.05675062537193298 0.08620043223423227\n",
            "after loss data in training 0.05675062537193298 0.0861728316654766\n",
            "before loss data in training 0.024790406227111816 0.0861728316654766\n",
            "after loss data in training 0.024790406227111816 0.08611535748435453\n",
            "before loss data in training 0.03131760284304619 0.08611535748435453\n",
            "after loss data in training 0.03131760284304619 0.08606409672229531\n",
            "before loss data in training 0.04556301236152649 0.08606409672229531\n",
            "after loss data in training 0.04556301236152649 0.08602624524158431\n",
            "before loss data in training 0.0458800308406353 0.08602624524158431\n",
            "after loss data in training 0.0458800308406353 0.08598876044755914\n",
            "before loss data in training 0.05509702116250992 0.08598876044755914\n",
            "after loss data in training 0.05509702116250992 0.08595994352658429\n",
            "before loss data in training 0.05049292370676994 0.08595994352658429\n",
            "after loss data in training 0.05049292370676994 0.08592688945405882\n",
            "before loss data in training 0.03175003454089165 0.08592688945405882\n",
            "after loss data in training 0.03175003454089165 0.08587644545507077\n",
            "before loss data in training 0.04565487056970596 0.08587644545507077\n",
            "after loss data in training 0.04565487056970596 0.08583903003657276\n",
            "before loss data in training 0.06967439502477646 0.08583903003657276\n",
            "after loss data in training 0.06967439502477646 0.08582400714158038\n",
            "before loss data in training 0.044650666415691376 0.08582400714158038\n",
            "after loss data in training 0.044650666415691376 0.08578577748445328\n",
            "before loss data in training 0.028849244117736816 0.08578577748445328\n",
            "after loss data in training 0.028849244117736816 0.08573296066314834\n",
            "before loss data in training 0.02878233790397644 0.08573296066314834\n",
            "after loss data in training 0.02878233790397644 0.08568017973380712\n",
            "before loss data in training 0.02893364429473877 0.08568017973380712\n",
            "after loss data in training 0.02893364429473877 0.08562763664543761\n",
            "before loss data in training 0.04260094463825226 0.08562763664543761\n",
            "after loss data in training 0.04260094463825226 0.08558783397013033\n",
            "before loss data in training 0.049234770238399506 0.08558783397013033\n",
            "after loss data in training 0.049234770238399506 0.08555423594450026\n",
            "before loss data in training 0.02870006114244461 0.08555423594450026\n",
            "after loss data in training 0.02870006114244461 0.08550173901485847\n",
            "before loss data in training 0.052660197019577026 0.08550173901485847\n",
            "after loss data in training 0.052660197019577026 0.08547144238940156\n",
            "before loss data in training 0.036368317902088165 0.08547144238940156\n",
            "after loss data in training 0.036368317902088165 0.08542618605346855\n",
            "before loss data in training 0.035829149186611176 0.08542618605346855\n",
            "after loss data in training 0.035829149186611176 0.08538051659042356\n",
            "before loss data in training 0.0702747255563736 0.08538051659042356\n",
            "after loss data in training 0.0702747255563736 0.08536661981854311\n",
            "before loss data in training 0.05879790708422661 0.08536661981854311\n",
            "after loss data in training 0.05879790708422661 0.08534220004580936\n",
            "before loss data in training 0.08001904934644699 0.08534220004580936\n",
            "after loss data in training 0.08001904934644699 0.08533731193681086\n",
            "before loss data in training 0.016763320192694664 0.08533731193681086\n",
            "after loss data in training 0.016763320192694664 0.08527440001777957\n",
            "before loss data in training 0.05327196046710014 0.08527440001777957\n",
            "after loss data in training 0.05327196046710014 0.08524506689261854\n",
            "before loss data in training 0.05203656479716301 0.08524506689261854\n",
            "after loss data in training 0.05203656479716301 0.08521465617641391\n",
            "before loss data in training 0.03724117577075958 0.08521465617641391\n",
            "after loss data in training 0.03724117577075958 0.0851707646115414\n",
            "before loss data in training 0.06929200887680054 0.0851707646115414\n",
            "after loss data in training 0.06929200887680054 0.08515625020959008\n",
            "before loss data in training 0.03457128629088402 0.08515625020959008\n",
            "after loss data in training 0.03457128629088402 0.08511005389550907\n",
            "before loss data in training 0.06198924034833908 0.08511005389550907\n",
            "after loss data in training 0.06198924034833908 0.08508895826271057\n",
            "before loss data in training 0.05138494819402695 0.08508895826271057\n",
            "after loss data in training 0.05138494819402695 0.08505823446137174\n",
            "before loss data in training 0.03076178953051567 0.08505823446137174\n",
            "after loss data in training 0.03076178953051567 0.08500878414722707\n",
            "before loss data in training 0.034728411585092545 0.08500878414722707\n",
            "after loss data in training 0.034728411585092545 0.08496303312578746\n",
            "before loss data in training 0.03273897245526314 0.08496303312578746\n",
            "after loss data in training 0.03273897245526314 0.08491555670699608\n",
            "before loss data in training 0.01995021104812622 0.08491555670699608\n",
            "after loss data in training 0.01995021104812622 0.08485655094345487\n",
            "before loss data in training 0.041226405650377274 0.08485655094345487\n",
            "after loss data in training 0.041226405650377274 0.08481695916006733\n",
            "before loss data in training 0.03487923741340637 0.08481695916006733\n",
            "after loss data in training 0.03487923741340637 0.08477168470698786\n",
            "before loss data in training 0.07066655158996582 0.08477168470698786\n",
            "after loss data in training 0.07066655158996582 0.0847589083182949\n",
            "before loss data in training 0.0674377903342247 0.0847589083182949\n",
            "after loss data in training 0.0674377903342247 0.08474323309839982\n",
            "before loss data in training 0.005436871200799942 0.08474323309839982\n",
            "after loss data in training 0.005436871200799942 0.08467152752706383\n",
            "before loss data in training 0.031510017812252045 0.08467152752706383\n",
            "after loss data in training 0.031510017812252045 0.08462350448305768\n",
            "before loss data in training 0.0711122378706932 0.08462350448305768\n",
            "after loss data in training 0.0711122378706932 0.0846113101991115\n",
            "before loss data in training 0.09264671802520752 0.0846113101991115\n",
            "after loss data in training 0.09264671802520752 0.08461855583285911\n",
            "before loss data in training 0.0646883100271225 0.08461855583285911\n",
            "after loss data in training 0.0646883100271225 0.08460060065645755\n",
            "before loss data in training 0.025322165340185165 0.08460060065645755\n",
            "after loss data in training 0.025322165340185165 0.08454724472908016\n",
            "before loss data in training 0.037292905151844025 0.08454724472908016\n",
            "after loss data in training 0.037292905151844025 0.08450474981938841\n",
            "before loss data in training 0.056953683495521545 0.08450474981938841\n",
            "after loss data in training 0.056953683495521545 0.0844799959412897\n",
            "before loss data in training 0.08120854198932648 0.0844799959412897\n",
            "after loss data in training 0.08120854198932648 0.08447705926808327\n",
            "before loss data in training 0.04628174751996994 0.08447705926808327\n",
            "after loss data in training 0.04628174751996994 0.08444280338310739\n",
            "before loss data in training 0.03552193194627762 0.08444280338310739\n",
            "after loss data in training 0.03552193194627762 0.08439896747680199\n",
            "before loss data in training 0.0509389266371727 0.08439896747680199\n",
            "after loss data in training 0.0509389266371727 0.08436901220299749\n",
            "before loss data in training 0.022434640675783157 0.08436901220299749\n",
            "after loss data in training 0.022434640675783157 0.08431361473293737\n",
            "before loss data in training 0.05443651229143143 0.08431361473293737\n",
            "after loss data in training 0.05443651229143143 0.08428691490948652\n",
            "before loss data in training 0.04225144907832146 0.08428691490948652\n",
            "after loss data in training 0.04225144907832146 0.08424938324356583\n",
            "before loss data in training 0.036508090794086456 0.08424938324356583\n",
            "after loss data in training 0.036508090794086456 0.08420679511470813\n",
            "before loss data in training 0.03387119993567467 0.08420679511470813\n",
            "after loss data in training 0.03387119993567467 0.0841619327304131\n",
            "before loss data in training 0.04860445857048035 0.0841619327304131\n",
            "after loss data in training 0.04860445857048035 0.08413026979705608\n",
            "before loss data in training 0.021055271849036217 0.08413026979705608\n",
            "after loss data in training 0.021055271849036217 0.08407415325083899\n",
            "before loss data in training 0.03826076537370682 0.08407415325083899\n",
            "after loss data in training 0.03826076537370682 0.08403343023939265\n",
            "before loss data in training 0.07641445100307465 0.08403343023939265\n",
            "after loss data in training 0.07641445100307465 0.08402666382799272\n",
            "before loss data in training 0.03755377233028412 0.08402666382799272\n",
            "after loss data in training 0.03755377233028412 0.08398542789942333\n",
            "before loss data in training 0.05597839877009392 0.08398542789942333\n",
            "after loss data in training 0.05597839877009392 0.08396059897289024\n",
            "before loss data in training 0.057651814073324203 0.08396059897289024\n",
            "after loss data in training 0.057651814073324203 0.08393729624047255\n",
            "before loss data in training 0.027034595608711243 0.08393729624047255\n",
            "after loss data in training 0.027034595608711243 0.08388693986823206\n",
            "before loss data in training 0.033233024179935455 0.08388693986823206\n",
            "after loss data in training 0.033233024179935455 0.08384215302854303\n",
            "before loss data in training 0.055787887424230576 0.08384215302854303\n",
            "after loss data in training 0.055787887424230576 0.08381737010839788\n",
            "before loss data in training 0.05271521210670471 0.08381737010839788\n",
            "after loss data in training 0.05271521210670471 0.08378991895393918\n",
            "before loss data in training 0.061354827135801315 0.08378991895393918\n",
            "after loss data in training 0.061354827135801315 0.08377013492235352\n",
            "before loss data in training 0.03963657096028328 0.08377013492235352\n",
            "after loss data in training 0.03963657096028328 0.08373125072503011\n",
            "before loss data in training 0.023515647277235985 0.08373125072503011\n",
            "after loss data in training 0.023515647277235985 0.08367824403185424\n",
            "before loss data in training 0.05455244705080986 0.08367824403185424\n",
            "after loss data in training 0.05455244705080986 0.08365262767567039\n",
            "before loss data in training 0.063055619597435 0.08365262767567039\n",
            "after loss data in training 0.063055619597435 0.08363452837155946\n",
            "before loss data in training 0.039420779794454575 0.08363452837155946\n",
            "after loss data in training 0.039420779794454575 0.08359571033066648\n",
            "before loss data in training 0.016001559793949127 0.08359571033066648\n",
            "after loss data in training 0.016001559793949127 0.08353641721616059\n",
            "before loss data in training 0.03201863914728165 0.08353641721616059\n",
            "after loss data in training 0.03201863914728165 0.08349126578928165\n",
            "before loss data in training 0.04363784193992615 0.08349126578928165\n",
            "after loss data in training 0.04363784193992615 0.08345636786997399\n",
            "before loss data in training 0.047853320837020874 0.08345636786997399\n",
            "after loss data in training 0.047853320837020874 0.08342521909741672\n",
            "before loss data in training 0.040453922003507614 0.08342521909741672\n",
            "after loss data in training 0.040453922003507614 0.08338765677478219\n",
            "before loss data in training 0.020459583029150963 0.08338765677478219\n",
            "after loss data in training 0.020459583029150963 0.08333269775841046\n",
            "before loss data in training 0.04276920482516289 0.08333269775841046\n",
            "after loss data in training 0.04276920482516289 0.08329730204031861\n",
            "before loss data in training 0.052760642021894455 0.08329730204031861\n",
            "after loss data in training 0.052760642021894455 0.08327067897142722\n",
            "before loss data in training 0.06781815737485886 0.08327067897142722\n",
            "after loss data in training 0.06781815737485886 0.08325721858676122\n",
            "before loss data in training 0.06532955169677734 0.08325721858676122\n",
            "after loss data in training 0.06532955169677734 0.08324161574351494\n",
            "before loss data in training 0.05883431434631348 0.08324161574351494\n",
            "after loss data in training 0.05883431434631348 0.08322039200316954\n",
            "before loss data in training 0.030462797731161118 0.08322039200316954\n",
            "after loss data in training 0.030462797731161118 0.08317455569189933\n",
            "before loss data in training 0.03982672095298767 0.08317455569189933\n",
            "after loss data in training 0.03982672095298767 0.08313692736313291\n",
            "before loss data in training 0.03988504037261009 0.08313692736313291\n",
            "after loss data in training 0.03988504037261009 0.08309941488525735\n",
            "before loss data in training 0.025059211999177933 0.08309941488525735\n",
            "after loss data in training 0.025059211999177933 0.08304912008206318\n",
            "before loss data in training 0.06356163322925568 0.08304912008206318\n",
            "after loss data in training 0.06356163322925568 0.08303224779907374\n",
            "before loss data in training 0.04936701059341431 0.08303224779907374\n",
            "after loss data in training 0.04936701059341431 0.08300312562156019\n",
            "before loss data in training 0.02334057167172432 0.08300312562156019\n",
            "after loss data in training 0.02334057167172432 0.08295155902350501\n",
            "before loss data in training 0.023139379918575287 0.08295155902350501\n",
            "after loss data in training 0.023139379918575287 0.08289990774621232\n",
            "before loss data in training 0.04132339358329773 0.08289990774621232\n",
            "after loss data in training 0.04132339358329773 0.0828640349988759\n",
            "before loss data in training 0.0608721487224102 0.0828640349988759\n",
            "after loss data in training 0.0608721487224102 0.08284507647622377\n",
            "before loss data in training 0.04439765214920044 0.08284507647622377\n",
            "after loss data in training 0.04439765214920044 0.08281196069299636\n",
            "before loss data in training 0.023688536137342453 0.08281196069299636\n",
            "after loss data in training 0.023688536137342453 0.08276107994897255\n",
            "before loss data in training 0.056710079312324524 0.08276107994897255\n",
            "after loss data in training 0.056710079312324524 0.08273868012039418\n",
            "before loss data in training 0.033624693751335144 0.08273868012039418\n",
            "after loss data in training 0.033624693751335144 0.08269648597402901\n",
            "before loss data in training 0.042539507150650024 0.08269648597402901\n",
            "after loss data in training 0.042539507150650024 0.08266201646430937\n",
            "before loss data in training 0.05305774509906769 0.08266201646430937\n",
            "after loss data in training 0.05305774509906769 0.08263662686622597\n",
            "before loss data in training 0.03504326939582825 0.08263662686622597\n",
            "after loss data in training 0.03504326939582825 0.08259584421200969\n",
            "before loss data in training 0.016877062618732452 0.08259584421200969\n",
            "after loss data in training 0.016877062618732452 0.08253957813187845\n",
            "before loss data in training 0.02769123576581478 0.08253957813187845\n",
            "after loss data in training 0.02769123576581478 0.08249265910504693\n",
            "before loss data in training 0.015090498141944408 0.08249265910504693\n",
            "after loss data in training 0.015090498141944408 0.08243505042046308\n",
            "before loss data in training 0.06995498389005661 0.08243505042046308\n",
            "after loss data in training 0.06995498389005661 0.082424392806005\n",
            "before loss data in training 0.03349835425615311 0.082424392806005\n",
            "after loss data in training 0.03349835425615311 0.08238264703932424\n",
            "before loss data in training 0.04155006259679794 0.08238264703932424\n",
            "after loss data in training 0.04155006259679794 0.0823478366519052\n",
            "before loss data in training 0.03924250602722168 0.0823478366519052\n",
            "after loss data in training 0.03924250602722168 0.0823111200159387\n",
            "before loss data in training 0.04602111503481865 0.0823111200159387\n",
            "after loss data in training 0.04602111503481865 0.08228023490531647\n",
            "before loss data in training 0.07089640945196152 0.08228023490531647\n",
            "after loss data in training 0.07089640945196152 0.08227055478163164\n",
            "before loss data in training 0.0656285360455513 0.08227055478163164\n",
            "after loss data in training 0.0656285360455513 0.08225641542841493\n",
            "before loss data in training 0.0692882239818573 0.08225641542841493\n",
            "after loss data in training 0.0692882239818573 0.08224540677693228\n",
            "before loss data in training 0.07025909423828125 0.08224540677693228\n",
            "after loss data in training 0.07025909423828125 0.0822352402692659\n",
            "before loss data in training 0.05919131264090538 0.0822352402692659\n",
            "after loss data in training 0.05919131264090538 0.08221571151703848\n",
            "before loss data in training 0.04187660664319992 0.08221571151703848\n",
            "after loss data in training 0.04187660664319992 0.0821815547813282\n",
            "before loss data in training 0.04778389260172844 0.0821815547813282\n",
            "after loss data in training 0.04778389260172844 0.08215245354428963\n",
            "before loss data in training 0.040515169501304626 0.08215245354428963\n",
            "after loss data in training 0.040515169501304626 0.08211725719260494\n",
            "before loss data in training 0.07760034501552582 0.08211725719260494\n",
            "after loss data in training 0.07760034501552582 0.08211344223299592\n",
            "before loss data in training 0.05697903037071228 0.08211344223299592\n",
            "after loss data in training 0.05697903037071228 0.08209223175885054\n",
            "before loss data in training 0.03834882006049156 0.08209223175885054\n",
            "after loss data in training 0.03834882006049156 0.08205534861239323\n",
            "before loss data in training 0.04504995793104172 0.08205534861239323\n",
            "after loss data in training 0.04504995793104172 0.08202417305158333\n",
            "before loss data in training 0.04852313920855522 0.08202417305158333\n",
            "after loss data in training 0.04852313920855522 0.08199597352814643\n",
            "before loss data in training 0.04535645246505737 0.08199597352814643\n",
            "after loss data in training 0.04535645246505737 0.08196515811934653\n",
            "before loss data in training 0.0703628659248352 0.08196515811934653\n",
            "after loss data in training 0.0703628659248352 0.08195540829397299\n",
            "before loss data in training 0.046163078397512436 0.08195540829397299\n",
            "after loss data in training 0.046163078397512436 0.08192535595988697\n",
            "before loss data in training 0.05484975129365921 0.08192535595988697\n",
            "after loss data in training 0.05484975129365921 0.08190264152644214\n",
            "before loss data in training 0.11421613395214081 0.08190264152644214\n",
            "after loss data in training 0.11421613395214081 0.08192972743794734\n",
            "before loss data in training 0.0330318883061409 0.08192972743794734\n",
            "after loss data in training 0.0330318883061409 0.08188877447385035\n",
            "before loss data in training 0.0312957726418972 0.08188877447385035\n",
            "after loss data in training 0.0312957726418972 0.08184643723382361\n",
            "before loss data in training 0.06780670583248138 0.08184643723382361\n",
            "after loss data in training 0.06780670583248138 0.08183469832796965\n",
            "before loss data in training 0.047720395028591156 0.08183469832796965\n",
            "after loss data in training 0.047720395028591156 0.08180619849229766\n",
            "before loss data in training 0.03862979635596275 0.08180619849229766\n",
            "after loss data in training 0.03862979635596275 0.08177015808984663\n",
            "before loss data in training 0.07795815169811249 0.08177015808984663\n",
            "after loss data in training 0.07795815169811249 0.08176697876841899\n",
            "before loss data in training 0.02361522614955902 0.08176697876841899\n",
            "after loss data in training 0.02361522614955902 0.08171851897456993\n",
            "before loss data in training 0.049919288605451584 0.08171851897456993\n",
            "after loss data in training 0.049919288605451584 0.08169204168034086\n",
            "before loss data in training 0.020249908789992332 0.08169204168034086\n",
            "after loss data in training 0.020249908789992332 0.0816409250972374\n",
            "before loss data in training 0.07253255695104599 0.0816409250972374\n",
            "after loss data in training 0.07253255695104599 0.08163335371889477\n",
            "before loss data in training 0.018085988238453865 0.08163335371889477\n",
            "after loss data in training 0.018085988238453865 0.08158057351500736\n",
            "before loss data in training 0.05073770880699158 0.08158057351500736\n",
            "after loss data in training 0.05073770880699158 0.08155497777666046\n",
            "before loss data in training 0.06580415368080139 0.08155497777666046\n",
            "after loss data in training 0.06580415368080139 0.08154191739183803\n",
            "before loss data in training 0.0732279047369957 0.08154191739183803\n",
            "after loss data in training 0.0732279047369957 0.08153502922890941\n",
            "before loss data in training 0.039236146956682205 0.08153502922890941\n",
            "after loss data in training 0.039236146956682205 0.08150001359788936\n",
            "before loss data in training 0.043814193457365036 0.08150001359788936\n",
            "after loss data in training 0.043814193457365036 0.0814688425307756\n",
            "before loss data in training 0.047412946820259094 0.0814688425307756\n",
            "after loss data in training 0.047412946820259094 0.0814406971624198\n",
            "before loss data in training 0.02617715671658516 0.0814406971624198\n",
            "after loss data in training 0.02617715671658516 0.08139506252951656\n",
            "before loss data in training 0.017334377393126488 0.08139506252951656\n",
            "after loss data in training 0.017334377393126488 0.08134220717874396\n",
            "before loss data in training 0.02951764687895775 0.08134220717874396\n",
            "after loss data in training 0.02951764687895775 0.08129948289160481\n",
            "before loss data in training 0.05854211002588272 0.08129948289160481\n",
            "after loss data in training 0.05854211002588272 0.0812807371149444\n",
            "before loss data in training 0.05838724970817566 0.0812807371149444\n",
            "after loss data in training 0.05838724970817566 0.08126189473847793\n",
            "before loss data in training 0.10070516169071198 0.08126189473847793\n",
            "after loss data in training 0.10070516169071198 0.08127788426722155\n",
            "before loss data in training 0.06442494690418243 0.08127788426722155\n",
            "after loss data in training 0.06442494690418243 0.08126403633183696\n",
            "before loss data in training 0.03797735273838043 0.08126403633183696\n",
            "after loss data in training 0.03797735273838043 0.08122849718274545\n",
            "before loss data in training 0.04002267122268677 0.08122849718274545\n",
            "after loss data in training 0.04002267122268677 0.0811946942082089\n",
            "before loss data in training 0.03373724967241287 0.0811946942082089\n",
            "after loss data in training 0.03373724967241287 0.08115579466350743\n",
            "before loss data in training 0.04976265877485275 0.08115579466350743\n",
            "after loss data in training 0.04976265877485275 0.08113008365950362\n",
            "before loss data in training 0.04380984231829643 0.08113008365950362\n",
            "after loss data in training 0.04380984231829643 0.08109954336380705\n",
            "before loss data in training 0.02239931747317314 0.08109954336380705\n",
            "after loss data in training 0.02239931747317314 0.08105154644975093\n",
            "before loss data in training 0.036358825862407684 0.08105154644975093\n",
            "after loss data in training 0.036358825862407684 0.08101503278914036\n",
            "before loss data in training 0.04628300666809082 0.08101503278914036\n",
            "after loss data in training 0.04628300666809082 0.08098668011475584\n",
            "before loss data in training 0.06882429122924805 0.08098668011475584\n",
            "after loss data in training 0.06882429122924805 0.08097675973230437\n",
            "before loss data in training 0.07157815992832184 0.08097675973230437\n",
            "after loss data in training 0.07157815992832184 0.08096909991176322\n",
            "before loss data in training 0.06894334405660629 0.08096909991176322\n",
            "after loss data in training 0.06894334405660629 0.08095930695096912\n",
            "before loss data in training 0.01757647469639778 0.08095930695096912\n",
            "after loss data in training 0.01757647469639778 0.0809077342640248\n",
            "before loss data in training 0.07630257308483124 0.0809077342640248\n",
            "after loss data in training 0.07630257308483124 0.08090399023054577\n",
            "before loss data in training 0.06739385426044464 0.08090399023054577\n",
            "after loss data in training 0.06739385426044464 0.080893015302869\n",
            "before loss data in training 0.04480285570025444 0.080893015302869\n",
            "after loss data in training 0.04480285570025444 0.0808637213421526\n",
            "before loss data in training 0.029963379725813866 0.0808637213421526\n",
            "after loss data in training 0.029963379725813866 0.08082243963767868\n",
            "before loss data in training 0.04367785155773163 0.08082243963767868\n",
            "after loss data in training 0.04367785155773163 0.08079233867489105\n",
            "before loss data in training 0.03833680599927902 0.08079233867489105\n",
            "after loss data in training 0.03833680599927902 0.08075796172535614\n",
            "before loss data in training 0.08506996929645538 0.08075796172535614\n",
            "after loss data in training 0.08506996929645538 0.08076145040462078\n",
            "before loss data in training 0.01858757995069027 0.08076145040462078\n",
            "after loss data in training 0.01858757995069027 0.08071118858533709\n",
            "before loss data in training 0.05458492040634155 0.08071118858533709\n",
            "after loss data in training 0.05458492040634155 0.08069008497614566\n",
            "before loss data in training 0.058236539363861084 0.08069008497614566\n",
            "after loss data in training 0.058236539363861084 0.08067196266330282\n",
            "before loss data in training 0.051229119300842285 0.08067196266330282\n",
            "after loss data in training 0.051229119300842285 0.0806482184347847\n",
            "before loss data in training 0.03323056176304817 0.0806482184347847\n",
            "after loss data in training 0.03323056176304817 0.08061000920297831\n",
            "before loss data in training 0.04325327277183533 0.08061000920297831\n",
            "after loss data in training 0.04325327277183533 0.0805799313153526\n",
            "before loss data in training 0.033810995519161224 0.0805799313153526\n",
            "after loss data in training 0.033810995519161224 0.08054230546193651\n",
            "before loss data in training 0.026006978005170822 0.08054230546193651\n",
            "after loss data in training 0.026006978005170822 0.0804984667742703\n",
            "before loss data in training 0.030651262030005455 0.0804984667742703\n",
            "after loss data in training 0.030651262030005455 0.08045842885881306\n",
            "before loss data in training 0.018606100231409073 0.08045842885881306\n",
            "after loss data in training 0.018606100231409073 0.08040878814562895\n",
            "before loss data in training 0.03667726740241051 0.08040878814562895\n",
            "after loss data in training 0.03667726740241051 0.0803737187625149\n",
            "before loss data in training 0.03241215646266937 0.0803737187625149\n",
            "after loss data in training 0.03241215646266937 0.08033528802349259\n",
            "before loss data in training 0.03427676856517792 0.08033528802349259\n",
            "after loss data in training 0.03427676856517792 0.08029841170687264\n",
            "before loss data in training 0.028548767790198326 0.08029841170687264\n",
            "after loss data in training 0.028548767790198326 0.0802570119917393\n",
            "before loss data in training 0.0491478368639946 0.0802570119917393\n",
            "after loss data in training 0.0491478368639946 0.08023214454559402\n",
            "before loss data in training 0.09588442742824554 0.08023214454559402\n",
            "after loss data in training 0.09588442742824554 0.08024464636898272\n",
            "before loss data in training 0.047224968671798706 0.08024464636898272\n",
            "after loss data in training 0.047224968671798706 0.08021829387281577\n",
            "before loss data in training 0.10353933274745941 0.08021829387281577\n",
            "after loss data in training 0.10353933274745941 0.08023689119249253\n",
            "before loss data in training 0.054322510957717896 0.08023689119249253\n",
            "after loss data in training 0.054322510957717896 0.08021624228393892\n",
            "before loss data in training 0.040750641375780106 0.08021624228393892\n",
            "after loss data in training 0.040750641375780106 0.08018482062716491\n",
            "before loss data in training 0.06194797158241272 0.08018482062716491\n",
            "after loss data in training 0.06194797158241272 0.08017031239403463\n",
            "before loss data in training 0.04773213714361191 0.08017031239403463\n",
            "after loss data in training 0.04773213714361191 0.08014452688111696\n",
            "before loss data in training 0.02837216854095459 0.08014452688111696\n",
            "after loss data in training 0.02837216854095459 0.08010340507147426\n",
            "before loss data in training 0.045041292905807495 0.08010340507147426\n",
            "after loss data in training 0.045041292905807495 0.0800755779983269\n",
            "before loss data in training 0.03641314432024956 0.0800755779983269\n",
            "after loss data in training 0.03641314432024956 0.08004095275353858\n",
            "before loss data in training 0.048358187079429626 0.08004095275353858\n",
            "after loss data in training 0.048358187079429626 0.0800158475509442\n",
            "before loss data in training 0.0699143037199974 0.0800158475509442\n",
            "after loss data in training 0.0699143037199974 0.08000784949565445\n",
            "before loss data in training 0.05293242260813713 0.08000784949565445\n",
            "after loss data in training 0.05293242260813713 0.07998642906299028\n",
            "before loss data in training 0.052312981337308884 0.07998642906299028\n",
            "after loss data in training 0.052312981337308884 0.07996455281972886\n",
            "before loss data in training 0.051652636379003525 0.07996455281972886\n",
            "after loss data in training 0.051652636379003525 0.07994218953660033\n",
            "before loss data in training 0.04548542946577072 0.07994218953660033\n",
            "after loss data in training 0.04548542946577072 0.07991499398800457\n",
            "before loss data in training 0.030576957389712334 0.07991499398800457\n",
            "after loss data in training 0.030576957389712334 0.07987608386450434\n",
            "before loss data in training 0.045641493052244186 0.07987608386450434\n",
            "after loss data in training 0.045641493052244186 0.07984910625157111\n",
            "before loss data in training 0.08839177340269089 0.07984910625157111\n",
            "after loss data in training 0.08839177340269089 0.07985583276113893\n",
            "before loss data in training 0.04231578856706619 0.07985583276113893\n",
            "after loss data in training 0.04231578856706619 0.07982629692778404\n",
            "before loss data in training 0.05356128513813019 0.07982629692778404\n",
            "after loss data in training 0.05356128513813019 0.07980564833360979\n",
            "before loss data in training 0.03242580592632294 0.07980564833360979\n",
            "after loss data in training 0.03242580592632294 0.07976842929008482\n",
            "before loss data in training 0.029675427824258804 0.07976842929008482\n",
            "after loss data in training 0.029675427824258804 0.07972910982268622\n",
            "before loss data in training 0.05337273329496384 0.07972910982268622\n",
            "after loss data in training 0.05337273329496384 0.07970843815482134\n",
            "before loss data in training 0.05713880807161331 0.07970843815482134\n",
            "after loss data in training 0.05713880807161331 0.07969075035695049\n",
            "before loss data in training 0.06253404915332794 0.07969075035695049\n",
            "after loss data in training 0.06253404915332794 0.07967731519547545\n",
            "before loss data in training 0.021982543170452118 0.07967731519547545\n",
            "after loss data in training 0.021982543170452118 0.07963217061642613\n",
            "before loss data in training 0.0891910046339035 0.07963217061642613\n",
            "after loss data in training 0.0891910046339035 0.07963964429431314\n",
            "before loss data in training 0.03122718445956707 0.07963964429431314\n",
            "after loss data in training 0.03122718445956707 0.07960182206006725\n",
            "before loss data in training 0.03713390976190567 0.07960182206006725\n",
            "after loss data in training 0.03713390976190567 0.07956866990370647\n",
            "before loss data in training 0.05440341681241989 0.07956866990370647\n",
            "after loss data in training 0.05440341681241989 0.07954904022110795\n",
            "before loss data in training 0.05052941292524338 0.07954904022110795\n",
            "after loss data in training 0.05052941292524338 0.07952642164956013\n",
            "before loss data in training 0.03644901141524315 0.07952642164956013\n",
            "after loss data in training 0.03644901141524315 0.07949287226464244\n",
            "before loss data in training 0.023201843723654747 0.07949287226464244\n",
            "after loss data in training 0.023201843723654747 0.07944906601675061\n",
            "before loss data in training 0.04601358622312546 0.07944906601675061\n",
            "after loss data in training 0.04601358622312546 0.07942306642126568\n",
            "before loss data in training 0.0317806713283062 0.07942306642126568\n",
            "after loss data in training 0.0317806713283062 0.07938604824326027\n",
            "before loss data in training 0.06340809911489487 0.07938604824326027\n",
            "after loss data in training 0.06340809911489487 0.07937364300325378\n",
            "before loss data in training 0.060900382697582245 0.07937364300325378\n",
            "after loss data in training 0.060900382697582245 0.07935931153676373\n",
            "before loss data in training 0.044402480125427246 0.07935931153676373\n",
            "after loss data in training 0.044402480125427246 0.07933221321784022\n",
            "before loss data in training 0.07334712892770767 0.07933221321784022\n",
            "after loss data in training 0.07334712892770767 0.07932757721141874\n",
            "before loss data in training 0.039845626801252365 0.07932757721141874\n",
            "after loss data in training 0.039845626801252365 0.07929701842627156\n",
            "before loss data in training 0.04137668013572693 0.07929701842627156\n",
            "after loss data in training 0.04137668013572693 0.07926769101846758\n",
            "before loss data in training 0.022200187668204308 0.07926769101846758\n",
            "after loss data in training 0.022200187668204308 0.0792235893930037\n",
            "before loss data in training 0.04533305764198303 0.0792235893930037\n",
            "after loss data in training 0.04533305764198303 0.07919741909821526\n",
            "before loss data in training 0.038975950330495834 0.07919741909821526\n",
            "after loss data in training 0.038975950330495834 0.07916638401428955\n",
            "before loss data in training 0.04096807911992073 0.07916638401428955\n",
            "after loss data in training 0.04096807911992073 0.0791369327383494\n",
            "before loss data in training 0.03364849090576172 0.0791369327383494\n",
            "after loss data in training 0.03364849090576172 0.07910188771382506\n",
            "before loss data in training 0.0876144990324974 0.07910188771382506\n",
            "after loss data in training 0.0876144990324974 0.07910844091730364\n",
            "before loss data in training 0.05417924374341965 0.07910844091730364\n",
            "after loss data in training 0.05417924374341965 0.07908926461178527\n",
            "before loss data in training 0.06537364423274994 0.07908926461178527\n",
            "after loss data in training 0.06537364423274994 0.07907872224408424\n",
            "before loss data in training 0.011668546125292778 0.07907872224408424\n",
            "after loss data in training 0.011668546125292778 0.07902694791526797\n",
            "before loss data in training 0.046778347343206406 0.07902694791526797\n",
            "after loss data in training 0.046778347343206406 0.07900219841367775\n",
            "before loss data in training 0.032162539660930634 0.07900219841367775\n",
            "after loss data in training 0.032162539660930634 0.07896627842997166\n",
            "before loss data in training 0.044414959847927094 0.07896627842997166\n",
            "after loss data in training 0.044414959847927094 0.07893980232377852\n",
            "before loss data in training 0.023419102653861046 0.07893980232377852\n",
            "after loss data in training 0.023419102653861046 0.07889729030259175\n",
            "before loss data in training 0.04637959599494934 0.07889729030259175\n",
            "after loss data in training 0.04637959599494934 0.07887241065889807\n",
            "before loss data in training 0.08586131781339645 0.07887241065889807\n",
            "after loss data in training 0.08586131781339645 0.07887775386008652\n",
            "before loss data in training 0.038489922881126404 0.07887775386008652\n",
            "after loss data in training 0.038489922881126404 0.0788468999021194\n",
            "before loss data in training 0.04056299477815628 0.0788468999021194\n",
            "after loss data in training 0.04056299477815628 0.07881767554706294\n",
            "before loss data in training 0.04655476659536362 0.07881767554706294\n",
            "after loss data in training 0.04655476659536362 0.07879306615808376\n",
            "before loss data in training 0.052561551332473755 0.07879306615808376\n",
            "after loss data in training 0.052561551332473755 0.0787730726254423\n",
            "before loss data in training 0.02824847400188446 0.0787730726254423\n",
            "after loss data in training 0.02824847400188446 0.07873459235230935\n",
            "before loss data in training 0.05808831378817558 0.07873459235230935\n",
            "after loss data in training 0.05808831378817558 0.07871887981154517\n",
            "before loss data in training 0.04667702317237854 0.07871887981154517\n",
            "after loss data in training 0.04667702317237854 0.07869451338064086\n",
            "before loss data in training 0.037992291152477264 0.07869451338064086\n",
            "after loss data in training 0.037992291152477264 0.0786635846403459\n",
            "before loss data in training 0.029968559741973877 0.0786635846403459\n",
            "after loss data in training 0.029968559741973877 0.07862661043768958\n",
            "before loss data in training 0.0485101193189621 0.07862661043768958\n",
            "after loss data in training 0.0485101193189621 0.07860376029268296\n",
            "before loss data in training 0.07036664336919785 0.07860376029268296\n",
            "after loss data in training 0.07036664336919785 0.07859751532155067\n",
            "before loss data in training 0.04112733528017998 0.07859751532155067\n",
            "after loss data in training 0.04112733528017998 0.07856912882151933\n",
            "before loss data in training 0.017615489661693573 0.07856912882151933\n",
            "after loss data in training 0.017615489661693573 0.0785229867782492\n",
            "before loss data in training 0.05117429420351982 0.0785229867782492\n",
            "after loss data in training 0.05117429420351982 0.0785022994162411\n",
            "before loss data in training 0.03736044466495514 0.0785022994162411\n",
            "after loss data in training 0.03736044466495514 0.07847120202035955\n",
            "before loss data in training 0.05438767373561859 0.07847120202035955\n",
            "after loss data in training 0.05438767373561859 0.07845301204431368\n",
            "before loss data in training 0.04171533137559891 0.07845301204431368\n",
            "after loss data in training 0.04171533137559891 0.07842528549286559\n",
            "before loss data in training 0.04513011872768402 0.07842528549286559\n",
            "after loss data in training 0.04513011872768402 0.07840017601566711\n",
            "before loss data in training 0.022379152476787567 0.07840017601566711\n",
            "after loss data in training 0.022379152476787567 0.07835795972061144\n",
            "before loss data in training 0.05433879792690277 0.07835795972061144\n",
            "after loss data in training 0.05433879792690277 0.07833987300239328\n",
            "before loss data in training 0.05873938649892807 0.07833987300239328\n",
            "after loss data in training 0.05873938649892807 0.07832512470555095\n",
            "before loss data in training 0.07087495177984238 0.07832512470555095\n",
            "after loss data in training 0.07087495177984238 0.07831952307177222\n",
            "before loss data in training 0.05319014936685562 0.07831952307177222\n",
            "after loss data in training 0.05319014936685562 0.07830064300137032\n",
            "before loss data in training 0.02744271606206894 0.07830064300137032\n",
            "after loss data in training 0.02744271606206894 0.07826246137453902\n",
            "before loss data in training 0.08745567500591278 0.07826246137453902\n",
            "after loss data in training 0.08745567500591278 0.07826935800892114\n",
            "before loss data in training 0.03851529210805893 0.07826935800892114\n",
            "after loss data in training 0.03851529210805893 0.07823955735982004\n",
            "before loss data in training 0.04077361151576042 0.07823955735982004\n",
            "after loss data in training 0.04077361151576042 0.07821149298091062\n",
            "before loss data in training 0.07129207253456116 0.07821149298091062\n",
            "after loss data in training 0.07129207253456116 0.0782063137739897\n",
            "before loss data in training 0.0298543032258749 0.0782063137739897\n",
            "after loss data in training 0.0298543032258749 0.07817014921860592\n",
            "before loss data in training 0.03350266069173813 0.07817014921860592\n",
            "after loss data in training 0.03350266069173813 0.07813676544541694\n",
            "before loss data in training 0.033140070736408234 0.07813676544541694\n",
            "after loss data in training 0.033140070736408234 0.07810316074436466\n",
            "before loss data in training 0.03148053213953972 0.07810316074436466\n",
            "after loss data in training 0.03148053213953972 0.07806836773794315\n",
            "before loss data in training 0.009059666655957699 0.07806836773794315\n",
            "after loss data in training 0.009059666655957699 0.0780169071107381\n",
            "before loss data in training 0.04188540577888489 0.0780169071107381\n",
            "after loss data in training 0.04188540577888489 0.07798998348828515\n",
            "before loss data in training 0.020602751523256302 0.07798998348828515\n",
            "after loss data in training 0.020602751523256302 0.0779472528613566\n",
            "before loss data in training 0.05941066890954971 0.0779472528613566\n",
            "after loss data in training 0.05941066890954971 0.07793346076020198\n",
            "before loss data in training 0.02777276560664177 0.07793346076020198\n",
            "after loss data in training 0.02777276560664177 0.07789616656306178\n",
            "before loss data in training 0.054173629730939865 0.07789616656306178\n",
            "after loss data in training 0.054173629730939865 0.07787854209290418\n",
            "before loss data in training 0.057697735726833344 0.07787854209290418\n",
            "after loss data in training 0.057697735726833344 0.0778635600540281\n",
            "before loss data in training 0.07963716983795166 0.0778635600540281\n",
            "after loss data in training 0.07963716983795166 0.07786487578828917\n",
            "before loss data in training 0.08668502420186996 0.07786487578828917\n",
            "after loss data in training 0.08668502420186996 0.07787141407473364\n",
            "before loss data in training 0.03043607622385025 0.07787141407473364\n",
            "after loss data in training 0.03043607622385025 0.07783627678743668\n",
            "before loss data in training 0.03339443728327751 0.07783627678743668\n",
            "after loss data in training 0.03339443728327751 0.07780338127336994\n",
            "before loss data in training 0.03618951514363289 0.07780338127336994\n",
            "after loss data in training 0.03618951514363289 0.0777726017865876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "example = testDf['comment_text'][0]\n",
        "encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    None,\n",
        "    add_special_tokens=True,\n",
        "    max_length=MAX_LEN,\n",
        "    padding='max_length',\n",
        "    return_token_type_ids=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "    attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "    token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "    output = model(input_ids, attention_mask, token_type_ids)\n",
        "    final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "    print(trainDf.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])"
      ],
      "metadata": {
        "id": "Cw_rNa7_JzMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(testDf))[:5]:\n",
        "  example = testDf['comment_text'][i]\n",
        "  \n",
        "\n",
        "  encodings = tokenizer.encode_plus(\n",
        "      example,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=MAX_LEN,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=True,\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "      attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "      token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "      output = model(input_ids, attention_mask, token_type_ids)\n",
        "      final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "      print(trainDf.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n",
        "      print(testDf[targetList][i])"
      ],
      "metadata": {
        "id": "BMiIslZ-JzJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ux4JST_tJzHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yZ5IYXOZJzEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dEorUUi_JzCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oSvzTX07Jy_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}